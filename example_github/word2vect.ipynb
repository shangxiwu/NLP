{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)\n",
    "#> Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Saudis': 0,\n",
       " 'The': 1,\n",
       " 'a': 2,\n",
       " 'acknowledge': 3,\n",
       " 'are': 4,\n",
       " 'preparing': 5,\n",
       " 'report': 6,\n",
       " 'that': 7,\n",
       " 'will': 8,\n",
       " 'Jamal': 9,\n",
       " \"Khashoggi's\": 10,\n",
       " 'Saudi': 11,\n",
       " 'an': 12,\n",
       " 'death': 13,\n",
       " 'journalist': 14,\n",
       " 'of': 15,\n",
       " 'result': 16,\n",
       " 'the': 17,\n",
       " 'was': 18,\n",
       " 'intended': 19,\n",
       " 'interrogation': 20,\n",
       " 'lead': 21,\n",
       " 'one': 22,\n",
       " 'to': 23,\n",
       " 'went': 24,\n",
       " 'wrong,': 25,\n",
       " 'Turkey,': 26,\n",
       " 'abduction': 27,\n",
       " 'according': 28,\n",
       " 'from': 29,\n",
       " 'his': 30,\n",
       " 'sources.': 31,\n",
       " 'two': 32}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'graph': 33, 'in': 34, 'intersection': 35, 'paths': 36, 'trees': 37, 'Graph': 38, 'IV': 39, 'Widths': 40, 'and': 41, 'minors': 42, 'ordering': 43, 'quasi': 44, 'well': 45, 'A': 46, 'survey': 47}\n"
     ]
    }
   ],
   "source": [
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "\n",
    "\n",
    "# If you check now, the dictionary should have been updated with the new words (tokens).\n",
    "print(dictionary)\n",
    "#> Dictionary(45 unique tokens: ['Human', 'abc', 'applications', 'computer', 'for']...)\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n",
      "Features name:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n",
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "# CountVectorizer(stop_words=\"english\")\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\n",
    "\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"Features name:\\n{}\".format(vect.get_feature_names()))\n",
    "print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result1_vector shape: (5, 40)\n",
      "result2_vector shape: (5, 24)\n",
      "result3_vector shape: (5, 8)\n",
      "result4_vector shape: (5, 35)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text =[\n",
    "      \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n",
    "      \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n",
    "      \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n",
    "      \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n",
    "      \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" \n",
    "     ]\n",
    "\n",
    "model1 = CountVectorizer(text,stop_words=[\"all\",\"in\",\"the\",\"is\",\"and\"])\n",
    "result1_vector = model1.fit_transform(text)\n",
    "print('result1_vector shape: {}'.format(result1_vector.shape))\n",
    "\n",
    "model2 = CountVectorizer(text,stop_words=\"english\")\n",
    "result2_vector = model2.fit_transform(text)\n",
    "print('result2_vector shape: {}'.format(result2_vector.shape))\n",
    "\n",
    "# use proportion here. Ignore terms that occurred in less than 25% of the documents\n",
    "#model3 = CountVectorizer(text,min_df=0.25)\n",
    "# ignore terms that appeared in less than n documents (can be proportion or absolute counts)\n",
    "model3 = CountVectorizer(text,min_df=2)\n",
    "result3_vector = model3.fit_transform(text)\n",
    "print('result3_vector shape: {}'.format(result3_vector.shape))\n",
    "\n",
    "# ignore terms that appeared in more than n documents (can be proportion or absolute counts)\n",
    "# use proportion here\n",
    "model4 = CountVectorizer(text,max_df=0.50)\n",
    "result4_vector = model4.fit_transform(text)\n",
    "print('result4_vector shape: {}'.format(result4_vector.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n",
      "-------output 0-th document tf-idf weight------\n",
      "and 0.0\n",
      "document 0.4387767428592343\n",
      "first 0.5419765697264572\n",
      "is 0.4387767428592343\n",
      "one 0.0\n",
      "second 0.0\n",
      "the 0.35872873824808993\n",
      "third 0.0\n",
      "this 0.4387767428592343\n",
      "-------output 1-th document tf-idf weight------\n",
      "and 0.0\n",
      "document 0.2723014675233404\n",
      "first 0.0\n",
      "is 0.2723014675233404\n",
      "one 0.0\n",
      "second 0.8532257361452786\n",
      "the 0.22262429232510395\n",
      "third 0.0\n",
      "this 0.2723014675233404\n",
      "-------output 2-th document tf-idf weight------\n",
      "and 0.5528053199908667\n",
      "document 0.0\n",
      "first 0.0\n",
      "is 0.0\n",
      "one 0.5528053199908667\n",
      "second 0.0\n",
      "the 0.2884767487500274\n",
      "third 0.5528053199908667\n",
      "this 0.0\n",
      "-------output 3-th document tf-idf weight------\n",
      "and 0.0\n",
      "document 0.4387767428592343\n",
      "first 0.5419765697264572\n",
      "is 0.4387767428592343\n",
      "one 0.0\n",
      "second 0.0\n",
      "the 0.35872873824808993\n",
      "third 0.0\n",
      "this 0.4387767428592343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "word = vectorizer.get_feature_names()\n",
    "print(word)\n",
    "\n",
    "print(X.toarray())\n",
    " \n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "tfidf_weight = tfidf.toarray() \n",
    "print(tfidf_weight)\n",
    "\n",
    "\n",
    "for i in range(len(tfidf_weight)):\n",
    "    print(\"-------output {}-th document tf-idf weight------\".format(i))\n",
    "    for j in range(len(word)):\n",
    "        print(word[j],tfidf_weight[i][j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n",
      "Vocabulary size: 14\n",
      "Vocabulary:\n",
      "['be fool', 'but the', 'doth think', 'fool doth', 'he is', 'himself to', 'is wise', 'knows himself', 'man knows', 'the fool', 'the wise', 'think he', 'to be', 'wise man']\n",
      "Transformed data (dense):\n",
      "[[0 0 1 1 1 0 1 0 0 1 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bards_words =[\"The fool doth think he is wise\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "vect1 = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(vect1.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect1.get_feature_names()))\n",
    "\n",
    "vect2 = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(vect2.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect2.get_feature_names()))\n",
    "print(\"Transformed data (dense):\\n{}\".format(vect2.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大年初六桃園八德大溪參香祈福祈求台灣平安桃園建設大步向前桃園市長鄭文燦今日上午前往桃園區清水巖下午前往八德區廣行宮大溪區中庄福德宮永安宮內柵仁安宮溪洲福山巖慈聖宮龍山寺參香並發送桃園福御守福袋給大年初六走春參香的市民朋友鄭市長表示大年初六是清水祖師聖誕也是開工的日子祈求清水祖師庇佑台灣平安健康武漢肺炎疫情不要蔓延到台灣也祈求桃園建設持續大步向前祝福所有鄉親信眾鼠來運轉今年的願望都能努力打拚實現鄭市長也呼籲市府將以高標準進行武漢肺炎防疫工作請市民朋友勤加洗手戴口罩量體溫如需前往人潮較多的地方記得要做好清潔消毒工作此外應避免聽信網路謠言造成恐慌亦可透過衛福部疾病管制署的疾管家獲知最新防疫資訊保護自身及周遭親友的健康安全鄭市長在中庄福德宮表示市府致力推動中庄地區發展中庄不只有調整池攔河堰中庄運動公園即將動工大漢溪邊也將興建堤防及防汛道路市民朋友無論在交通或觀光休憩都將更加便利另外國道號增設大鶯豐德交流道可行性研究已獲得交通部審議通過並陸續辦理相關建設計畫府會也將攜手合作讓交流道順利推動完成今日包括立法委員趙正宇市議員朱珍瑤呂林小鳳李柏坊陳治文黃家齊蔡永芳桃園工策會總幹事陳家濬市府民政局副局長林香美警察局督察長吳坤旭桃園區長陳玉明八德區長邱瑞朝大溪區長陳嘉聰桃園果菜市場公司董事長邱素芬大嵙崁文教基金會執行長李世明清水巖主委邱顯來廣行宮主委李秀明中庄福德宮主委沈琳容永安宮主委林繼雄內柵仁安宮主委簡子嚴溪洲福山巖主委楊賴傳慈聖宮主委蔡水木龍山寺董事長陳有盛等均一同參香\n"
     ]
    }
   ],
   "source": [
    "#https://medium.com/%E6%89%8B%E5%AF%AB%E7%AD%86%E8%A8%98/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-%E4%BD%BF%E7%94%A8-n-gram-%E5%AF%A6%E7%8F%BE%E8%BC%B8%E5%85%A5%E6%96%87%E5%AD%97%E9%A0%90%E6%B8%AC-10ac622aab7a\n",
    "\n",
    "from collections import Counter, namedtuple\n",
    "import json\n",
    "import re\n",
    "\n",
    "DATASET_DIR = 'dataset/WebNews.json'\n",
    "with open(DATASET_DIR, encoding = 'utf8') as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "seg_list = list(map(lambda d: d['detailcontent'], dataset))\n",
    "rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "seg_list = [rule.sub('', seg) for seg in seg_list]\n",
    "print(seg_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(documents, N=2):\n",
    "    ngram_prediction = dict()\n",
    "    total_grams = list()\n",
    "    words = list()\n",
    "    Word = namedtuple('Word', ['word', 'prob'])\n",
    "\n",
    "    for doc in documents:\n",
    "        split_words = ['<s>'] + list(doc) + ['</s>']\n",
    "        # 計算分子\n",
    "        [total_grams.append(tuple(split_words[i:i+N])) for i in range(len(split_words)-N+1)]\n",
    "        # 計算分母\n",
    "        [words.append(tuple(split_words[i:i+N-1])) for i in range(len(split_words)-N+2)]\n",
    "        \n",
    "    total_word_counter = Counter(total_grams)\n",
    "    word_counter = Counter(words)\n",
    "    \n",
    "    for key in total_word_counter:\n",
    "        word = ''.join(key[:N-1])\n",
    "        if word not in ngram_prediction:\n",
    "            ngram_prediction.update({word: set()})\n",
    "            \n",
    "        next_word_prob = total_word_counter[key]/word_counter[key[:N-1]]\n",
    "        w = Word(key[-1], '{:.3g}'.format(next_word_prob))\n",
    "        ngram_prediction[word].add(w)\n",
    "        \n",
    "    return ngram_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tri_prediction = ngram(seg_list, N=3)\n",
    "print(tri_prediction)\n",
    "for word, ng in tri_prediction.items():\n",
    "    tri_prediction[word] = sorted(ng, key=lambda x: x.prob, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word: 隊, probability: 0.2\n",
      "next word: 首, probability: 0.143\n",
      "next word: 日, probability: 0.0571\n",
      "next word: 及, probability: 0.0571\n",
      "next word: 代, probability: 0.0571\n"
     ]
    }
   ],
   "source": [
    "text = '韓國'\n",
    "next_words = list(tri_prediction[text])[:5]\n",
    "for next_word in next_words:\n",
    "    print('next word: {}, probability: {}'.format(next_word.word, next_word.prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW & Skip-gram  \n",
    "yield explain: https://pyzh.readthedocs.io/en/latest/the-python-yield-keyword-explained.html#id8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "data_file=\"./dataset/reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('./dataset/reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file ./dataset/reviews_data.txt.gz...this may take a while\n",
      "read 0 reviews\n",
      "read 10000 reviews\n",
      "read 20000 reviews\n",
      "read 30000 reviews\n",
      "read 40000 reviews\n",
      "read 50000 reviews\n",
      "read 60000 reviews\n",
      "read 70000 reviews\n",
      "read 80000 reviews\n",
      "read 90000 reviews\n",
      "read 100000 reviews\n",
      "read 110000 reviews\n",
      "read 120000 reviews\n",
      "read 130000 reviews\n",
      "read 140000 reviews\n",
      "read 150000 reviews\n",
      "read 160000 reviews\n",
      "read 170000 reviews\n",
      "read 180000 reviews\n",
      "read 190000 reviews\n",
      "read 200000 reviews\n",
      "read 210000 reviews\n",
      "read 220000 reviews\n",
      "read 230000 reviews\n",
      "read 240000 reviews\n",
      "read 250000 reviews\n",
      "Done reading data file\n"
     ]
    }
   ],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    print(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                print(\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "print(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oct',\n",
       " 'nice',\n",
       " 'trendy',\n",
       " 'hotel',\n",
       " 'location',\n",
       " 'not',\n",
       " 'too',\n",
       " 'bad',\n",
       " 'stayed',\n",
       " 'in',\n",
       " 'this',\n",
       " 'hotel',\n",
       " 'for',\n",
       " 'one',\n",
       " 'night',\n",
       " 'as',\n",
       " 'this',\n",
       " 'is',\n",
       " 'fairly',\n",
       " 'new',\n",
       " 'place',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'taxi',\n",
       " 'drivers',\n",
       " 'did',\n",
       " 'not',\n",
       " 'know',\n",
       " 'where',\n",
       " 'it',\n",
       " 'was',\n",
       " 'and',\n",
       " 'or',\n",
       " 'did',\n",
       " 'not',\n",
       " 'want',\n",
       " 'to',\n",
       " 'drive',\n",
       " 'there',\n",
       " 'once',\n",
       " 'have',\n",
       " 'eventually',\n",
       " 'arrived',\n",
       " 'at',\n",
       " 'the',\n",
       " 'hotel',\n",
       " 'was',\n",
       " 'very',\n",
       " 'pleasantly',\n",
       " 'surprised',\n",
       " 'with',\n",
       " 'the',\n",
       " 'decor',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lobby',\n",
       " 'ground',\n",
       " 'floor',\n",
       " 'area',\n",
       " 'it',\n",
       " 'was',\n",
       " 'very',\n",
       " 'stylish',\n",
       " 'and',\n",
       " 'modern',\n",
       " 'found',\n",
       " 'the',\n",
       " 'reception',\n",
       " 'staff',\n",
       " 'geeting',\n",
       " 'me',\n",
       " 'with',\n",
       " 'aloha',\n",
       " 'bit',\n",
       " 'out',\n",
       " 'of',\n",
       " 'place',\n",
       " 'but',\n",
       " 'guess',\n",
       " 'they',\n",
       " 'are',\n",
       " 'briefed',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'up',\n",
       " 'the',\n",
       " 'coroporate',\n",
       " 'image',\n",
       " 'as',\n",
       " 'have',\n",
       " 'starwood',\n",
       " 'preferred',\n",
       " 'guest',\n",
       " 'member',\n",
       " 'was',\n",
       " 'given',\n",
       " 'small',\n",
       " 'gift',\n",
       " 'upon',\n",
       " 'check',\n",
       " 'in',\n",
       " 'it',\n",
       " 'was',\n",
       " 'only',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'fridge',\n",
       " 'magnets',\n",
       " 'in',\n",
       " 'gift',\n",
       " 'box',\n",
       " 'but',\n",
       " 'nevertheless',\n",
       " 'nice',\n",
       " 'gesture',\n",
       " 'my',\n",
       " 'room',\n",
       " 'was',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'roomy',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tea',\n",
       " 'and',\n",
       " 'coffee',\n",
       " 'facilities',\n",
       " 'in',\n",
       " 'each',\n",
       " 'room',\n",
       " 'and',\n",
       " 'you',\n",
       " 'get',\n",
       " 'two',\n",
       " 'complimentary',\n",
       " 'bottles',\n",
       " 'of',\n",
       " 'water',\n",
       " 'plus',\n",
       " 'some',\n",
       " 'toiletries',\n",
       " 'by',\n",
       " 'bliss',\n",
       " 'the',\n",
       " 'location',\n",
       " 'is',\n",
       " 'not',\n",
       " 'great',\n",
       " 'it',\n",
       " 'is',\n",
       " 'at',\n",
       " 'the',\n",
       " 'last',\n",
       " 'metro',\n",
       " 'stop',\n",
       " 'and',\n",
       " 'you',\n",
       " 'then',\n",
       " 'need',\n",
       " 'to',\n",
       " 'take',\n",
       " 'taxi',\n",
       " 'but',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are',\n",
       " 'not',\n",
       " 'planning',\n",
       " 'on',\n",
       " 'going',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'historic',\n",
       " 'sites',\n",
       " 'in',\n",
       " 'beijing',\n",
       " 'then',\n",
       " 'you',\n",
       " 'will',\n",
       " 'be',\n",
       " 'ok',\n",
       " 'chose',\n",
       " 'to',\n",
       " 'have',\n",
       " 'some',\n",
       " 'breakfast',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hotel',\n",
       " 'which',\n",
       " 'was',\n",
       " 'really',\n",
       " 'tasty',\n",
       " 'and',\n",
       " 'there',\n",
       " 'was',\n",
       " 'good',\n",
       " 'selection',\n",
       " 'of',\n",
       " 'dishes',\n",
       " 'there',\n",
       " 'are',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'use',\n",
       " 'in',\n",
       " 'the',\n",
       " 'communal',\n",
       " 'area',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'pool',\n",
       " 'table',\n",
       " 'there',\n",
       " 'is',\n",
       " 'also',\n",
       " 'small',\n",
       " 'swimming',\n",
       " 'pool',\n",
       " 'and',\n",
       " 'gym',\n",
       " 'area',\n",
       " 'would',\n",
       " 'definitely',\n",
       " 'stay',\n",
       " 'in',\n",
       " 'this',\n",
       " 'hotel',\n",
       " 'again',\n",
       " 'but',\n",
       " 'only',\n",
       " 'if',\n",
       " 'did',\n",
       " 'not',\n",
       " 'plan',\n",
       " 'to',\n",
       " 'travel',\n",
       " 'to',\n",
       " 'central',\n",
       " 'beijing',\n",
       " 'as',\n",
       " 'it',\n",
       " 'can',\n",
       " 'take',\n",
       " 'long',\n",
       " 'time',\n",
       " 'the',\n",
       " 'location',\n",
       " 'is',\n",
       " 'ok',\n",
       " 'if',\n",
       " 'you',\n",
       " 'plan',\n",
       " 'to',\n",
       " 'do',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'shopping',\n",
       " 'as',\n",
       " 'there',\n",
       " 'is',\n",
       " 'big',\n",
       " 'shopping',\n",
       " 'centre',\n",
       " 'just',\n",
       " 'few',\n",
       " 'minutes',\n",
       " 'away',\n",
       " 'from',\n",
       " 'the',\n",
       " 'hotel',\n",
       " 'and',\n",
       " 'there',\n",
       " 'are',\n",
       " 'plenty',\n",
       " 'of',\n",
       " 'eating',\n",
       " 'options',\n",
       " 'around',\n",
       " 'including',\n",
       " 'restaurants',\n",
       " 'that',\n",
       " 'serve',\n",
       " 'dog',\n",
       " 'meat']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:25:19,451 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2020-01-31 09:25:19,451 : INFO : collecting all words and their counts\n",
      "2020-01-31 09:25:19,455 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-01-31 09:25:19,891 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2020-01-31 09:25:20,276 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2020-01-31 09:25:20,741 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2020-01-31 09:25:21,130 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2020-01-31 09:25:21,667 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2020-01-31 09:25:22,075 : INFO : PROGRESS: at sentence #60000, processed 11013723 words, keeping 76781 word types\n",
      "2020-01-31 09:25:22,496 : INFO : PROGRESS: at sentence #70000, processed 12637525 words, keeping 83194 word types\n",
      "2020-01-31 09:25:22,789 : INFO : PROGRESS: at sentence #80000, processed 14099751 words, keeping 88454 word types\n",
      "2020-01-31 09:25:23,087 : INFO : PROGRESS: at sentence #90000, processed 15662149 words, keeping 93352 word types\n",
      "2020-01-31 09:25:23,404 : INFO : PROGRESS: at sentence #100000, processed 17164487 words, keeping 97881 word types\n",
      "2020-01-31 09:25:23,727 : INFO : PROGRESS: at sentence #110000, processed 18652292 words, keeping 102127 word types\n",
      "2020-01-31 09:25:24,079 : INFO : PROGRESS: at sentence #120000, processed 20152529 words, keeping 105918 word types\n",
      "2020-01-31 09:25:24,404 : INFO : PROGRESS: at sentence #130000, processed 21684330 words, keeping 110099 word types\n",
      "2020-01-31 09:25:24,735 : INFO : PROGRESS: at sentence #140000, processed 23330206 words, keeping 114103 word types\n",
      "2020-01-31 09:25:25,115 : INFO : PROGRESS: at sentence #150000, processed 24838754 words, keeping 118169 word types\n",
      "2020-01-31 09:25:25,463 : INFO : PROGRESS: at sentence #160000, processed 26390910 words, keeping 118665 word types\n",
      "2020-01-31 09:25:25,747 : INFO : PROGRESS: at sentence #170000, processed 27913916 words, keeping 123350 word types\n",
      "2020-01-31 09:25:26,087 : INFO : PROGRESS: at sentence #180000, processed 29535612 words, keeping 126742 word types\n",
      "2020-01-31 09:25:26,515 : INFO : PROGRESS: at sentence #190000, processed 31096459 words, keeping 129841 word types\n",
      "2020-01-31 09:25:26,875 : INFO : PROGRESS: at sentence #200000, processed 32805271 words, keeping 133249 word types\n",
      "2020-01-31 09:25:27,198 : INFO : PROGRESS: at sentence #210000, processed 34434198 words, keeping 136358 word types\n",
      "2020-01-31 09:25:27,522 : INFO : PROGRESS: at sentence #220000, processed 36083482 words, keeping 139412 word types\n",
      "2020-01-31 09:25:27,835 : INFO : PROGRESS: at sentence #230000, processed 37571762 words, keeping 142393 word types\n",
      "2020-01-31 09:25:28,248 : INFO : PROGRESS: at sentence #240000, processed 39138190 words, keeping 145226 word types\n",
      "2020-01-31 09:25:28,578 : INFO : PROGRESS: at sentence #250000, processed 40695049 words, keeping 147960 word types\n",
      "2020-01-31 09:25:28,781 : INFO : collected 150053 word types from a corpus of 41519355 raw words and 255404 sentences\n",
      "2020-01-31 09:25:28,781 : INFO : Loading a fresh vocabulary\n",
      "2020-01-31 09:25:29,691 : INFO : effective_min_count=2 retains 70538 unique words (47% of original 150053, drops 79515)\n",
      "2020-01-31 09:25:29,695 : INFO : effective_min_count=2 leaves 41439840 word corpus (99% of original 41519355, drops 79515)\n",
      "2020-01-31 09:25:30,035 : INFO : deleting the raw counts dictionary of 150053 items\n",
      "2020-01-31 09:25:30,039 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2020-01-31 09:25:30,043 : INFO : downsampling leaves estimated 30349255 word corpus (73.2% of prior 41439840)\n",
      "2020-01-31 09:25:30,333 : INFO : estimated required memory for 70538 words and 150 dimensions: 119914600 bytes\n",
      "2020-01-31 09:25:30,333 : INFO : resetting layer weights\n",
      "2020-01-31 09:25:49,323 : INFO : training model with 10 workers on 70538 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-01-31 09:25:50,391 : INFO : EPOCH 1 - PROGRESS: at 2.10% examples, 635346 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:25:51,391 : INFO : EPOCH 1 - PROGRESS: at 4.28% examples, 641258 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:25:52,414 : INFO : EPOCH 1 - PROGRESS: at 6.46% examples, 648520 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:25:53,415 : INFO : EPOCH 1 - PROGRESS: at 8.48% examples, 642900 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:25:54,415 : INFO : EPOCH 1 - PROGRESS: at 10.27% examples, 645124 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:25:55,438 : INFO : EPOCH 1 - PROGRESS: at 11.98% examples, 643942 words/s, in_qsize 20, out_qsize 2\n",
      "2020-01-31 09:25:56,446 : INFO : EPOCH 1 - PROGRESS: at 13.79% examples, 641620 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:25:57,446 : INFO : EPOCH 1 - PROGRESS: at 15.78% examples, 645239 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:25:58,448 : INFO : EPOCH 1 - PROGRESS: at 17.66% examples, 650677 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:25:59,459 : INFO : EPOCH 1 - PROGRESS: at 19.46% examples, 653398 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:00,463 : INFO : EPOCH 1 - PROGRESS: at 21.10% examples, 652574 words/s, in_qsize 15, out_qsize 4\n",
      "2020-01-31 09:26:01,475 : INFO : EPOCH 1 - PROGRESS: at 23.27% examples, 656705 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:02,478 : INFO : EPOCH 1 - PROGRESS: at 24.99% examples, 653418 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:03,518 : INFO : EPOCH 1 - PROGRESS: at 27.07% examples, 645229 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:26:04,526 : INFO : EPOCH 1 - PROGRESS: at 29.34% examples, 643319 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:05,529 : INFO : EPOCH 1 - PROGRESS: at 31.59% examples, 641390 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:06,545 : INFO : EPOCH 1 - PROGRESS: at 33.86% examples, 642819 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:07,562 : INFO : EPOCH 1 - PROGRESS: at 36.20% examples, 643064 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:08,580 : INFO : EPOCH 1 - PROGRESS: at 38.36% examples, 641169 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:26:09,625 : INFO : EPOCH 1 - PROGRESS: at 40.63% examples, 639853 words/s, in_qsize 16, out_qsize 3\n",
      "2020-01-31 09:26:10,640 : INFO : EPOCH 1 - PROGRESS: at 43.03% examples, 640214 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:11,651 : INFO : EPOCH 1 - PROGRESS: at 45.53% examples, 640863 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:12,724 : INFO : EPOCH 1 - PROGRESS: at 47.78% examples, 640117 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:26:13,770 : INFO : EPOCH 1 - PROGRESS: at 50.29% examples, 641845 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:26:14,778 : INFO : EPOCH 1 - PROGRESS: at 52.54% examples, 642711 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:15,791 : INFO : EPOCH 1 - PROGRESS: at 54.75% examples, 643780 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:16,813 : INFO : EPOCH 1 - PROGRESS: at 56.96% examples, 642196 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:26:17,814 : INFO : EPOCH 1 - PROGRESS: at 59.23% examples, 642881 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:18,820 : INFO : EPOCH 1 - PROGRESS: at 61.35% examples, 641898 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:26:19,854 : INFO : EPOCH 1 - PROGRESS: at 63.84% examples, 642591 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:26:20,866 : INFO : EPOCH 1 - PROGRESS: at 66.09% examples, 642980 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:26:21,871 : INFO : EPOCH 1 - PROGRESS: at 68.16% examples, 641891 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:22,913 : INFO : EPOCH 1 - PROGRESS: at 70.25% examples, 641454 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:26:23,943 : INFO : EPOCH 1 - PROGRESS: at 72.28% examples, 639899 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:26:24,937 : INFO : EPOCH 1 - PROGRESS: at 74.59% examples, 640013 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:25,970 : INFO : EPOCH 1 - PROGRESS: at 76.56% examples, 639662 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:26:26,973 : INFO : EPOCH 1 - PROGRESS: at 78.62% examples, 639974 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:27,986 : INFO : EPOCH 1 - PROGRESS: at 80.76% examples, 640196 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:28,996 : INFO : EPOCH 1 - PROGRESS: at 83.00% examples, 640610 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:29,993 : INFO : EPOCH 1 - PROGRESS: at 85.20% examples, 641991 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:26:31,012 : INFO : EPOCH 1 - PROGRESS: at 87.47% examples, 641734 words/s, in_qsize 17, out_qsize 4\n",
      "2020-01-31 09:26:32,004 : INFO : EPOCH 1 - PROGRESS: at 89.91% examples, 642687 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:33,019 : INFO : EPOCH 1 - PROGRESS: at 92.19% examples, 642631 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:34,039 : INFO : EPOCH 1 - PROGRESS: at 94.38% examples, 642694 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:35,050 : INFO : EPOCH 1 - PROGRESS: at 96.49% examples, 641913 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:36,057 : INFO : EPOCH 1 - PROGRESS: at 98.69% examples, 641666 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:26:36,482 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:26:36,502 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:26:36,502 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:26:36,506 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:26:36,510 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:26:36,534 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:26:36,538 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:26:36,550 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:26:36,550 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:26:36,566 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:26:36,566 : INFO : EPOCH - 1 : training on 41519355 raw words (30350210 effective words) took 47.2s, 642696 effective words/s\n",
      "2020-01-31 09:26:37,598 : INFO : EPOCH 2 - PROGRESS: at 2.11% examples, 649202 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:26:38,595 : INFO : EPOCH 2 - PROGRESS: at 4.44% examples, 676808 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:39,610 : INFO : EPOCH 2 - PROGRESS: at 6.71% examples, 681702 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:26:40,646 : INFO : EPOCH 2 - PROGRESS: at 8.75% examples, 665190 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:41,650 : INFO : EPOCH 2 - PROGRESS: at 10.61% examples, 668554 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:26:42,681 : INFO : EPOCH 2 - PROGRESS: at 12.35% examples, 665886 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:43,707 : INFO : EPOCH 2 - PROGRESS: at 14.36% examples, 666194 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:44,717 : INFO : EPOCH 2 - PROGRESS: at 16.16% examples, 659645 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:45,722 : INFO : EPOCH 2 - PROGRESS: at 18.12% examples, 665560 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:26:46,739 : INFO : EPOCH 2 - PROGRESS: at 19.91% examples, 666179 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:47,760 : INFO : EPOCH 2 - PROGRESS: at 21.99% examples, 668331 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:48,758 : INFO : EPOCH 2 - PROGRESS: at 23.76% examples, 669536 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:49,765 : INFO : EPOCH 2 - PROGRESS: at 25.93% examples, 670364 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:26:50,796 : INFO : EPOCH 2 - PROGRESS: at 28.46% examples, 669441 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:51,823 : INFO : EPOCH 2 - PROGRESS: at 30.65% examples, 665152 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:52,826 : INFO : EPOCH 2 - PROGRESS: at 32.92% examples, 661721 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:53,834 : INFO : EPOCH 2 - PROGRESS: at 34.98% examples, 659652 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:54,860 : INFO : EPOCH 2 - PROGRESS: at 37.40% examples, 659936 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:55,878 : INFO : EPOCH 2 - PROGRESS: at 39.68% examples, 658417 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:26:56,875 : INFO : EPOCH 2 - PROGRESS: at 42.05% examples, 656683 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:26:57,887 : INFO : EPOCH 2 - PROGRESS: at 44.44% examples, 656835 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:26:58,931 : INFO : EPOCH 2 - PROGRESS: at 46.73% examples, 655387 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:26:59,938 : INFO : EPOCH 2 - PROGRESS: at 49.08% examples, 655661 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:00,927 : INFO : EPOCH 2 - PROGRESS: at 51.24% examples, 654933 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:01,937 : INFO : EPOCH 2 - PROGRESS: at 53.34% examples, 654693 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:02,959 : INFO : EPOCH 2 - PROGRESS: at 55.73% examples, 655034 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:04,001 : INFO : EPOCH 2 - PROGRESS: at 58.03% examples, 654747 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:05,004 : INFO : EPOCH 2 - PROGRESS: at 60.39% examples, 655760 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:06,090 : INFO : EPOCH 2 - PROGRESS: at 62.44% examples, 651835 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:27:07,093 : INFO : EPOCH 2 - PROGRESS: at 64.89% examples, 651361 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:27:08,100 : INFO : EPOCH 2 - PROGRESS: at 66.80% examples, 649674 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:09,101 : INFO : EPOCH 2 - PROGRESS: at 69.17% examples, 651132 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:10,136 : INFO : EPOCH 2 - PROGRESS: at 71.47% examples, 652324 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:11,152 : INFO : EPOCH 2 - PROGRESS: at 73.86% examples, 652759 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:12,163 : INFO : EPOCH 2 - PROGRESS: at 75.88% examples, 652064 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:13,175 : INFO : EPOCH 2 - PROGRESS: at 77.52% examples, 648519 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:14,172 : INFO : EPOCH 2 - PROGRESS: at 79.20% examples, 644781 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:15,179 : INFO : EPOCH 2 - PROGRESS: at 81.03% examples, 642931 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:27:16,203 : INFO : EPOCH 2 - PROGRESS: at 82.93% examples, 640468 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:17,234 : INFO : EPOCH 2 - PROGRESS: at 84.86% examples, 639205 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:18,281 : INFO : EPOCH 2 - PROGRESS: at 87.03% examples, 638201 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:19,297 : INFO : EPOCH 2 - PROGRESS: at 89.13% examples, 636687 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:20,324 : INFO : EPOCH 2 - PROGRESS: at 91.32% examples, 635977 words/s, in_qsize 16, out_qsize 3\n",
      "2020-01-31 09:27:21,328 : INFO : EPOCH 2 - PROGRESS: at 93.26% examples, 634733 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:22,339 : INFO : EPOCH 2 - PROGRESS: at 95.13% examples, 632413 words/s, in_qsize 16, out_qsize 7\n",
      "2020-01-31 09:27:23,332 : INFO : EPOCH 2 - PROGRESS: at 97.00% examples, 630513 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:24,348 : INFO : EPOCH 2 - PROGRESS: at 99.28% examples, 631001 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:24,571 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:27:24,593 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:27:24,597 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:27:24,609 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:27:24,621 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:27:24,625 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:27:24,629 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:27:24,633 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:27:24,637 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:27:24,657 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:27:24,661 : INFO : EPOCH - 2 : training on 41519355 raw words (30350930 effective words) took 48.1s, 631173 effective words/s\n",
      "2020-01-31 09:27:25,676 : INFO : EPOCH 3 - PROGRESS: at 2.04% examples, 638932 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:26,680 : INFO : EPOCH 3 - PROGRESS: at 3.75% examples, 577840 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:27,696 : INFO : EPOCH 3 - PROGRESS: at 5.82% examples, 593775 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:28,728 : INFO : EPOCH 3 - PROGRESS: at 7.54% examples, 575736 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:29,732 : INFO : EPOCH 3 - PROGRESS: at 8.99% examples, 553858 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:30,763 : INFO : EPOCH 3 - PROGRESS: at 10.28% examples, 540451 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:31,765 : INFO : EPOCH 3 - PROGRESS: at 11.74% examples, 542130 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:32,825 : INFO : EPOCH 3 - PROGRESS: at 13.56% examples, 547571 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:33,870 : INFO : EPOCH 3 - PROGRESS: at 15.20% examples, 547811 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:34,923 : INFO : EPOCH 3 - PROGRESS: at 16.66% examples, 541614 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:35,984 : INFO : EPOCH 3 - PROGRESS: at 18.09% examples, 537353 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:36,984 : INFO : EPOCH 3 - PROGRESS: at 19.68% examples, 543082 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:38,022 : INFO : EPOCH 3 - PROGRESS: at 21.57% examples, 547428 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:39,025 : INFO : EPOCH 3 - PROGRESS: at 23.33% examples, 556338 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:40,026 : INFO : EPOCH 3 - PROGRESS: at 25.01% examples, 559402 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:41,035 : INFO : EPOCH 3 - PROGRESS: at 27.24% examples, 561596 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:42,057 : INFO : EPOCH 3 - PROGRESS: at 29.66% examples, 567105 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:43,059 : INFO : EPOCH 3 - PROGRESS: at 31.97% examples, 569993 words/s, in_qsize 15, out_qsize 4\n",
      "2020-01-31 09:27:44,073 : INFO : EPOCH 3 - PROGRESS: at 34.18% examples, 574455 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:45,076 : INFO : EPOCH 3 - PROGRESS: at 35.97% examples, 571016 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:46,080 : INFO : EPOCH 3 - PROGRESS: at 38.01% examples, 571778 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:47,079 : INFO : EPOCH 3 - PROGRESS: at 40.16% examples, 572564 words/s, in_qsize 13, out_qsize 6\n",
      "2020-01-31 09:27:48,102 : INFO : EPOCH 3 - PROGRESS: at 42.60% examples, 576495 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:49,115 : INFO : EPOCH 3 - PROGRESS: at 44.91% examples, 577994 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:50,116 : INFO : EPOCH 3 - PROGRESS: at 47.20% examples, 581562 words/s, in_qsize 16, out_qsize 3\n",
      "2020-01-31 09:27:51,115 : INFO : EPOCH 3 - PROGRESS: at 49.34% examples, 582147 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:52,162 : INFO : EPOCH 3 - PROGRESS: at 51.62% examples, 584336 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:53,165 : INFO : EPOCH 3 - PROGRESS: at 53.24% examples, 581975 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:54,168 : INFO : EPOCH 3 - PROGRESS: at 55.20% examples, 581071 words/s, in_qsize 16, out_qsize 3\n",
      "2020-01-31 09:27:55,173 : INFO : EPOCH 3 - PROGRESS: at 57.44% examples, 582745 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:56,193 : INFO : EPOCH 3 - PROGRESS: at 59.77% examples, 585455 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:57,186 : INFO : EPOCH 3 - PROGRESS: at 62.03% examples, 587969 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:27:58,200 : INFO : EPOCH 3 - PROGRESS: at 64.57% examples, 590266 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:27:59,250 : INFO : EPOCH 3 - PROGRESS: at 66.63% examples, 590881 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:28:00,276 : INFO : EPOCH 3 - PROGRESS: at 68.78% examples, 591426 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:28:01,272 : INFO : EPOCH 3 - PROGRESS: at 70.89% examples, 593560 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:02,278 : INFO : EPOCH 3 - PROGRESS: at 72.92% examples, 593069 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:03,293 : INFO : EPOCH 3 - PROGRESS: at 75.07% examples, 593934 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:04,319 : INFO : EPOCH 3 - PROGRESS: at 77.03% examples, 594529 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:28:05,341 : INFO : EPOCH 3 - PROGRESS: at 79.27% examples, 596663 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:06,349 : INFO : EPOCH 3 - PROGRESS: at 81.39% examples, 598009 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:07,356 : INFO : EPOCH 3 - PROGRESS: at 83.67% examples, 599752 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:08,397 : INFO : EPOCH 3 - PROGRESS: at 85.73% examples, 600373 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:09,420 : INFO : EPOCH 3 - PROGRESS: at 88.12% examples, 601540 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:28:10,426 : INFO : EPOCH 3 - PROGRESS: at 90.45% examples, 602693 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:11,435 : INFO : EPOCH 3 - PROGRESS: at 92.80% examples, 604220 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:12,463 : INFO : EPOCH 3 - PROGRESS: at 95.09% examples, 605404 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:13,490 : INFO : EPOCH 3 - PROGRESS: at 97.46% examples, 606644 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:14,484 : INFO : EPOCH 3 - PROGRESS: at 99.70% examples, 607487 words/s, in_qsize 12, out_qsize 0\n",
      "2020-01-31 09:28:14,519 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:28:14,523 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:28:14,527 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:28:14,531 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:28:14,531 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:28:14,538 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:28:14,550 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:28:14,554 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:28:14,558 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:28:14,570 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:28:14,574 : INFO : EPOCH - 3 : training on 41519355 raw words (30349411 effective words) took 49.9s, 608183 effective words/s\n",
      "2020-01-31 09:28:15,597 : INFO : EPOCH 4 - PROGRESS: at 1.90% examples, 588463 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:16,608 : INFO : EPOCH 4 - PROGRESS: at 3.97% examples, 605826 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:17,628 : INFO : EPOCH 4 - PROGRESS: at 6.17% examples, 622492 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:18,637 : INFO : EPOCH 4 - PROGRESS: at 8.11% examples, 619113 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:19,647 : INFO : EPOCH 4 - PROGRESS: at 10.02% examples, 628632 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:20,660 : INFO : EPOCH 4 - PROGRESS: at 11.69% examples, 628048 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:21,671 : INFO : EPOCH 4 - PROGRESS: at 13.57% examples, 630684 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:22,678 : INFO : EPOCH 4 - PROGRESS: at 15.61% examples, 637785 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:23,684 : INFO : EPOCH 4 - PROGRESS: at 17.51% examples, 644795 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:24,691 : INFO : EPOCH 4 - PROGRESS: at 19.29% examples, 646562 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:25,699 : INFO : EPOCH 4 - PROGRESS: at 20.96% examples, 647291 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:28:26,744 : INFO : EPOCH 4 - PROGRESS: at 23.07% examples, 648108 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:28:27,761 : INFO : EPOCH 4 - PROGRESS: at 24.73% examples, 646360 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:28,833 : INFO : EPOCH 4 - PROGRESS: at 27.30% examples, 645760 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:29,873 : INFO : EPOCH 4 - PROGRESS: at 29.43% examples, 640464 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:28:30,875 : INFO : EPOCH 4 - PROGRESS: at 32.03% examples, 644472 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:28:31,884 : INFO : EPOCH 4 - PROGRESS: at 34.17% examples, 643884 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:32,915 : INFO : EPOCH 4 - PROGRESS: at 36.60% examples, 644809 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:33,949 : INFO : EPOCH 4 - PROGRESS: at 38.87% examples, 643789 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:28:34,937 : INFO : EPOCH 4 - PROGRESS: at 41.29% examples, 645120 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:35,938 : INFO : EPOCH 4 - PROGRESS: at 43.60% examples, 644868 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:37,010 : INFO : EPOCH 4 - PROGRESS: at 46.29% examples, 647226 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:38,007 : INFO : EPOCH 4 - PROGRESS: at 48.74% examples, 650075 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:39,074 : INFO : EPOCH 4 - PROGRESS: at 51.18% examples, 650594 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:40,094 : INFO : EPOCH 4 - PROGRESS: at 53.44% examples, 652028 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:41,136 : INFO : EPOCH 4 - PROGRESS: at 55.95% examples, 653052 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:28:42,172 : INFO : EPOCH 4 - PROGRESS: at 58.38% examples, 654135 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:43,188 : INFO : EPOCH 4 - PROGRESS: at 60.90% examples, 656235 words/s, in_qsize 16, out_qsize 3\n",
      "2020-01-31 09:28:44,213 : INFO : EPOCH 4 - PROGRESS: at 63.21% examples, 656361 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:45,285 : INFO : EPOCH 4 - PROGRESS: at 65.85% examples, 657700 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:46,303 : INFO : EPOCH 4 - PROGRESS: at 68.29% examples, 659109 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:47,326 : INFO : EPOCH 4 - PROGRESS: at 70.51% examples, 659713 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:48,340 : INFO : EPOCH 4 - PROGRESS: at 72.72% examples, 659416 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:28:49,343 : INFO : EPOCH 4 - PROGRESS: at 75.06% examples, 659885 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:28:50,341 : INFO : EPOCH 4 - PROGRESS: at 77.07% examples, 659313 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:51,397 : INFO : EPOCH 4 - PROGRESS: at 79.32% examples, 659557 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:28:52,396 : INFO : EPOCH 4 - PROGRESS: at 81.39% examples, 659221 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:53,398 : INFO : EPOCH 4 - PROGRESS: at 83.71% examples, 659963 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:28:54,401 : INFO : EPOCH 4 - PROGRESS: at 85.94% examples, 660550 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:55,427 : INFO : EPOCH 4 - PROGRESS: at 88.54% examples, 661799 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:56,425 : INFO : EPOCH 4 - PROGRESS: at 90.88% examples, 661830 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:57,463 : INFO : EPOCH 4 - PROGRESS: at 93.30% examples, 662770 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:28:58,484 : INFO : EPOCH 4 - PROGRESS: at 95.70% examples, 662954 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:28:59,501 : INFO : EPOCH 4 - PROGRESS: at 98.08% examples, 663327 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:00,196 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:29:00,212 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:29:00,232 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:29:00,236 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:29:00,252 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:29:00,264 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:29:00,264 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:29:00,272 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:29:00,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:29:00,288 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:29:00,288 : INFO : EPOCH - 4 : training on 41519355 raw words (30349294 effective words) took 45.7s, 663984 effective words/s\n",
      "2020-01-31 09:29:01,323 : INFO : EPOCH 5 - PROGRESS: at 2.04% examples, 619309 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:02,350 : INFO : EPOCH 5 - PROGRESS: at 4.42% examples, 664455 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:03,349 : INFO : EPOCH 5 - PROGRESS: at 6.63% examples, 668210 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:04,359 : INFO : EPOCH 5 - PROGRESS: at 8.59% examples, 653371 words/s, in_qsize 15, out_qsize 4\n",
      "2020-01-31 09:29:05,388 : INFO : EPOCH 5 - PROGRESS: at 10.37% examples, 652204 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:06,425 : INFO : EPOCH 5 - PROGRESS: at 12.22% examples, 657032 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:07,433 : INFO : EPOCH 5 - PROGRESS: at 14.14% examples, 653692 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:29:08,457 : INFO : EPOCH 5 - PROGRESS: at 16.23% examples, 659518 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:09,472 : INFO : EPOCH 5 - PROGRESS: at 18.19% examples, 666170 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:10,474 : INFO : EPOCH 5 - PROGRESS: at 19.95% examples, 666073 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:11,490 : INFO : EPOCH 5 - PROGRESS: at 22.01% examples, 667871 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:12,527 : INFO : EPOCH 5 - PROGRESS: at 23.85% examples, 670083 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:13,519 : INFO : EPOCH 5 - PROGRESS: at 26.16% examples, 672441 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:14,542 : INFO : EPOCH 5 - PROGRESS: at 28.65% examples, 672097 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:15,553 : INFO : EPOCH 5 - PROGRESS: at 31.06% examples, 671366 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:16,568 : INFO : EPOCH 5 - PROGRESS: at 33.40% examples, 669892 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:17,563 : INFO : EPOCH 5 - PROGRESS: at 35.84% examples, 672995 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:18,577 : INFO : EPOCH 5 - PROGRESS: at 38.27% examples, 673049 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:29:19,580 : INFO : EPOCH 5 - PROGRESS: at 40.55% examples, 671764 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:20,577 : INFO : EPOCH 5 - PROGRESS: at 43.01% examples, 671590 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:21,589 : INFO : EPOCH 5 - PROGRESS: at 45.44% examples, 670323 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:29:22,592 : INFO : EPOCH 5 - PROGRESS: at 47.58% examples, 668368 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:23,613 : INFO : EPOCH 5 - PROGRESS: at 49.91% examples, 667774 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:29:24,643 : INFO : EPOCH 5 - PROGRESS: at 52.32% examples, 668629 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:25,651 : INFO : EPOCH 5 - PROGRESS: at 54.55% examples, 668917 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:26,685 : INFO : EPOCH 5 - PROGRESS: at 57.09% examples, 669855 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:27,689 : INFO : EPOCH 5 - PROGRESS: at 59.41% examples, 670096 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:28,699 : INFO : EPOCH 5 - PROGRESS: at 61.69% examples, 669740 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:29,717 : INFO : EPOCH 5 - PROGRESS: at 64.41% examples, 671255 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:30,741 : INFO : EPOCH 5 - PROGRESS: at 66.73% examples, 672133 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:29:31,746 : INFO : EPOCH 5 - PROGRESS: at 69.12% examples, 673030 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:29:32,759 : INFO : EPOCH 5 - PROGRESS: at 71.33% examples, 673171 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:29:33,785 : INFO : EPOCH 5 - PROGRESS: at 73.85% examples, 674022 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:34,777 : INFO : EPOCH 5 - PROGRESS: at 76.05% examples, 674479 words/s, in_qsize 16, out_qsize 3\n",
      "2020-01-31 09:29:35,794 : INFO : EPOCH 5 - PROGRESS: at 78.07% examples, 673690 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:36,819 : INFO : EPOCH 5 - PROGRESS: at 80.29% examples, 673231 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:37,852 : INFO : EPOCH 5 - PROGRESS: at 82.54% examples, 672718 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:29:38,851 : INFO : EPOCH 5 - PROGRESS: at 84.67% examples, 672587 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:39,872 : INFO : EPOCH 5 - PROGRESS: at 86.93% examples, 671732 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:29:40,875 : INFO : EPOCH 5 - PROGRESS: at 89.45% examples, 672623 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:41,900 : INFO : EPOCH 5 - PROGRESS: at 91.98% examples, 673207 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:42,914 : INFO : EPOCH 5 - PROGRESS: at 94.38% examples, 674001 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:43,913 : INFO : EPOCH 5 - PROGRESS: at 96.71% examples, 674275 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:44,928 : INFO : EPOCH 5 - PROGRESS: at 99.06% examples, 673967 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:29:45,178 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:29:45,194 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:29:45,224 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:29:45,248 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:29:45,248 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:29:45,260 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:29:45,264 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:29:45,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:29:45,268 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:29:45,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:29:45,272 : INFO : EPOCH - 5 : training on 41519355 raw words (30348231 effective words) took 45.0s, 674809 effective words/s\n",
      "2020-01-31 09:29:45,272 : INFO : training on a 207596775 raw words (151748076 effective words) took 235.9s, 643178 effective words/s\n",
      "2020-01-31 09:29:45,272 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-01-31 09:29:45,272 : INFO : training model with 10 workers on 70538 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-01-31 09:29:46,278 : INFO : EPOCH 1 - PROGRESS: at 2.18% examples, 689110 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:47,289 : INFO : EPOCH 1 - PROGRESS: at 4.48% examples, 689187 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:29:48,295 : INFO : EPOCH 1 - PROGRESS: at 6.78% examples, 694282 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:29:49,309 : INFO : EPOCH 1 - PROGRESS: at 8.93% examples, 690473 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:50,338 : INFO : EPOCH 1 - PROGRESS: at 10.85% examples, 691474 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:51,338 : INFO : EPOCH 1 - PROGRESS: at 12.64% examples, 689522 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:29:52,339 : INFO : EPOCH 1 - PROGRESS: at 14.57% examples, 682130 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:53,395 : INFO : EPOCH 1 - PROGRESS: at 16.51% examples, 677327 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:54,399 : INFO : EPOCH 1 - PROGRESS: at 18.36% examples, 677085 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:55,414 : INFO : EPOCH 1 - PROGRESS: at 20.06% examples, 674275 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:29:56,435 : INFO : EPOCH 1 - PROGRESS: at 22.18% examples, 676302 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:29:57,437 : INFO : EPOCH 1 - PROGRESS: at 23.93% examples, 677401 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:58,441 : INFO : EPOCH 1 - PROGRESS: at 26.17% examples, 676662 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:29:59,438 : INFO : EPOCH 1 - PROGRESS: at 28.82% examples, 679876 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:00,440 : INFO : EPOCH 1 - PROGRESS: at 31.22% examples, 678651 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:01,442 : INFO : EPOCH 1 - PROGRESS: at 33.67% examples, 680123 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:02,457 : INFO : EPOCH 1 - PROGRESS: at 36.22% examples, 682236 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:03,472 : INFO : EPOCH 1 - PROGRESS: at 38.75% examples, 683383 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:04,474 : INFO : EPOCH 1 - PROGRESS: at 41.25% examples, 683829 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:05,489 : INFO : EPOCH 1 - PROGRESS: at 43.68% examples, 683155 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:06,496 : INFO : EPOCH 1 - PROGRESS: at 46.27% examples, 684138 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:30:07,510 : INFO : EPOCH 1 - PROGRESS: at 48.54% examples, 683094 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:08,517 : INFO : EPOCH 1 - PROGRESS: at 50.90% examples, 682371 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:09,540 : INFO : EPOCH 1 - PROGRESS: at 53.05% examples, 681069 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:10,559 : INFO : EPOCH 1 - PROGRESS: at 55.40% examples, 680117 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:11,588 : INFO : EPOCH 1 - PROGRESS: at 57.77% examples, 679597 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:30:12,613 : INFO : EPOCH 1 - PROGRESS: at 60.12% examples, 679235 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:30:13,624 : INFO : EPOCH 1 - PROGRESS: at 62.56% examples, 679802 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:14,652 : INFO : EPOCH 1 - PROGRESS: at 65.17% examples, 679873 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:15,651 : INFO : EPOCH 1 - PROGRESS: at 67.50% examples, 680819 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:16,658 : INFO : EPOCH 1 - PROGRESS: at 69.76% examples, 680780 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:30:17,701 : INFO : EPOCH 1 - PROGRESS: at 72.02% examples, 680831 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:18,707 : INFO : EPOCH 1 - PROGRESS: at 74.55% examples, 681491 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:19,721 : INFO : EPOCH 1 - PROGRESS: at 76.69% examples, 681119 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:20,729 : INFO : EPOCH 1 - PROGRESS: at 78.97% examples, 682360 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:30:21,758 : INFO : EPOCH 1 - PROGRESS: at 81.24% examples, 682419 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:22,790 : INFO : EPOCH 1 - PROGRESS: at 83.67% examples, 682720 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:30:23,833 : INFO : EPOCH 1 - PROGRESS: at 85.96% examples, 682786 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:24,832 : INFO : EPOCH 1 - PROGRESS: at 88.25% examples, 681529 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:30:25,871 : INFO : EPOCH 1 - PROGRESS: at 90.66% examples, 680815 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:26,878 : INFO : EPOCH 1 - PROGRESS: at 92.88% examples, 680195 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:27,873 : INFO : EPOCH 1 - PROGRESS: at 95.06% examples, 679019 words/s, in_qsize 16, out_qsize 4\n",
      "2020-01-31 09:30:28,892 : INFO : EPOCH 1 - PROGRESS: at 97.51% examples, 679503 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:29,846 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:30:29,862 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:30:29,862 : INFO : worker thread finished; awaiting finish of 7 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:30:29,874 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:30:29,878 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:30:29,882 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:30:29,886 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:30:29,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:30:29,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:30:29,910 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 680126 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-31 09:30:29,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:30:29,910 : INFO : EPOCH - 1 : training on 41519355 raw words (30350658 effective words) took 44.6s, 680079 effective words/s\n",
      "2020-01-31 09:30:30,909 : INFO : EPOCH 2 - PROGRESS: at 2.17% examples, 684860 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:31,923 : INFO : EPOCH 2 - PROGRESS: at 4.40% examples, 674665 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:32,944 : INFO : EPOCH 2 - PROGRESS: at 6.74% examples, 685604 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:33,969 : INFO : EPOCH 2 - PROGRESS: at 8.95% examples, 686859 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:34,982 : INFO : EPOCH 2 - PROGRESS: at 10.83% examples, 686162 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:36,042 : INFO : EPOCH 2 - PROGRESS: at 12.67% examples, 684055 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:37,067 : INFO : EPOCH 2 - PROGRESS: at 14.84% examples, 688078 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:38,066 : INFO : EPOCH 2 - PROGRESS: at 16.80% examples, 687547 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:30:39,092 : INFO : EPOCH 2 - PROGRESS: at 18.72% examples, 687331 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:40,109 : INFO : EPOCH 2 - PROGRESS: at 20.40% examples, 682819 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:30:41,127 : INFO : EPOCH 2 - PROGRESS: at 22.41% examples, 680534 words/s, in_qsize 20, out_qsize 2\n",
      "2020-01-31 09:30:42,133 : INFO : EPOCH 2 - PROGRESS: at 24.11% examples, 679249 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:43,143 : INFO : EPOCH 2 - PROGRESS: at 26.29% examples, 675584 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:30:44,157 : INFO : EPOCH 2 - PROGRESS: at 28.94% examples, 677677 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:45,171 : INFO : EPOCH 2 - PROGRESS: at 31.44% examples, 678363 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:46,205 : INFO : EPOCH 2 - PROGRESS: at 33.84% examples, 678303 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:30:47,213 : INFO : EPOCH 2 - PROGRESS: at 36.40% examples, 680061 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:48,219 : INFO : EPOCH 2 - PROGRESS: at 38.83% examples, 681017 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:49,213 : INFO : EPOCH 2 - PROGRESS: at 41.36% examples, 681523 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:30:50,261 : INFO : EPOCH 2 - PROGRESS: at 43.91% examples, 681594 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:51,260 : INFO : EPOCH 2 - PROGRESS: at 46.34% examples, 680945 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:52,308 : INFO : EPOCH 2 - PROGRESS: at 48.79% examples, 681069 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:53,327 : INFO : EPOCH 2 - PROGRESS: at 51.26% examples, 682124 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:54,358 : INFO : EPOCH 2 - PROGRESS: at 53.48% examples, 681541 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:55,360 : INFO : EPOCH 2 - PROGRESS: at 55.96% examples, 682026 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:56,353 : INFO : EPOCH 2 - PROGRESS: at 58.20% examples, 681061 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:57,392 : INFO : EPOCH 2 - PROGRESS: at 60.40% examples, 678442 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:30:58,420 : INFO : EPOCH 2 - PROGRESS: at 62.49% examples, 675532 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:30:59,453 : INFO : EPOCH 2 - PROGRESS: at 64.96% examples, 673894 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:00,453 : INFO : EPOCH 2 - PROGRESS: at 67.15% examples, 674115 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:01,456 : INFO : EPOCH 2 - PROGRESS: at 69.58% examples, 675914 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:02,451 : INFO : EPOCH 2 - PROGRESS: at 71.72% examples, 675404 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:03,496 : INFO : EPOCH 2 - PROGRESS: at 74.28% examples, 676001 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:04,506 : INFO : EPOCH 2 - PROGRESS: at 76.46% examples, 676763 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:31:05,503 : INFO : EPOCH 2 - PROGRESS: at 78.45% examples, 675098 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:06,529 : INFO : EPOCH 2 - PROGRESS: at 80.29% examples, 671635 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:31:07,534 : INFO : EPOCH 2 - PROGRESS: at 82.48% examples, 671158 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:08,548 : INFO : EPOCH 2 - PROGRESS: at 84.51% examples, 670010 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:09,560 : INFO : EPOCH 2 - PROGRESS: at 86.64% examples, 668702 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:10,564 : INFO : EPOCH 2 - PROGRESS: at 88.91% examples, 667630 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:11,620 : INFO : EPOCH 2 - PROGRESS: at 91.06% examples, 665450 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:12,611 : INFO : EPOCH 2 - PROGRESS: at 93.06% examples, 663858 words/s, in_qsize 16, out_qsize 3\n",
      "2020-01-31 09:31:13,647 : INFO : EPOCH 2 - PROGRESS: at 95.33% examples, 663037 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:14,651 : INFO : EPOCH 2 - PROGRESS: at 97.72% examples, 663861 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:15,572 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:31:15,603 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:31:15,603 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:31:15,603 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:31:15,603 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:31:15,621 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:31:15,621 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:31:15,625 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:31:15,633 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:31:15,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:31:15,645 : INFO : EPOCH - 2 : training on 41519355 raw words (30348160 effective words) took 45.7s, 663719 effective words/s\n",
      "2020-01-31 09:31:16,655 : INFO : EPOCH 3 - PROGRESS: at 1.88% examples, 592711 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:31:17,668 : INFO : EPOCH 3 - PROGRESS: at 4.17% examples, 635145 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:18,711 : INFO : EPOCH 3 - PROGRESS: at 6.40% examples, 644544 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:19,723 : INFO : EPOCH 3 - PROGRESS: at 8.43% examples, 641249 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:20,748 : INFO : EPOCH 3 - PROGRESS: at 10.14% examples, 634853 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:21,779 : INFO : EPOCH 3 - PROGRESS: at 11.73% examples, 626830 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:22,779 : INFO : EPOCH 3 - PROGRESS: at 13.60% examples, 628651 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:23,779 : INFO : EPOCH 3 - PROGRESS: at 15.45% examples, 629227 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:24,828 : INFO : EPOCH 3 - PROGRESS: at 17.25% examples, 629028 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:25,831 : INFO : EPOCH 3 - PROGRESS: at 18.94% examples, 628209 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:26,851 : INFO : EPOCH 3 - PROGRESS: at 20.55% examples, 628031 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:31:27,931 : INFO : EPOCH 3 - PROGRESS: at 22.35% examples, 619582 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:28,931 : INFO : EPOCH 3 - PROGRESS: at 23.90% examples, 618951 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:29,935 : INFO : EPOCH 3 - PROGRESS: at 25.82% examples, 617248 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:31:30,990 : INFO : EPOCH 3 - PROGRESS: at 28.40% examples, 619403 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:32,014 : INFO : EPOCH 3 - PROGRESS: at 30.79% examples, 621861 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:33,072 : INFO : EPOCH 3 - PROGRESS: at 33.32% examples, 624240 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:34,082 : INFO : EPOCH 3 - PROGRESS: at 35.74% examples, 629194 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:35,096 : INFO : EPOCH 3 - PROGRESS: at 38.11% examples, 630561 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:36,106 : INFO : EPOCH 3 - PROGRESS: at 40.49% examples, 632805 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:31:37,119 : INFO : EPOCH 3 - PROGRESS: at 43.21% examples, 637191 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:38,140 : INFO : EPOCH 3 - PROGRESS: at 45.81% examples, 639703 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:31:39,183 : INFO : EPOCH 3 - PROGRESS: at 48.19% examples, 640992 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:40,185 : INFO : EPOCH 3 - PROGRESS: at 50.66% examples, 643323 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:41,198 : INFO : EPOCH 3 - PROGRESS: at 52.99% examples, 645864 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:31:42,216 : INFO : EPOCH 3 - PROGRESS: at 55.33% examples, 646766 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:31:43,248 : INFO : EPOCH 3 - PROGRESS: at 57.78% examples, 647884 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:44,265 : INFO : EPOCH 3 - PROGRESS: at 60.05% examples, 648121 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:31:45,295 : INFO : EPOCH 3 - PROGRESS: at 62.27% examples, 647462 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:46,297 : INFO : EPOCH 3 - PROGRESS: at 64.81% examples, 648138 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:47,334 : INFO : EPOCH 3 - PROGRESS: at 66.97% examples, 648037 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:48,372 : INFO : EPOCH 3 - PROGRESS: at 69.43% examples, 649992 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:49,410 : INFO : EPOCH 3 - PROGRESS: at 71.76% examples, 651195 words/s, in_qsize 15, out_qsize 7\n",
      "2020-01-31 09:31:50,437 : INFO : EPOCH 3 - PROGRESS: at 74.35% examples, 653090 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:51,448 : INFO : EPOCH 3 - PROGRESS: at 76.42% examples, 653592 words/s, in_qsize 20, out_qsize 2\n",
      "2020-01-31 09:31:52,505 : INFO : EPOCH 3 - PROGRESS: at 78.81% examples, 654974 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:53,509 : INFO : EPOCH 3 - PROGRESS: at 81.05% examples, 656050 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:54,511 : INFO : EPOCH 3 - PROGRESS: at 83.42% examples, 656971 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:55,532 : INFO : EPOCH 3 - PROGRESS: at 85.75% examples, 658446 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:56,527 : INFO : EPOCH 3 - PROGRESS: at 88.24% examples, 659407 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:31:57,533 : INFO : EPOCH 3 - PROGRESS: at 90.76% examples, 660568 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:58,527 : INFO : EPOCH 3 - PROGRESS: at 93.04% examples, 660882 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:31:59,528 : INFO : EPOCH 3 - PROGRESS: at 95.47% examples, 661605 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:00,558 : INFO : EPOCH 3 - PROGRESS: at 97.54% examples, 660282 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:01,526 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:32:01,573 : INFO : EPOCH 3 - PROGRESS: at 99.80% examples, 659761 words/s, in_qsize 7, out_qsize 4\n",
      "2020-01-31 09:32:01,573 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:32:01,573 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:32:01,577 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:32:01,581 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:32:01,585 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:32:01,585 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:32:01,589 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:32:01,593 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:32:01,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:32:01,597 : INFO : EPOCH - 3 : training on 41519355 raw words (30351868 effective words) took 45.9s, 660639 effective words/s\n",
      "2020-01-31 09:32:02,620 : INFO : EPOCH 4 - PROGRESS: at 1.93% examples, 600418 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:03,626 : INFO : EPOCH 4 - PROGRESS: at 4.24% examples, 646732 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:04,640 : INFO : EPOCH 4 - PROGRESS: at 6.58% examples, 667908 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:05,656 : INFO : EPOCH 4 - PROGRESS: at 8.79% examples, 672783 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:06,654 : INFO : EPOCH 4 - PROGRESS: at 10.66% examples, 676023 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:07,677 : INFO : EPOCH 4 - PROGRESS: at 12.48% examples, 678190 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:08,699 : INFO : EPOCH 4 - PROGRESS: at 14.65% examples, 682946 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:09,697 : INFO : EPOCH 4 - PROGRESS: at 16.64% examples, 684584 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:10,731 : INFO : EPOCH 4 - PROGRESS: at 18.57% examples, 684799 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:32:11,782 : INFO : EPOCH 4 - PROGRESS: at 20.37% examples, 682778 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:32:12,812 : INFO : EPOCH 4 - PROGRESS: at 22.55% examples, 684954 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:13,835 : INFO : EPOCH 4 - PROGRESS: at 24.43% examples, 688077 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:14,841 : INFO : EPOCH 4 - PROGRESS: at 26.87% examples, 686672 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:15,851 : INFO : EPOCH 4 - PROGRESS: at 29.38% examples, 686070 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:16,868 : INFO : EPOCH 4 - PROGRESS: at 31.91% examples, 685257 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:32:17,870 : INFO : EPOCH 4 - PROGRESS: at 34.05% examples, 682887 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:18,883 : INFO : EPOCH 4 - PROGRESS: at 36.33% examples, 679642 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:19,915 : INFO : EPOCH 4 - PROGRESS: at 38.74% examples, 679149 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:20,943 : INFO : EPOCH 4 - PROGRESS: at 41.25% examples, 678887 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:21,950 : INFO : EPOCH 4 - PROGRESS: at 43.84% examples, 680632 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:32:22,991 : INFO : EPOCH 4 - PROGRESS: at 46.40% examples, 680605 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:23,991 : INFO : EPOCH 4 - PROGRESS: at 48.84% examples, 681652 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:32:24,991 : INFO : EPOCH 4 - PROGRESS: at 51.22% examples, 681899 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:26,005 : INFO : EPOCH 4 - PROGRESS: at 53.48% examples, 682337 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:27,041 : INFO : EPOCH 4 - PROGRESS: at 55.93% examples, 681736 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:28,041 : INFO : EPOCH 4 - PROGRESS: at 58.39% examples, 683217 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:29,074 : INFO : EPOCH 4 - PROGRESS: at 60.76% examples, 682467 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:32:30,108 : INFO : EPOCH 4 - PROGRESS: at 63.38% examples, 683858 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:31,136 : INFO : EPOCH 4 - PROGRESS: at 65.79% examples, 683129 words/s, in_qsize 20, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:32:32,138 : INFO : EPOCH 4 - PROGRESS: at 67.98% examples, 681725 words/s, in_qsize 13, out_qsize 6\n",
      "2020-01-31 09:32:33,155 : INFO : EPOCH 4 - PROGRESS: at 69.96% examples, 679521 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:34,157 : INFO : EPOCH 4 - PROGRESS: at 72.10% examples, 678520 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:35,194 : INFO : EPOCH 4 - PROGRESS: at 74.47% examples, 677535 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:32:36,196 : INFO : EPOCH 4 - PROGRESS: at 76.36% examples, 675557 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:37,196 : INFO : EPOCH 4 - PROGRESS: at 78.56% examples, 675833 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:38,200 : INFO : EPOCH 4 - PROGRESS: at 80.78% examples, 675949 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:32:39,249 : INFO : EPOCH 4 - PROGRESS: at 83.03% examples, 675030 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:32:40,248 : INFO : EPOCH 4 - PROGRESS: at 85.22% examples, 675292 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:41,286 : INFO : EPOCH 4 - PROGRESS: at 87.75% examples, 675482 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:42,294 : INFO : EPOCH 4 - PROGRESS: at 90.24% examples, 676122 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:43,331 : INFO : EPOCH 4 - PROGRESS: at 92.51% examples, 674952 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:44,365 : INFO : EPOCH 4 - PROGRESS: at 94.86% examples, 675113 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:32:45,382 : INFO : EPOCH 4 - PROGRESS: at 97.29% examples, 675314 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:46,378 : INFO : EPOCH 4 - PROGRESS: at 99.50% examples, 674533 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:46,497 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:32:46,512 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:32:46,512 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:32:46,527 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:32:46,531 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:32:46,535 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:32:46,539 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:32:46,547 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:32:46,551 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:32:46,567 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:32:46,567 : INFO : EPOCH - 4 : training on 41519355 raw words (30348278 effective words) took 45.0s, 675023 effective words/s\n",
      "2020-01-31 09:32:47,615 : INFO : EPOCH 5 - PROGRESS: at 1.98% examples, 596709 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:48,618 : INFO : EPOCH 5 - PROGRESS: at 4.12% examples, 621776 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:49,628 : INFO : EPOCH 5 - PROGRESS: at 6.07% examples, 612451 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:50,634 : INFO : EPOCH 5 - PROGRESS: at 8.23% examples, 626276 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:51,645 : INFO : EPOCH 5 - PROGRESS: at 10.01% examples, 628452 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:52,651 : INFO : EPOCH 5 - PROGRESS: at 11.90% examples, 641001 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:53,691 : INFO : EPOCH 5 - PROGRESS: at 13.95% examples, 647316 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:54,720 : INFO : EPOCH 5 - PROGRESS: at 16.04% examples, 652449 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:55,742 : INFO : EPOCH 5 - PROGRESS: at 17.97% examples, 658173 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:56,772 : INFO : EPOCH 5 - PROGRESS: at 19.79% examples, 659609 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:32:57,778 : INFO : EPOCH 5 - PROGRESS: at 21.92% examples, 664438 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:58,778 : INFO : EPOCH 5 - PROGRESS: at 23.69% examples, 665323 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:32:59,787 : INFO : EPOCH 5 - PROGRESS: at 25.70% examples, 663850 words/s, in_qsize 14, out_qsize 5\n",
      "2020-01-31 09:33:00,827 : INFO : EPOCH 5 - PROGRESS: at 28.28% examples, 664274 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:01,829 : INFO : EPOCH 5 - PROGRESS: at 30.74% examples, 665883 words/s, in_qsize 19, out_qsize 5\n",
      "2020-01-31 09:33:02,837 : INFO : EPOCH 5 - PROGRESS: at 33.31% examples, 668006 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:03,869 : INFO : EPOCH 5 - PROGRESS: at 35.59% examples, 667740 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:04,879 : INFO : EPOCH 5 - PROGRESS: at 37.89% examples, 666691 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:05,913 : INFO : EPOCH 5 - PROGRESS: at 40.24% examples, 664539 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:33:06,907 : INFO : EPOCH 5 - PROGRESS: at 42.85% examples, 667415 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:07,939 : INFO : EPOCH 5 - PROGRESS: at 45.28% examples, 665699 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:08,974 : INFO : EPOCH 5 - PROGRESS: at 47.65% examples, 666218 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:09,981 : INFO : EPOCH 5 - PROGRESS: at 50.06% examples, 666492 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:10,977 : INFO : EPOCH 5 - PROGRESS: at 52.36% examples, 667430 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:33:11,987 : INFO : EPOCH 5 - PROGRESS: at 54.69% examples, 669464 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:12,981 : INFO : EPOCH 5 - PROGRESS: at 57.15% examples, 669519 words/s, in_qsize 15, out_qsize 4\n",
      "2020-01-31 09:33:14,003 : INFO : EPOCH 5 - PROGRESS: at 59.57% examples, 670627 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:33:14,994 : INFO : EPOCH 5 - PROGRESS: at 61.88% examples, 670716 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:16,029 : INFO : EPOCH 5 - PROGRESS: at 64.60% examples, 671747 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:33:17,071 : INFO : EPOCH 5 - PROGRESS: at 66.95% examples, 672924 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:18,069 : INFO : EPOCH 5 - PROGRESS: at 69.28% examples, 673402 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:33:19,087 : INFO : EPOCH 5 - PROGRESS: at 71.57% examples, 674177 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:20,119 : INFO : EPOCH 5 - PROGRESS: at 74.06% examples, 674271 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:21,146 : INFO : EPOCH 5 - PROGRESS: at 76.13% examples, 673511 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:22,168 : INFO : EPOCH 5 - PROGRESS: at 78.23% examples, 672822 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:23,192 : INFO : EPOCH 5 - PROGRESS: at 80.37% examples, 671902 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:33:24,195 : INFO : EPOCH 5 - PROGRESS: at 82.70% examples, 672539 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:25,222 : INFO : EPOCH 5 - PROGRESS: at 84.89% examples, 672728 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:33:26,220 : INFO : EPOCH 5 - PROGRESS: at 87.25% examples, 672654 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:27,226 : INFO : EPOCH 5 - PROGRESS: at 89.82% examples, 673824 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:28,255 : INFO : EPOCH 5 - PROGRESS: at 92.29% examples, 674018 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:29,279 : INFO : EPOCH 5 - PROGRESS: at 94.70% examples, 674864 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:33:30,307 : INFO : EPOCH 5 - PROGRESS: at 97.14% examples, 675187 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:33:31,329 : INFO : EPOCH 5 - PROGRESS: at 99.68% examples, 676022 words/s, in_qsize 13, out_qsize 0\n",
      "2020-01-31 09:33:31,406 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:33:31,410 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:33:31,426 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:33:31,426 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:33:31,430 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:33:31,434 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:33:31,446 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:33:31,454 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:33:31,458 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:33:31,458 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:33:31,462 : INFO : EPOCH - 5 : training on 41519355 raw words (30346305 effective words) took 44.9s, 676045 effective words/s\n",
      "2020-01-31 09:33:32,471 : INFO : EPOCH 6 - PROGRESS: at 2.09% examples, 654000 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:33,472 : INFO : EPOCH 6 - PROGRESS: at 4.27% examples, 655317 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:33:34,507 : INFO : EPOCH 6 - PROGRESS: at 6.60% examples, 669048 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:33:35,539 : INFO : EPOCH 6 - PROGRESS: at 8.81% examples, 671035 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:36,598 : INFO : EPOCH 6 - PROGRESS: at 10.69% examples, 666576 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:37,619 : INFO : EPOCH 6 - PROGRESS: at 12.47% examples, 669782 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:38,625 : INFO : EPOCH 6 - PROGRESS: at 14.50% examples, 670157 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:39,635 : INFO : EPOCH 6 - PROGRESS: at 16.37% examples, 666507 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:40,638 : INFO : EPOCH 6 - PROGRESS: at 18.18% examples, 666389 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:41,668 : INFO : EPOCH 6 - PROGRESS: at 20.10% examples, 670911 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:33:42,698 : INFO : EPOCH 6 - PROGRESS: at 22.20% examples, 671428 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:33:43,762 : INFO : EPOCH 6 - PROGRESS: at 24.05% examples, 672884 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:33:44,808 : INFO : EPOCH 6 - PROGRESS: at 26.60% examples, 675882 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:45,828 : INFO : EPOCH 6 - PROGRESS: at 29.30% examples, 678853 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:46,842 : INFO : EPOCH 6 - PROGRESS: at 31.85% examples, 678958 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:47,871 : INFO : EPOCH 6 - PROGRESS: at 34.18% examples, 679443 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:48,888 : INFO : EPOCH 6 - PROGRESS: at 36.72% examples, 680792 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:33:49,936 : INFO : EPOCH 6 - PROGRESS: at 39.21% examples, 680024 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:33:50,950 : INFO : EPOCH 6 - PROGRESS: at 41.77% examples, 680696 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:51,958 : INFO : EPOCH 6 - PROGRESS: at 44.21% examples, 680451 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:52,960 : INFO : EPOCH 6 - PROGRESS: at 46.52% examples, 679176 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:53,959 : INFO : EPOCH 6 - PROGRESS: at 48.79% examples, 677662 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:54,966 : INFO : EPOCH 6 - PROGRESS: at 51.11% examples, 677232 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:55,999 : INFO : EPOCH 6 - PROGRESS: at 53.19% examples, 675220 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:56,999 : INFO : EPOCH 6 - PROGRESS: at 55.79% examples, 677637 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:33:58,011 : INFO : EPOCH 6 - PROGRESS: at 58.10% examples, 677336 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:33:59,006 : INFO : EPOCH 6 - PROGRESS: at 60.44% examples, 677426 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:00,013 : INFO : EPOCH 6 - PROGRESS: at 62.87% examples, 678268 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:34:01,013 : INFO : EPOCH 6 - PROGRESS: at 65.49% examples, 679179 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:34:02,027 : INFO : EPOCH 6 - PROGRESS: at 67.75% examples, 679195 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:03,038 : INFO : EPOCH 6 - PROGRESS: at 70.08% examples, 679910 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:34:04,057 : INFO : EPOCH 6 - PROGRESS: at 72.39% examples, 680473 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:05,074 : INFO : EPOCH 6 - PROGRESS: at 74.90% examples, 680969 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:06,077 : INFO : EPOCH 6 - PROGRESS: at 77.04% examples, 681130 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:34:07,106 : INFO : EPOCH 6 - PROGRESS: at 79.26% examples, 680923 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:34:08,130 : INFO : EPOCH 6 - PROGRESS: at 81.46% examples, 680635 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:34:09,131 : INFO : EPOCH 6 - PROGRESS: at 83.74% examples, 680526 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:34:10,130 : INFO : EPOCH 6 - PROGRESS: at 85.78% examples, 679556 words/s, in_qsize 15, out_qsize 4\n",
      "2020-01-31 09:34:11,190 : INFO : EPOCH 6 - PROGRESS: at 88.20% examples, 678228 words/s, in_qsize 16, out_qsize 3\n",
      "2020-01-31 09:34:12,208 : INFO : EPOCH 6 - PROGRESS: at 90.61% examples, 678173 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:34:13,205 : INFO : EPOCH 6 - PROGRESS: at 93.04% examples, 678966 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:34:14,234 : INFO : EPOCH 6 - PROGRESS: at 95.62% examples, 679990 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:15,245 : INFO : EPOCH 6 - PROGRESS: at 97.94% examples, 679811 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:16,003 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:34:16,019 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:34:16,019 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:34:16,019 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:34:16,034 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:34:16,034 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:34:16,034 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:34:16,047 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:34:16,051 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:34:16,055 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:34:16,055 : INFO : EPOCH - 6 : training on 41519355 raw words (30350684 effective words) took 44.6s, 680712 effective words/s\n",
      "2020-01-31 09:34:17,089 : INFO : EPOCH 7 - PROGRESS: at 2.08% examples, 639079 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:18,104 : INFO : EPOCH 7 - PROGRESS: at 4.47% examples, 671404 words/s, in_qsize 19, out_qsize 1\n",
      "2020-01-31 09:34:19,118 : INFO : EPOCH 7 - PROGRESS: at 6.76% examples, 679546 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:34:20,135 : INFO : EPOCH 7 - PROGRESS: at 8.94% examples, 682716 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:21,154 : INFO : EPOCH 7 - PROGRESS: at 10.83% examples, 683682 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:22,161 : INFO : EPOCH 7 - PROGRESS: at 12.61% examples, 682861 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:34:23,175 : INFO : EPOCH 7 - PROGRESS: at 14.67% examples, 681726 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:24,170 : INFO : EPOCH 7 - PROGRESS: at 16.56% examples, 679472 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:25,180 : INFO : EPOCH 7 - PROGRESS: at 18.33% examples, 675474 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:26,207 : INFO : EPOCH 7 - PROGRESS: at 20.08% examples, 674641 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:27,211 : INFO : EPOCH 7 - PROGRESS: at 22.10% examples, 673847 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:28,208 : INFO : EPOCH 7 - PROGRESS: at 23.73% examples, 670443 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:29,222 : INFO : EPOCH 7 - PROGRESS: at 25.85% examples, 670489 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:30,270 : INFO : EPOCH 7 - PROGRESS: at 28.52% examples, 671697 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:31,302 : INFO : EPOCH 7 - PROGRESS: at 31.19% examples, 674326 words/s, in_qsize 18, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:34:32,320 : INFO : EPOCH 7 - PROGRESS: at 33.66% examples, 675713 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:34:33,350 : INFO : EPOCH 7 - PROGRESS: at 36.05% examples, 675797 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:34:34,342 : INFO : EPOCH 7 - PROGRESS: at 38.50% examples, 676408 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:35,355 : INFO : EPOCH 7 - PROGRESS: at 41.11% examples, 678562 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:34:36,366 : INFO : EPOCH 7 - PROGRESS: at 43.61% examples, 679366 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:37,415 : INFO : EPOCH 7 - PROGRESS: at 46.19% examples, 678109 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:38,429 : INFO : EPOCH 7 - PROGRESS: at 48.56% examples, 678845 words/s, in_qsize 19, out_qsize 2\n",
      "2020-01-31 09:34:39,431 : INFO : EPOCH 7 - PROGRESS: at 51.04% examples, 679950 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:40,453 : INFO : EPOCH 7 - PROGRESS: at 53.15% examples, 678725 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:41,498 : INFO : EPOCH 7 - PROGRESS: at 55.45% examples, 676270 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:34:42,517 : INFO : EPOCH 7 - PROGRESS: at 57.79% examples, 675940 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:43,522 : INFO : EPOCH 7 - PROGRESS: at 60.10% examples, 675733 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:44,523 : INFO : EPOCH 7 - PROGRESS: at 62.50% examples, 676363 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:45,536 : INFO : EPOCH 7 - PROGRESS: at 65.03% examples, 675865 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:46,550 : INFO : EPOCH 7 - PROGRESS: at 67.38% examples, 677244 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:34:47,556 : INFO : EPOCH 7 - PROGRESS: at 69.66% examples, 677388 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:48,565 : INFO : EPOCH 7 - PROGRESS: at 71.97% examples, 678740 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:34:49,596 : INFO : EPOCH 7 - PROGRESS: at 74.47% examples, 678620 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:34:50,598 : INFO : EPOCH 7 - PROGRESS: at 76.73% examples, 679701 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:51,613 : INFO : EPOCH 7 - PROGRESS: at 78.99% examples, 680650 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:52,630 : INFO : EPOCH 7 - PROGRESS: at 81.28% examples, 680751 words/s, in_qsize 14, out_qsize 5\n",
      "2020-01-31 09:34:53,675 : INFO : EPOCH 7 - PROGRESS: at 83.78% examples, 681901 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:54,693 : INFO : EPOCH 7 - PROGRESS: at 86.00% examples, 681757 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:55,689 : INFO : EPOCH 7 - PROGRESS: at 88.46% examples, 681644 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:34:56,690 : INFO : EPOCH 7 - PROGRESS: at 90.78% examples, 680981 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:57,709 : INFO : EPOCH 7 - PROGRESS: at 93.06% examples, 680682 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:58,719 : INFO : EPOCH 7 - PROGRESS: at 95.32% examples, 679875 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:34:59,727 : INFO : EPOCH 7 - PROGRESS: at 97.57% examples, 678996 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:00,737 : INFO : EPOCH 7 - PROGRESS: at 99.70% examples, 677462 words/s, in_qsize 12, out_qsize 0\n",
      "2020-01-31 09:35:00,769 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:35:00,769 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:35:00,769 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:35:00,782 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:35:00,786 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:35:00,786 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:35:00,790 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:35:00,798 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:35:00,806 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:35:00,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:35:00,818 : INFO : EPOCH - 7 : training on 41519355 raw words (30350866 effective words) took 44.8s, 678176 effective words/s\n",
      "2020-01-31 09:35:01,830 : INFO : EPOCH 8 - PROGRESS: at 2.02% examples, 624784 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:35:02,866 : INFO : EPOCH 8 - PROGRESS: at 4.42% examples, 665313 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:03,876 : INFO : EPOCH 8 - PROGRESS: at 6.63% examples, 667718 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:35:04,899 : INFO : EPOCH 8 - PROGRESS: at 8.87% examples, 674728 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:05,897 : INFO : EPOCH 8 - PROGRESS: at 10.82% examples, 683912 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:06,949 : INFO : EPOCH 8 - PROGRESS: at 12.56% examples, 676438 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:35:07,976 : INFO : EPOCH 8 - PROGRESS: at 14.78% examples, 684627 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:08,975 : INFO : EPOCH 8 - PROGRESS: at 16.76% examples, 685358 words/s, in_qsize 19, out_qsize 2\n",
      "2020-01-31 09:35:09,972 : INFO : EPOCH 8 - PROGRESS: at 18.65% examples, 685613 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:10,994 : INFO : EPOCH 8 - PROGRESS: at 20.44% examples, 686486 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:12,019 : INFO : EPOCH 8 - PROGRESS: at 22.51% examples, 684379 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:13,084 : INFO : EPOCH 8 - PROGRESS: at 24.24% examples, 680676 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:35:14,087 : INFO : EPOCH 8 - PROGRESS: at 26.49% examples, 677587 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:35:15,137 : INFO : EPOCH 8 - PROGRESS: at 29.02% examples, 675525 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:16,162 : INFO : EPOCH 8 - PROGRESS: at 31.50% examples, 675432 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:17,188 : INFO : EPOCH 8 - PROGRESS: at 33.88% examples, 676241 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:18,191 : INFO : EPOCH 8 - PROGRESS: at 36.29% examples, 675719 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:19,189 : INFO : EPOCH 8 - PROGRESS: at 38.75% examples, 676800 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:20,211 : INFO : EPOCH 8 - PROGRESS: at 41.37% examples, 678537 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:21,225 : INFO : EPOCH 8 - PROGRESS: at 43.74% examples, 677390 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:22,224 : INFO : EPOCH 8 - PROGRESS: at 46.38% examples, 679574 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:23,255 : INFO : EPOCH 8 - PROGRESS: at 48.82% examples, 680095 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:24,323 : INFO : EPOCH 8 - PROGRESS: at 51.24% examples, 679175 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:25,340 : INFO : EPOCH 8 - PROGRESS: at 53.50% examples, 679693 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:26,357 : INFO : EPOCH 8 - PROGRESS: at 55.82% examples, 677775 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:35:27,373 : INFO : EPOCH 8 - PROGRESS: at 58.31% examples, 679132 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:28,387 : INFO : EPOCH 8 - PROGRESS: at 60.56% examples, 677715 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:29,430 : INFO : EPOCH 8 - PROGRESS: at 62.85% examples, 676487 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:30,446 : INFO : EPOCH 8 - PROGRESS: at 65.21% examples, 674600 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:31,494 : INFO : EPOCH 8 - PROGRESS: at 67.50% examples, 674309 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:35:32,556 : INFO : EPOCH 8 - PROGRESS: at 69.80% examples, 673566 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:33,571 : INFO : EPOCH 8 - PROGRESS: at 72.04% examples, 674031 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:34,607 : INFO : EPOCH 8 - PROGRESS: at 74.67% examples, 675381 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:35:35,607 : INFO : EPOCH 8 - PROGRESS: at 76.77% examples, 675134 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:35:36,643 : INFO : EPOCH 8 - PROGRESS: at 79.02% examples, 675748 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:37,648 : INFO : EPOCH 8 - PROGRESS: at 81.27% examples, 675918 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:38,649 : INFO : EPOCH 8 - PROGRESS: at 83.54% examples, 675896 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:39,677 : INFO : EPOCH 8 - PROGRESS: at 85.65% examples, 675366 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:40,711 : INFO : EPOCH 8 - PROGRESS: at 88.35% examples, 676500 words/s, in_qsize 20, out_qsize 1\n",
      "2020-01-31 09:35:41,702 : INFO : EPOCH 8 - PROGRESS: at 90.74% examples, 676565 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:42,750 : INFO : EPOCH 8 - PROGRESS: at 93.24% examples, 677517 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:43,755 : INFO : EPOCH 8 - PROGRESS: at 95.64% examples, 677495 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:44,763 : INFO : EPOCH 8 - PROGRESS: at 97.89% examples, 676927 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:45,637 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:35:45,637 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:35:45,637 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:35:45,637 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:35:45,637 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:35:45,649 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:35:45,661 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:35:45,669 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:35:45,669 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:35:45,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:35:45,681 : INFO : EPOCH - 8 : training on 41519355 raw words (30348225 effective words) took 44.9s, 676570 effective words/s\n",
      "2020-01-31 09:35:46,681 : INFO : EPOCH 9 - PROGRESS: at 1.88% examples, 592069 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:47,695 : INFO : EPOCH 9 - PROGRESS: at 4.00% examples, 618094 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:48,688 : INFO : EPOCH 9 - PROGRESS: at 6.25% examples, 639095 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:49,704 : INFO : EPOCH 9 - PROGRESS: at 8.57% examples, 660239 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:35:50,722 : INFO : EPOCH 9 - PROGRESS: at 10.40% examples, 660526 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:51,755 : INFO : EPOCH 9 - PROGRESS: at 12.26% examples, 665714 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:52,787 : INFO : EPOCH 9 - PROGRESS: at 14.26% examples, 664250 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:53,801 : INFO : EPOCH 9 - PROGRESS: at 16.37% examples, 670609 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:54,829 : INFO : EPOCH 9 - PROGRESS: at 18.27% examples, 671696 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:35:55,842 : INFO : EPOCH 9 - PROGRESS: at 20.06% examples, 673208 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:56,882 : INFO : EPOCH 9 - PROGRESS: at 22.18% examples, 672790 words/s, in_qsize 14, out_qsize 5\n",
      "2020-01-31 09:35:57,881 : INFO : EPOCH 9 - PROGRESS: at 24.01% examples, 676444 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:35:58,898 : INFO : EPOCH 9 - PROGRESS: at 26.45% examples, 678453 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:35:59,925 : INFO : EPOCH 9 - PROGRESS: at 28.82% examples, 675573 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:00,938 : INFO : EPOCH 9 - PROGRESS: at 31.22% examples, 674149 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:01,964 : INFO : EPOCH 9 - PROGRESS: at 33.58% examples, 673335 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:02,969 : INFO : EPOCH 9 - PROGRESS: at 35.86% examples, 672843 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:03,973 : INFO : EPOCH 9 - PROGRESS: at 38.04% examples, 669688 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:04,979 : INFO : EPOCH 9 - PROGRESS: at 40.38% examples, 668410 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:36:06,001 : INFO : EPOCH 9 - PROGRESS: at 43.04% examples, 670960 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:36:06,997 : INFO : EPOCH 9 - PROGRESS: at 45.56% examples, 670890 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:08,010 : INFO : EPOCH 9 - PROGRESS: at 47.97% examples, 672875 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:09,066 : INFO : EPOCH 9 - PROGRESS: at 50.37% examples, 671380 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:10,065 : INFO : EPOCH 9 - PROGRESS: at 52.81% examples, 674028 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:11,100 : INFO : EPOCH 9 - PROGRESS: at 55.16% examples, 673850 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:12,129 : INFO : EPOCH 9 - PROGRESS: at 57.61% examples, 674018 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:36:13,142 : INFO : EPOCH 9 - PROGRESS: at 59.99% examples, 674817 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:36:14,144 : INFO : EPOCH 9 - PROGRESS: at 62.47% examples, 676223 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:36:15,162 : INFO : EPOCH 9 - PROGRESS: at 65.02% examples, 675677 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:36:16,165 : INFO : EPOCH 9 - PROGRESS: at 67.25% examples, 675919 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:17,188 : INFO : EPOCH 9 - PROGRESS: at 69.41% examples, 674719 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:18,191 : INFO : EPOCH 9 - PROGRESS: at 71.43% examples, 672930 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:19,236 : INFO : EPOCH 9 - PROGRESS: at 73.84% examples, 672357 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:20,239 : INFO : EPOCH 9 - PROGRESS: at 76.05% examples, 673013 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:21,237 : INFO : EPOCH 9 - PROGRESS: at 78.19% examples, 673232 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:22,275 : INFO : EPOCH 9 - PROGRESS: at 80.38% examples, 672681 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:23,281 : INFO : EPOCH 9 - PROGRESS: at 82.73% examples, 673330 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:24,321 : INFO : EPOCH 9 - PROGRESS: at 85.02% examples, 674062 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:36:25,346 : INFO : EPOCH 9 - PROGRESS: at 87.47% examples, 673933 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:26,357 : INFO : EPOCH 9 - PROGRESS: at 89.91% examples, 674011 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:27,378 : INFO : EPOCH 9 - PROGRESS: at 92.34% examples, 674151 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:36:28,405 : INFO : EPOCH 9 - PROGRESS: at 94.81% examples, 675194 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:36:29,474 : INFO : EPOCH 9 - PROGRESS: at 97.35% examples, 675453 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:30,472 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:36:30,509 : INFO : EPOCH 9 - PROGRESS: at 99.81% examples, 675846 words/s, in_qsize 8, out_qsize 1\n",
      "2020-01-31 09:36:30,513 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:36:30,521 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:36:30,525 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:36:30,541 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:36:30,545 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:36:30,557 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:36:30,565 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:36:30,569 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:36:30,577 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:36:30,581 : INFO : EPOCH - 9 : training on 41519355 raw words (30345375 effective words) took 44.9s, 675972 effective words/s\n",
      "2020-01-31 09:36:31,592 : INFO : EPOCH 10 - PROGRESS: at 1.88% examples, 592658 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:36:32,584 : INFO : EPOCH 10 - PROGRESS: at 4.12% examples, 633354 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:33,618 : INFO : EPOCH 10 - PROGRESS: at 6.22% examples, 631241 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:36:34,650 : INFO : EPOCH 10 - PROGRESS: at 8.33% examples, 633411 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:35,678 : INFO : EPOCH 10 - PROGRESS: at 10.09% examples, 633599 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:36,695 : INFO : EPOCH 10 - PROGRESS: at 11.93% examples, 640056 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:36:37,690 : INFO : EPOCH 10 - PROGRESS: at 14.05% examples, 652835 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:38,711 : INFO : EPOCH 10 - PROGRESS: at 16.03% examples, 654194 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:39,715 : INFO : EPOCH 10 - PROGRESS: at 17.79% examples, 654172 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:36:40,740 : INFO : EPOCH 10 - PROGRESS: at 19.65% examples, 657071 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:41,752 : INFO : EPOCH 10 - PROGRESS: at 21.58% examples, 654390 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:42,771 : INFO : EPOCH 10 - PROGRESS: at 23.29% examples, 654380 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:43,773 : INFO : EPOCH 10 - PROGRESS: at 25.19% examples, 654393 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:36:44,813 : INFO : EPOCH 10 - PROGRESS: at 27.68% examples, 654152 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:36:45,820 : INFO : EPOCH 10 - PROGRESS: at 30.12% examples, 655882 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:46,866 : INFO : EPOCH 10 - PROGRESS: at 32.53% examples, 654482 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:47,857 : INFO : EPOCH 10 - PROGRESS: at 34.60% examples, 651742 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:36:48,915 : INFO : EPOCH 10 - PROGRESS: at 36.96% examples, 651382 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:36:49,911 : INFO : EPOCH 10 - PROGRESS: at 39.27% examples, 650638 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:36:50,914 : INFO : EPOCH 10 - PROGRESS: at 41.61% examples, 650123 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:51,936 : INFO : EPOCH 10 - PROGRESS: at 43.97% examples, 649786 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:52,946 : INFO : EPOCH 10 - PROGRESS: at 46.26% examples, 648672 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:36:53,962 : INFO : EPOCH 10 - PROGRESS: at 48.56% examples, 649707 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:36:54,962 : INFO : EPOCH 10 - PROGRESS: at 50.78% examples, 648793 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:55,961 : INFO : EPOCH 10 - PROGRESS: at 52.99% examples, 650097 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:56,973 : INFO : EPOCH 10 - PROGRESS: at 55.37% examples, 651209 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:58,001 : INFO : EPOCH 10 - PROGRESS: at 57.69% examples, 651254 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:36:59,017 : INFO : EPOCH 10 - PROGRESS: at 59.89% examples, 650429 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:00,029 : INFO : EPOCH 10 - PROGRESS: at 62.05% examples, 649713 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:37:01,049 : INFO : EPOCH 10 - PROGRESS: at 64.44% examples, 648336 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:02,066 : INFO : EPOCH 10 - PROGRESS: at 66.43% examples, 647032 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:03,113 : INFO : EPOCH 10 - PROGRESS: at 68.60% examples, 645987 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:04,115 : INFO : EPOCH 10 - PROGRESS: at 70.66% examples, 645453 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:05,142 : INFO : EPOCH 10 - PROGRESS: at 72.93% examples, 645698 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:06,143 : INFO : EPOCH 10 - PROGRESS: at 75.31% examples, 647164 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:07,200 : INFO : EPOCH 10 - PROGRESS: at 77.37% examples, 646875 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:37:08,206 : INFO : EPOCH 10 - PROGRESS: at 79.54% examples, 647306 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:37:09,232 : INFO : EPOCH 10 - PROGRESS: at 81.59% examples, 646728 words/s, in_qsize 16, out_qsize 3\n",
      "2020-01-31 09:37:10,240 : INFO : EPOCH 10 - PROGRESS: at 83.94% examples, 647804 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:11,243 : INFO : EPOCH 10 - PROGRESS: at 86.24% examples, 649272 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:37:12,264 : INFO : EPOCH 10 - PROGRESS: at 88.72% examples, 649602 words/s, in_qsize 18, out_qsize 1\n",
      "2020-01-31 09:37:13,287 : INFO : EPOCH 10 - PROGRESS: at 91.18% examples, 650673 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:14,301 : INFO : EPOCH 10 - PROGRESS: at 93.62% examples, 651932 words/s, in_qsize 17, out_qsize 2\n",
      "2020-01-31 09:37:15,331 : INFO : EPOCH 10 - PROGRESS: at 96.01% examples, 652731 words/s, in_qsize 20, out_qsize 0\n",
      "2020-01-31 09:37:16,333 : INFO : EPOCH 10 - PROGRESS: at 98.36% examples, 653155 words/s, in_qsize 19, out_qsize 0\n",
      "2020-01-31 09:37:16,894 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-01-31 09:37:16,909 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-01-31 09:37:16,925 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-01-31 09:37:16,925 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-01-31 09:37:16,925 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-01-31 09:37:16,925 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-31 09:37:16,942 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-31 09:37:16,950 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-31 09:37:16,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-31 09:37:16,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-31 09:37:16,966 : INFO : EPOCH - 10 : training on 41519355 raw words (30346384 effective words) took 46.4s, 654366 effective words/s\n",
      "2020-01-31 09:37:16,966 : INFO : training on a 415193550 raw words (303486803 effective words) took 451.7s, 671891 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(303486803, 415193550)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Word2Vec model parameters\n",
    "\n",
    "size:\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me.\n",
    "\n",
    "window:\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n",
    "\n",
    "min_count:\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "workers:\n",
    "How many threads to use behind the scenes?\n",
    "\n",
    "sg: sg=1 means skip-gram and sg=0 menascbow\n",
    "'''\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10, sg=0)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 09:37:28,222 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('filthy', 0.8649767637252808),\n",
       " ('unclean', 0.7816728949546814),\n",
       " ('stained', 0.7680723667144775),\n",
       " ('grubby', 0.7676006555557251),\n",
       " ('dusty', 0.7526466846466064),\n",
       " ('smelly', 0.7517306208610535),\n",
       " ('dingy', 0.7335317134857178),\n",
       " ('gross', 0.7160970568656921),\n",
       " ('disgusting', 0.7145689129829407),\n",
       " ('mouldy', 0.7115984559059143)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9213926196098328),\n",
       " ('friendly', 0.8371918201446533),\n",
       " ('cordial', 0.8164287209510803),\n",
       " ('professional', 0.7834046483039856),\n",
       " ('curteous', 0.7721035480499268),\n",
       " ('attentive', 0.7692217826843262)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canada', 0.6766113042831421),\n",
       " ('germany', 0.6655616164207458),\n",
       " ('spain', 0.6482937335968018),\n",
       " ('mexico', 0.6131438612937927),\n",
       " ('hawaii', 0.611734926700592),\n",
       " ('england', 0.6073362827301025)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('duvet', 0.720068633556366),\n",
       " ('blanket', 0.7177280187606812),\n",
       " ('mattress', 0.711969256401062),\n",
       " ('quilt', 0.6905918717384338),\n",
       " ('matress', 0.6876316070556641),\n",
       " ('pillows', 0.6443690657615662),\n",
       " ('pillowcase', 0.6401588916778564),\n",
       " ('sheets', 0.6301946043968201),\n",
       " ('comforter', 0.6158015727996826),\n",
       " ('foam', 0.6149501800537109)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7517306"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Anaconda3\\envs\\NLP\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2328315 ,  2.3876312 , -3.145297  ,  2.4398775 , -2.596409  ,\n",
       "       -2.5369923 , -0.3779383 ,  3.4048283 , -0.36356768, -1.6974839 ,\n",
       "        0.21198772, -1.1418074 ,  1.4528807 , -0.67290956,  0.88678986,\n",
       "       -2.4212637 ,  5.3189297 ,  1.0757921 , -2.4973161 ,  0.21883395,\n",
       "       -2.4325886 , -1.4251771 ,  3.550189  ,  0.36202204,  3.1418197 ,\n",
       "        1.9262061 ,  1.0360358 ,  2.9860694 , -1.8334814 ,  1.6458569 ,\n",
       "        1.7575328 ,  2.3585575 , -3.864418  , -2.1340988 , -1.0610147 ,\n",
       "       -1.7900633 ,  3.1246374 ,  0.31782976,  1.6041994 ,  0.49191818,\n",
       "        1.6365526 , -2.7526255 ,  3.4192998 , -1.1165683 , -0.84559005,\n",
       "        0.9656256 , -3.1016958 ,  0.8639137 , -0.59646815,  2.7678509 ,\n",
       "        2.8266351 , -2.735863  ,  2.1288145 ,  1.3084606 , -1.9494946 ,\n",
       "       -0.96154636, -0.5961025 , -2.9837945 ,  0.7618791 ,  2.1486118 ,\n",
       "       -3.4807298 ,  0.33803144, -0.95271575,  2.6181896 ,  3.8802657 ,\n",
       "       -0.8704985 ,  1.5160646 ,  0.15534614,  0.55781347, -1.9372633 ,\n",
       "        0.7225972 ,  2.857038  , -3.2920902 , -0.77717316, -1.587394  ,\n",
       "       -0.8114726 , -3.1985092 ,  1.0431175 ,  0.09454054, -5.1148047 ,\n",
       "       -0.35874292,  3.8931413 , -2.0489347 , -0.35246804,  0.8232968 ,\n",
       "        0.40359715, -2.841697  , -0.45281878,  0.8118131 , -0.12569554,\n",
       "        5.863548  , -1.0763898 ,  1.5114409 ,  0.5568004 , -1.9327534 ,\n",
       "       -1.9598192 ,  4.682773  , -2.7540982 , -5.906088  ,  0.49742305,\n",
       "       -2.1691818 , -1.7054299 ,  3.2397747 ,  4.2002807 , -0.7368754 ,\n",
       "       -0.0997859 , -0.888647  , -0.88506114,  0.0230938 , -2.2096436 ,\n",
       "       -1.7616323 ,  4.6121025 ,  0.9891673 ,  2.2840521 ,  1.492369  ,\n",
       "        1.5405442 ,  0.5839662 , -2.263378  ,  2.755474  , -1.4817541 ,\n",
       "        0.33290967, -0.23318163,  1.5165209 ,  1.054779  ,  1.3456327 ,\n",
       "        0.33532783,  0.4543316 ,  2.0408225 , -6.3294945 , -2.1492605 ,\n",
       "       -1.4057964 , -1.9981467 ,  0.8704227 ,  0.04830516,  0.94912523,\n",
       "        0.95892876,  0.49442592,  0.6572975 , -1.0443769 , -3.9163826 ,\n",
       "        3.2578733 ,  0.7180099 ,  1.6083987 , -0.23412071,  0.75017524,\n",
       "       -0.24184383, -4.702938  , -1.4930636 , -0.25222751,  3.3205628 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print word vector\n",
    "model.wv['dirty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove  \n",
    "ref: https://github.com/maciejkula/glove-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 11:51:01,086 : INFO : loading projection weights from C:\\Users\\isaac/gensim-data\\glove-twitter-25\\glove-twitter-25.gz\n",
      "2020-01-31 11:52:13,857 : INFO : loaded (1193514, 25) matrix from C:\\Users\\isaac/gensim-data\\glove-twitter-25\\glove-twitter-25.gz\n",
      "C:\\Users\\isaac\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n",
      "2020-01-31 11:52:13,872 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('clegg', 0.9653650522232056),\n",
       " ('miliband', 0.9515050649642944),\n",
       " ('bachmann', 0.9484401345252991),\n",
       " ('mcconnell', 0.9416399002075195),\n",
       " ('carney', 0.9340257048606873),\n",
       " ('coulter', 0.9311323761940002),\n",
       " ('boehner', 0.9286301732063293),\n",
       " ('santorum', 0.9269058704376221),\n",
       " ('farage', 0.919365406036377),\n",
       " ('mourdock', 0.9186689853668213)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# download the model and return as object ready for use\n",
    "model_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "\n",
    "model_glove_twitter.wv.most_similar(\"pelosi\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4.0317e-01,  6.1409e-03,  3.8262e-01,  3.9230e-01,  5.8592e-01,\n",
       "        6.5074e-01,  1.0542e+00, -6.0706e-01, -4.1578e-01,  2.8592e-01,\n",
       "       -6.4108e-01,  4.4421e-01, -3.6479e+00, -1.0700e+00,  3.5746e-03,\n",
       "        9.2253e-02,  6.6907e-01, -7.1866e-01, -2.7309e-02,  6.2390e-01,\n",
       "        4.4713e-01,  8.7072e-01,  9.5165e-01, -1.0244e+00,  1.2813e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.wv['dirty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('policy', 0.9484813213348389),\n",
       " ('reforms', 0.9403933882713318),\n",
       " ('laws', 0.94012051820755),\n",
       " ('government', 0.9230710864067078),\n",
       " ('regulations', 0.9168934226036072),\n",
       " ('economy', 0.9110006093978882),\n",
       " ('immigration', 0.9105910062789917),\n",
       " ('legislation', 0.9089650511741638),\n",
       " ('govt', 0.9054746627807617),\n",
       " ('regulation', 0.9050779342651367)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.wv.most_similar(\"policies\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'orange'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.wv.doesnt_match([\"trump\",\"bernie\",\"obama\",\"pelosi\",\"orange\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-31 11:54:51,687 : INFO : glove-wiki-gigaword-100 downloaded\n",
      "2020-01-31 11:54:51,717 : INFO : loading projection weights from C:\\Users\\isaac/gensim-data\\glove-wiki-gigaword-100\\glove-wiki-gigaword-100.gz\n",
      "2020-01-31 11:55:59,469 : INFO : loaded (400000, 100) matrix from C:\\Users\\isaac/gensim-data\\glove-wiki-gigaword-100\\glove-wiki-gigaword-100.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "#again, download and load the model\n",
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "2020-01-31 11:57:21,180 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('filthy', 0.7690386772155762),\n",
       " ('smelly', 0.7392697334289551),\n",
       " ('shabby', 0.7025482058525085),\n",
       " ('dingy', 0.7022336721420288),\n",
       " ('grubby', 0.6754513382911682),\n",
       " ('grungy', 0.6414023041725159),\n",
       " ('dank', 0.626369833946228),\n",
       " ('sweaty', 0.622745156288147),\n",
       " ('dreary', 0.6216243505477905),\n",
       " ('gritty', 0.621574878692627)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similarity\n",
    "model_gigaword.wv.most_similar(positive=['dirty','grimy'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Anaconda3\\envs\\NLP\\lib\\site-packages\\gensim\\models\\doc2vec.py:570: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "C:\\Users\\isaac\\Anaconda3\\envs\\NLP\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2020-01-29 19:38:25,965 : INFO : collecting all words and their counts\n",
      "2020-01-29 19:38:25,994 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-01-29 19:38:26,033 : INFO : collected 11097 word types and 1000 unique tags from a corpus of 1000 examples and 84408 words\n",
      "2020-01-29 19:38:26,034 : INFO : Loading a fresh vocabulary\n",
      "2020-01-29 19:38:26,063 : INFO : effective_min_count=1 retains 11097 unique words (100% of original 11097, drops 0)\n",
      "2020-01-29 19:38:26,064 : INFO : effective_min_count=1 leaves 84408 word corpus (100% of original 84408, drops 0)\n",
      "2020-01-29 19:38:26,141 : INFO : deleting the raw counts dictionary of 11097 items\n",
      "2020-01-29 19:38:26,143 : INFO : sample=1e-05 downsamples 3599 most-common words\n",
      "2020-01-29 19:38:26,145 : INFO : downsampling leaves estimated 22704 word corpus (26.9% of prior 84408)\n",
      "2020-01-29 19:38:26,176 : INFO : estimated required memory for 11097 words and 300 dimensions: 33381300 bytes\n",
      "2020-01-29 19:38:26,177 : INFO : resetting layer weights\n",
      "2020-01-29 19:38:29,166 : INFO : training model with 1 workers on 11098 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=15\n",
      "2020-01-29 19:38:29,988 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:29,989 : INFO : EPOCH - 1 : training on 84408 raw words (23740 effective words) took 0.8s, 28905 effective words/s\n",
      "2020-01-29 19:38:30,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:30,940 : INFO : EPOCH - 2 : training on 84408 raw words (23650 effective words) took 1.0s, 24644 effective words/s\n",
      "2020-01-29 19:38:31,956 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 23576 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:38:31,958 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:31,959 : INFO : EPOCH - 3 : training on 84408 raw words (23619 effective words) took 1.0s, 23514 effective words/s\n",
      "2020-01-29 19:38:32,783 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:32,783 : INFO : EPOCH - 4 : training on 84408 raw words (23652 effective words) took 0.8s, 28575 effective words/s\n",
      "2020-01-29 19:38:33,635 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:33,635 : INFO : EPOCH - 5 : training on 84408 raw words (23580 effective words) took 0.8s, 27829 effective words/s\n",
      "2020-01-29 19:38:34,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:34,438 : INFO : EPOCH - 6 : training on 84408 raw words (23768 effective words) took 0.8s, 29787 effective words/s\n",
      "2020-01-29 19:38:35,200 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:35,200 : INFO : EPOCH - 7 : training on 84408 raw words (23697 effective words) took 0.8s, 30905 effective words/s\n",
      "2020-01-29 19:38:36,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:36,021 : INFO : EPOCH - 8 : training on 84408 raw words (23643 effective words) took 0.8s, 28700 effective words/s\n",
      "2020-01-29 19:38:36,834 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:36,834 : INFO : EPOCH - 9 : training on 84408 raw words (23579 effective words) took 0.8s, 29552 effective words/s\n",
      "2020-01-29 19:38:37,827 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:37,829 : INFO : EPOCH - 10 : training on 84408 raw words (23674 effective words) took 1.0s, 23892 effective words/s\n",
      "2020-01-29 19:38:38,725 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:38,726 : INFO : EPOCH - 11 : training on 84408 raw words (23720 effective words) took 0.9s, 26491 effective words/s\n",
      "2020-01-29 19:38:39,554 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:39,554 : INFO : EPOCH - 12 : training on 84408 raw words (23711 effective words) took 0.8s, 28600 effective words/s\n",
      "2020-01-29 19:38:40,328 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:40,329 : INFO : EPOCH - 13 : training on 84408 raw words (23598 effective words) took 0.8s, 30681 effective words/s\n",
      "2020-01-29 19:38:41,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:41,079 : INFO : EPOCH - 14 : training on 84408 raw words (23691 effective words) took 0.8s, 31382 effective words/s\n",
      "2020-01-29 19:38:41,923 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:41,924 : INFO : EPOCH - 15 : training on 84408 raw words (23755 effective words) took 0.8s, 28497 effective words/s\n",
      "2020-01-29 19:38:42,692 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:42,694 : INFO : EPOCH - 16 : training on 84408 raw words (23605 effective words) took 0.8s, 30709 effective words/s\n",
      "2020-01-29 19:38:43,457 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:43,457 : INFO : EPOCH - 17 : training on 84408 raw words (23645 effective words) took 0.8s, 30944 effective words/s\n",
      "2020-01-29 19:38:44,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:44,273 : INFO : EPOCH - 18 : training on 84408 raw words (23656 effective words) took 0.8s, 29138 effective words/s\n",
      "2020-01-29 19:38:45,049 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:45,051 : INFO : EPOCH - 19 : training on 84408 raw words (23646 effective words) took 0.8s, 30578 effective words/s\n",
      "2020-01-29 19:38:45,833 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:45,833 : INFO : EPOCH - 20 : training on 84408 raw words (23645 effective words) took 0.8s, 30287 effective words/s\n",
      "2020-01-29 19:38:46,596 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:46,596 : INFO : EPOCH - 21 : training on 84408 raw words (23647 effective words) took 0.8s, 30965 effective words/s\n",
      "2020-01-29 19:38:47,576 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:47,576 : INFO : EPOCH - 22 : training on 84408 raw words (23759 effective words) took 1.0s, 24362 effective words/s\n",
      "2020-01-29 19:38:48,345 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:48,345 : INFO : EPOCH - 23 : training on 84408 raw words (23638 effective words) took 0.8s, 30336 effective words/s\n",
      "2020-01-29 19:38:49,125 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:49,125 : INFO : EPOCH - 24 : training on 84408 raw words (23759 effective words) took 0.8s, 30678 effective words/s\n",
      "2020-01-29 19:38:50,038 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:50,039 : INFO : EPOCH - 25 : training on 84408 raw words (23859 effective words) took 0.9s, 26530 effective words/s\n",
      "2020-01-29 19:38:50,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:50,938 : INFO : EPOCH - 26 : training on 84408 raw words (23691 effective words) took 0.9s, 26429 effective words/s\n",
      "2020-01-29 19:38:51,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:51,854 : INFO : EPOCH - 27 : training on 84408 raw words (23647 effective words) took 0.9s, 25858 effective words/s\n",
      "2020-01-29 19:38:52,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:52,684 : INFO : EPOCH - 28 : training on 84408 raw words (23749 effective words) took 0.8s, 28731 effective words/s\n",
      "2020-01-29 19:38:53,555 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-29 19:38:53,556 : INFO : EPOCH - 29 : training on 84408 raw words (23552 effective words) took 0.9s, 27079 effective words/s\n",
      "2020-01-29 19:38:54,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:54,392 : INFO : EPOCH - 30 : training on 84408 raw words (23625 effective words) took 0.8s, 28326 effective words/s\n",
      "2020-01-29 19:38:55,193 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:55,193 : INFO : EPOCH - 31 : training on 84408 raw words (23690 effective words) took 0.8s, 29215 effective words/s\n",
      "2020-01-29 19:38:55,984 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:55,985 : INFO : EPOCH - 32 : training on 84408 raw words (23772 effective words) took 0.8s, 30588 effective words/s\n",
      "2020-01-29 19:38:56,773 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:56,775 : INFO : EPOCH - 33 : training on 84408 raw words (23742 effective words) took 0.8s, 30149 effective words/s\n",
      "2020-01-29 19:38:57,556 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:57,558 : INFO : EPOCH - 34 : training on 84408 raw words (23695 effective words) took 0.8s, 30312 effective words/s\n",
      "2020-01-29 19:38:58,360 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:58,362 : INFO : EPOCH - 35 : training on 84408 raw words (23736 effective words) took 0.8s, 29642 effective words/s\n",
      "2020-01-29 19:38:59,200 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:38:59,200 : INFO : EPOCH - 36 : training on 84408 raw words (23741 effective words) took 0.8s, 28272 effective words/s\n",
      "2020-01-29 19:39:00,007 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:00,008 : INFO : EPOCH - 37 : training on 84408 raw words (23740 effective words) took 0.8s, 29596 effective words/s\n",
      "2020-01-29 19:39:00,817 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:00,819 : INFO : EPOCH - 38 : training on 84408 raw words (23639 effective words) took 0.8s, 29261 effective words/s\n",
      "2020-01-29 19:39:01,865 : INFO : EPOCH 39 - PROGRESS: at 100.00% examples, 22714 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:01,865 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:01,867 : INFO : EPOCH - 39 : training on 84408 raw words (23703 effective words) took 1.0s, 22641 effective words/s\n",
      "2020-01-29 19:39:02,845 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:02,847 : INFO : EPOCH - 40 : training on 84408 raw words (23608 effective words) took 1.0s, 24183 effective words/s\n",
      "2020-01-29 19:39:03,934 : INFO : EPOCH 41 - PROGRESS: at 95.60% examples, 20524 words/s, in_qsize 1, out_qsize 0\n",
      "2020-01-29 19:39:03,998 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:03,999 : INFO : EPOCH - 41 : training on 84408 raw words (23651 effective words) took 1.1s, 20575 effective words/s\n",
      "2020-01-29 19:39:04,995 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:04,995 : INFO : EPOCH - 42 : training on 84408 raw words (23713 effective words) took 1.0s, 23834 effective words/s\n",
      "2020-01-29 19:39:06,041 : INFO : EPOCH 43 - PROGRESS: at 100.00% examples, 22863 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:06,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:06,044 : INFO : EPOCH - 43 : training on 84408 raw words (23803 effective words) took 1.0s, 22796 effective words/s\n",
      "2020-01-29 19:39:06,977 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:06,978 : INFO : EPOCH - 44 : training on 84408 raw words (23624 effective words) took 0.9s, 25361 effective words/s\n",
      "2020-01-29 19:39:08,000 : INFO : EPOCH 45 - PROGRESS: at 100.00% examples, 23237 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:08,008 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:08,010 : INFO : EPOCH - 45 : training on 84408 raw words (23853 effective words) took 1.0s, 23166 effective words/s\n",
      "2020-01-29 19:39:08,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:08,941 : INFO : EPOCH - 46 : training on 84408 raw words (23692 effective words) took 0.9s, 25538 effective words/s\n",
      "2020-01-29 19:39:09,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:09,896 : INFO : EPOCH - 47 : training on 84408 raw words (23694 effective words) took 1.0s, 24871 effective words/s\n",
      "2020-01-29 19:39:10,914 : INFO : EPOCH 48 - PROGRESS: at 95.60% examples, 22237 words/s, in_qsize 1, out_qsize 0\n",
      "2020-01-29 19:39:10,975 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:10,976 : INFO : EPOCH - 48 : training on 84408 raw words (23883 effective words) took 1.1s, 22179 effective words/s\n",
      "2020-01-29 19:39:12,022 : INFO : EPOCH 49 - PROGRESS: at 100.00% examples, 22559 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:12,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:12,024 : INFO : EPOCH - 49 : training on 84408 raw words (23530 effective words) took 1.0s, 22505 effective words/s\n",
      "2020-01-29 19:39:13,043 : INFO : EPOCH 50 - PROGRESS: at 100.00% examples, 23311 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:13,045 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:13,047 : INFO : EPOCH - 50 : training on 84408 raw words (23694 effective words) took 1.0s, 23229 effective words/s\n",
      "2020-01-29 19:39:14,061 : INFO : EPOCH 51 - PROGRESS: at 95.60% examples, 22091 words/s, in_qsize 1, out_qsize 0\n",
      "2020-01-29 19:39:14,121 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:14,123 : INFO : EPOCH - 51 : training on 84408 raw words (23651 effective words) took 1.1s, 22046 effective words/s\n",
      "2020-01-29 19:39:15,156 : INFO : EPOCH 52 - PROGRESS: at 100.00% examples, 23019 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:15,157 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:15,159 : INFO : EPOCH - 52 : training on 84408 raw words (23727 effective words) took 1.0s, 22959 effective words/s\n",
      "2020-01-29 19:39:16,176 : INFO : EPOCH 53 - PROGRESS: at 100.00% examples, 23364 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:16,177 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:16,179 : INFO : EPOCH - 53 : training on 84408 raw words (23705 effective words) took 1.0s, 23297 effective words/s\n",
      "2020-01-29 19:39:17,214 : INFO : EPOCH 54 - PROGRESS: at 100.00% examples, 22960 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:17,215 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:17,216 : INFO : EPOCH - 54 : training on 84408 raw words (23723 effective words) took 1.0s, 22919 effective words/s\n",
      "2020-01-29 19:39:18,165 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:18,167 : INFO : EPOCH - 55 : training on 84408 raw words (23740 effective words) took 0.9s, 25050 effective words/s\n",
      "2020-01-29 19:39:19,301 : INFO : EPOCH 56 - PROGRESS: at 95.60% examples, 19787 words/s, in_qsize 1, out_qsize 0\n",
      "2020-01-29 19:39:19,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:19,386 : INFO : EPOCH - 56 : training on 84408 raw words (23695 effective words) took 1.2s, 19483 effective words/s\n",
      "2020-01-29 19:39:20,395 : INFO : EPOCH 57 - PROGRESS: at 100.00% examples, 23398 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:20,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:20,395 : INFO : EPOCH - 57 : training on 84408 raw words (23704 effective words) took 1.0s, 23332 effective words/s\n",
      "2020-01-29 19:39:21,376 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:21,377 : INFO : EPOCH - 58 : training on 84408 raw words (23747 effective words) took 1.0s, 24548 effective words/s\n",
      "2020-01-29 19:39:22,416 : INFO : EPOCH 59 - PROGRESS: at 100.00% examples, 22892 words/s, in_qsize 0, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-29 19:39:22,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:22,418 : INFO : EPOCH - 59 : training on 84408 raw words (23693 effective words) took 1.0s, 22852 effective words/s\n",
      "2020-01-29 19:39:23,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:23,322 : INFO : EPOCH - 60 : training on 84408 raw words (23766 effective words) took 0.9s, 26339 effective words/s\n",
      "2020-01-29 19:39:24,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:24,211 : INFO : EPOCH - 61 : training on 84408 raw words (23851 effective words) took 0.9s, 26917 effective words/s\n",
      "2020-01-29 19:39:25,068 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:25,068 : INFO : EPOCH - 62 : training on 84408 raw words (23707 effective words) took 0.9s, 27426 effective words/s\n",
      "2020-01-29 19:39:25,970 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:25,972 : INFO : EPOCH - 63 : training on 84408 raw words (23613 effective words) took 0.9s, 26460 effective words/s\n",
      "2020-01-29 19:39:26,840 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:26,841 : INFO : EPOCH - 64 : training on 84408 raw words (23530 effective words) took 0.9s, 27171 effective words/s\n",
      "2020-01-29 19:39:27,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:27,742 : INFO : EPOCH - 65 : training on 84408 raw words (23816 effective words) took 0.9s, 26523 effective words/s\n",
      "2020-01-29 19:39:28,602 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:28,603 : INFO : EPOCH - 66 : training on 84408 raw words (23767 effective words) took 0.9s, 27681 effective words/s\n",
      "2020-01-29 19:39:29,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:29,392 : INFO : EPOCH - 67 : training on 84408 raw words (23622 effective words) took 0.8s, 29977 effective words/s\n",
      "2020-01-29 19:39:30,222 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:30,223 : INFO : EPOCH - 68 : training on 84408 raw words (23583 effective words) took 0.8s, 28494 effective words/s\n",
      "2020-01-29 19:39:31,027 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:31,027 : INFO : EPOCH - 69 : training on 84408 raw words (23776 effective words) took 0.8s, 29239 effective words/s\n",
      "2020-01-29 19:39:31,827 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:31,835 : INFO : EPOCH - 70 : training on 84408 raw words (23846 effective words) took 0.8s, 30018 effective words/s\n",
      "2020-01-29 19:39:32,623 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:32,623 : INFO : EPOCH - 71 : training on 84408 raw words (23750 effective words) took 0.8s, 30128 effective words/s\n",
      "2020-01-29 19:39:33,379 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:33,379 : INFO : EPOCH - 72 : training on 84408 raw words (23696 effective words) took 0.8s, 30978 effective words/s\n",
      "2020-01-29 19:39:34,149 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:34,165 : INFO : EPOCH - 73 : training on 84408 raw words (23873 effective words) took 0.8s, 31054 effective words/s\n",
      "2020-01-29 19:39:35,068 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:35,069 : INFO : EPOCH - 74 : training on 84408 raw words (23756 effective words) took 0.9s, 26341 effective words/s\n",
      "2020-01-29 19:39:36,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:36,007 : INFO : EPOCH - 75 : training on 84408 raw words (23639 effective words) took 0.9s, 25339 effective words/s\n",
      "2020-01-29 19:39:36,803 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:36,803 : INFO : EPOCH - 76 : training on 84408 raw words (23739 effective words) took 0.8s, 29781 effective words/s\n",
      "2020-01-29 19:39:37,544 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:37,546 : INFO : EPOCH - 77 : training on 84408 raw words (23756 effective words) took 0.7s, 32224 effective words/s\n",
      "2020-01-29 19:39:38,269 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:38,269 : INFO : EPOCH - 78 : training on 84408 raw words (23555 effective words) took 0.7s, 32387 effective words/s\n",
      "2020-01-29 19:39:38,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:38,991 : INFO : EPOCH - 79 : training on 84408 raw words (23690 effective words) took 0.7s, 33010 effective words/s\n",
      "2020-01-29 19:39:39,722 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:39,722 : INFO : EPOCH - 80 : training on 84408 raw words (23673 effective words) took 0.7s, 32631 effective words/s\n",
      "2020-01-29 19:39:40,595 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:40,596 : INFO : EPOCH - 81 : training on 84408 raw words (23681 effective words) took 0.9s, 27201 effective words/s\n",
      "2020-01-29 19:39:41,320 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:41,320 : INFO : EPOCH - 82 : training on 84408 raw words (23832 effective words) took 0.7s, 32400 effective words/s\n",
      "2020-01-29 19:39:42,065 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:42,065 : INFO : EPOCH - 83 : training on 84408 raw words (23887 effective words) took 0.7s, 32473 effective words/s\n",
      "2020-01-29 19:39:42,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:42,919 : INFO : EPOCH - 84 : training on 84408 raw words (23716 effective words) took 0.8s, 28045 effective words/s\n",
      "2020-01-29 19:39:43,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:43,857 : INFO : EPOCH - 85 : training on 84408 raw words (23784 effective words) took 0.9s, 25450 effective words/s\n",
      "2020-01-29 19:39:44,732 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:44,734 : INFO : EPOCH - 86 : training on 84408 raw words (23725 effective words) took 0.9s, 27119 effective words/s\n",
      "2020-01-29 19:39:45,660 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:45,661 : INFO : EPOCH - 87 : training on 84408 raw words (23652 effective words) took 0.9s, 25590 effective words/s\n",
      "2020-01-29 19:39:46,554 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:46,556 : INFO : EPOCH - 88 : training on 84408 raw words (23855 effective words) took 0.9s, 26712 effective words/s\n",
      "2020-01-29 19:39:47,360 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:47,361 : INFO : EPOCH - 89 : training on 84408 raw words (23598 effective words) took 0.8s, 29424 effective words/s\n",
      "2020-01-29 19:39:48,120 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:48,122 : INFO : EPOCH - 90 : training on 84408 raw words (23600 effective words) took 0.8s, 31111 effective words/s\n",
      "2020-01-29 19:39:48,978 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:48,979 : INFO : EPOCH - 91 : training on 84408 raw words (23829 effective words) took 0.9s, 27871 effective words/s\n",
      "2020-01-29 19:39:49,816 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:49,818 : INFO : EPOCH - 92 : training on 84408 raw words (23689 effective words) took 0.8s, 28321 effective words/s\n",
      "2020-01-29 19:39:50,573 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:50,574 : INFO : EPOCH - 93 : training on 84408 raw words (23954 effective words) took 0.8s, 31759 effective words/s\n",
      "2020-01-29 19:39:51,528 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:51,528 : INFO : EPOCH - 94 : training on 84408 raw words (23740 effective words) took 1.0s, 24815 effective words/s\n",
      "2020-01-29 19:39:52,280 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:52,280 : INFO : EPOCH - 95 : training on 84408 raw words (23639 effective words) took 0.7s, 31635 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-29 19:39:53,088 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:53,089 : INFO : EPOCH - 96 : training on 84408 raw words (23672 effective words) took 0.8s, 29428 effective words/s\n",
      "2020-01-29 19:39:54,096 : INFO : EPOCH 97 - PROGRESS: at 82.60% examples, 19470 words/s, in_qsize 2, out_qsize 0\n",
      "2020-01-29 19:39:54,258 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:54,260 : INFO : EPOCH - 97 : training on 84408 raw words (23762 effective words) took 1.2s, 20323 effective words/s\n",
      "2020-01-29 19:39:55,062 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:55,064 : INFO : EPOCH - 98 : training on 84408 raw words (23675 effective words) took 0.8s, 29595 effective words/s\n",
      "2020-01-29 19:39:56,033 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:56,034 : INFO : EPOCH - 99 : training on 84408 raw words (23771 effective words) took 1.0s, 24539 effective words/s\n",
      "2020-01-29 19:39:57,084 : INFO : EPOCH 100 - PROGRESS: at 100.00% examples, 22781 words/s, in_qsize 0, out_qsize 1\n",
      "2020-01-29 19:39:57,086 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-29 19:39:57,089 : INFO : EPOCH - 100 : training on 84408 raw words (23843 effective words) took 1.1s, 22672 effective words/s\n",
      "2020-01-29 19:39:57,091 : INFO : training on a 8440800 raw words (2370696 effective words) took 87.9s, 26963 effective words/s\n",
      "2020-01-29 19:39:57,093 : INFO : saving Doc2Vec object under ./dataset/toy_data/model.bin, separately None\n",
      "2020-01-29 19:39:57,469 : INFO : saved ./dataset/toy_data/model.bin\n"
     ]
    }
   ],
   "source": [
    "#python example to train doc2vec model (with or without pre-trained word embeddings)\n",
    "\n",
    "import gensim.models as g\n",
    "import logging\n",
    "\n",
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0 #0 = dbow; 1 = dmpv\n",
    "worker_count = 1 #number of parallel processes\n",
    "\n",
    "#pretrained word embeddings\n",
    "pretrained_emb = \"./dataset/toy_data/pretrained_word_embeddings.txt\" #None if use without pretrained embeddings\n",
    "\n",
    "#input corpus\n",
    "train_corpus = \"./dataset/toy_data/train_docs.txt\"\n",
    "\n",
    "#output model\n",
    "saved_path = \"./dataset/toy_data/model.bin\"\n",
    "\n",
    "#enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)\n",
    "\n",
    "#save model\n",
    "model.save(saved_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-29 19:42:55,761 : INFO : loading Doc2Vec object from ./dataset/toy_data/model.bin\n",
      "2020-01-29 19:42:56,008 : INFO : loading vocabulary recursively from ./dataset/toy_data/model.bin.vocabulary.* with mmap=None\n",
      "2020-01-29 19:42:56,008 : INFO : loading trainables recursively from ./dataset/toy_data/model.bin.trainables.* with mmap=None\n",
      "2020-01-29 19:42:56,008 : INFO : loading wv recursively from ./dataset/toy_data/model.bin.wv.* with mmap=None\n",
      "2020-01-29 19:42:56,008 : INFO : loading docvecs recursively from ./dataset/toy_data/model.bin.docvecs.* with mmap=None\n",
      "2020-01-29 19:42:56,008 : INFO : loaded ./dataset/toy_data/model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test docs:\n",
      "[['the', 'cardigan', 'welsh', 'corgi', 'is', 'one', 'of', 'two', 'separate', 'dog', 'breeds', 'known', 'as', 'welsh', 'corgis', 'that', 'originated', 'in', 'wales', ',', 'the', 'other', 'being', 'the', 'pembroke', 'welsh', 'corgi', '.', 'it', 'is', 'one', 'of', 'the', 'oldest', 'herding', 'breeds', '.'], ['cardigan', 'welsh', 'corgis', 'can', 'be', 'extremely', 'loyal', 'family', 'dogs', '.', 'they', 'are', 'able', 'to', 'live', 'in', 'a', 'variety', 'of', 'settings', ',', 'from', 'apartments', 'to', 'farms', '.', 'for', 'their', 'size', ',', 'however', ',', 'they', 'need', 'a', 'surprising', 'amount', 'of', 'daily', 'physical', 'and', 'mental', 'stimulation', '.', 'cardigans', 'are', 'a', 'very', 'versatile', 'breed', 'and', 'a', 'wonderful', 'family', 'companion', '.'], ['pembrokes', 'and', 'cardigans', 'first', 'appeared', 'together', 'in', 'dog', 'shows', 'in', '1925', 'when', 'they', 'were', 'shown', 'under', 'the', 'rules', 'of', 'the', 'kennel', 'club', 'in', 'britain', '.', 'the', 'corgi', 'club', 'was', 'founded', 'in', 'december', ',', '1925', 'in', 'carmarthen', 'in', 'south', 'wales', '.', 'it', 'is', 'reported', 'that', 'the', 'local', 'members', 'favored', 'the', 'pembroke', 'breed', ',', 'so', 'a', 'club', 'for', 'cardigan', 'enthusiasts', 'was', 'founded', 'a', 'year', 'later', '-lrb-', '1926', '-rrb-', '.', 'both', 'groups', 'have', 'worked', 'hard', 'to', 'ensure', 'the', 'appearance', 'and', 'type', 'of', 'breed', 'are', 'standardized', 'through', 'careful', 'selective', 'breeding', '.', 'pembrokes', 'and', 'cardigans', 'were', 'officially', 'recognized', 'by', 'the', 'kennel', 'club', 'in', '1928', 'and', 'were', 'lumped', 'together', 'under', 'the', 'heading', 'welsh', 'corgis', '.', 'in', '1934', ',', 'the', 'two', 'breeds', 'were', 'recognized', 'individually', 'and', 'shown', 'separately', '.'], ['cardigans', 'are', 'said', 'to', 'originate', 'from', 'the', 'teckel', 'family', 'of', 'dogs', ',', 'which', 'also', 'produced', 'dachshunds', '.'], ['they', 'are', 'among', 'the', 'oldest', 'of', 'all', 'herding', 'breeds', ',', 'believed', 'to', 'have', 'been', 'in', 'existence', 'in', 'wales', 'for', 'over', '3,000', 'years', '.'], ['there', 'is', 'an', 'old', 'folktale', 'that', 'says', 'that', 'queen', 'victoria', 'was', 'traveling', 'down', 'a', 'country', 'road', 'one', 'day', 'until', 'her', 'carriage', 'came', 'up', 'on', 'a', 'fallen', 'tree', 'branch', '.', 'while', 'wondering', 'how', 'she', 'would', 'get', 'across', ',', 'a', 'fairy', 'came', 'out', 'of', 'nowhere', 'and', ',', 'in', 'order', 'to', 'assist', 'the', 'queen', ',', 'produced', 'two', 'corgis', 'out', 'of', 'thin', 'air', '.', 'one', 'was', 'the', 'pembroke', 'welsh', 'corgi', 'and', 'the', 'other', 'the', 'cardigan', 'welsh', 'corgi', '.', 'the', 'two', 'corgis', 'moved', 'the', 'tree', 'for', 'the', 'queen', ',', 'and', 'they', 'say', 'that', 'is', 'why', 'the', 'breed', 'is', 'currently', 'prized', 'by', 'the', 'british', 'queen', ',', 'elizabeth', 'ii', '.'], ['another', 'old', 'folktale', 'features', 'a', 'cardigan', 'welsh', 'corgi', 'battling', 'an', 'ancient', 'dragon', '.'], ['cardigans', 'have', 'never', 'had', 'the', 'same', 'popularity', 'as', 'pembrokes', ',', 'probably', 'due', 'to', 'the', 'influence', 'of', 'the', 'royal', 'family', '.', 'however', ',', 'they', 'have', 'found', 'their', 'own', 'place', 'in', 'many', 'parts', 'of', 'the', 'world', '.', 'cardigan', 'welsh', 'corgis', 'can', 'compete', 'in', 'dog', 'sports', 'also', 'known', 'as', 'dog', 'agility', 'trials', ',', 'obedience', ',', 'showmanship', ',', 'flyball', ',', 'tracking', ',', 'and', 'herding', 'events', '.'], ['the', 'phrase', '``', 'cor', 'gi', \"''\", 'is', 'sometimes', 'translated', 'as', '``', 'dwarf', 'dog', \"''\", 'in', 'welsh', '.', 'the', 'breed', 'was', 'often', 'called', '``', 'yard-long', 'dogs', \"''\", 'in', 'older', 'times', '.', 'today', \"'s\", 'name', 'comes', 'from', 'their', 'area', 'of', 'origin', ':', 'ceredigion', 'in', 'wales', '.'], ['modern', 'breed', '.']]\n"
     ]
    }
   ],
   "source": [
    "#python example to infer document vectors from trained doc2vec model\n",
    "import gensim.models as g\n",
    "import codecs\n",
    "\n",
    "#parameters\n",
    "model=\"./dataset/toy_data/model.bin\"\n",
    "test_docs=\"./dataset/toy_data/test_docs.txt\"\n",
    "output_file=\"./dataset/toy_data/test_vectors.txt\"\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "#load model\n",
    "m = g.Doc2Vec.load(model)\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n",
    "print('test docs:\\n{}'.format(test_docs))\n",
    "#infer test vectors\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
