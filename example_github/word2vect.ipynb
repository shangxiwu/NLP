{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)\n",
    "#> Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Saudis': 0,\n",
       " 'The': 1,\n",
       " 'a': 2,\n",
       " 'acknowledge': 3,\n",
       " 'are': 4,\n",
       " 'preparing': 5,\n",
       " 'report': 6,\n",
       " 'that': 7,\n",
       " 'will': 8,\n",
       " 'Jamal': 9,\n",
       " \"Khashoggi's\": 10,\n",
       " 'Saudi': 11,\n",
       " 'an': 12,\n",
       " 'death': 13,\n",
       " 'journalist': 14,\n",
       " 'of': 15,\n",
       " 'result': 16,\n",
       " 'the': 17,\n",
       " 'was': 18,\n",
       " 'intended': 19,\n",
       " 'interrogation': 20,\n",
       " 'lead': 21,\n",
       " 'one': 22,\n",
       " 'to': 23,\n",
       " 'went': 24,\n",
       " 'wrong,': 25,\n",
       " 'Turkey,': 26,\n",
       " 'abduction': 27,\n",
       " 'according': 28,\n",
       " 'from': 29,\n",
       " 'his': 30,\n",
       " 'sources.': 31,\n",
       " 'two': 32}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'graph': 33, 'in': 34, 'intersection': 35, 'paths': 36, 'trees': 37, 'Graph': 38, 'IV': 39, 'Widths': 40, 'and': 41, 'minors': 42, 'ordering': 43, 'quasi': 44, 'well': 45, 'A': 46, 'survey': 47}\n"
     ]
    }
   ],
   "source": [
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "\n",
    "\n",
    "# If you check now, the dictionary should have been updated with the new words (tokens).\n",
    "print(dictionary)\n",
    "#> Dictionary(45 unique tokens: ['Human', 'abc', 'applications', 'computer', 'for']...)\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n",
      "Features name:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n",
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "# CountVectorizer(stop_words=\"english\")\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\n",
    "\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"Features name:\\n{}\".format(vect.get_feature_names()))\n",
    "print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result1_vector shape: (5, 40)\n",
      "result2_vector shape: (5, 24)\n",
      "result3_vector shape: (5, 8)\n",
      "result4_vector shape: (5, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\.conda\\envs\\NLP_env\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass input=[\"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\", \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\", \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\", \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\", \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\"] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text =[\n",
    "      \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n",
    "      \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n",
    "      \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n",
    "      \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n",
    "      \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" \n",
    "     ]\n",
    "\n",
    "model1 = CountVectorizer(text,stop_words=[\"all\",\"in\",\"the\",\"is\",\"and\"])\n",
    "result1_vector = model1.fit_transform(text)\n",
    "print('result1_vector shape: {}'.format(result1_vector.shape))\n",
    "\n",
    "model2 = CountVectorizer(text,stop_words=\"english\")\n",
    "result2_vector = model2.fit_transform(text)\n",
    "print('result2_vector shape: {}'.format(result2_vector.shape))\n",
    "\n",
    "# use proportion here. Ignore terms that occurred in less than 25% of the documents\n",
    "#model3 = CountVectorizer(text,min_df=0.25)\n",
    "# ignore terms that appeared in less than n documents (can be proportion or absolute counts)\n",
    "model3 = CountVectorizer(text,min_df=2)\n",
    "result3_vector = model3.fit_transform(text)\n",
    "print('result3_vector shape: {}'.format(result3_vector.shape))\n",
    "\n",
    "# ignore terms that appeared in more than n documents (can be proportion or absolute counts)\n",
    "# use proportion here\n",
    "model4 = CountVectorizer(text,max_df=0.50)\n",
    "result4_vector = model4.fit_transform(text)\n",
    "print('result4_vector shape: {}'.format(result4_vector.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n",
      "-------output 0-th document tf-idf weight------\n",
      "and 0.0\n",
      "document 0.4387767428592343\n",
      "first 0.5419765697264572\n",
      "is 0.4387767428592343\n",
      "one 0.0\n",
      "second 0.0\n",
      "the 0.35872873824808993\n",
      "third 0.0\n",
      "this 0.4387767428592343\n",
      "-------output 1-th document tf-idf weight------\n",
      "and 0.0\n",
      "document 0.2723014675233404\n",
      "first 0.0\n",
      "is 0.2723014675233404\n",
      "one 0.0\n",
      "second 0.8532257361452786\n",
      "the 0.22262429232510395\n",
      "third 0.0\n",
      "this 0.2723014675233404\n",
      "-------output 2-th document tf-idf weight------\n",
      "and 0.5528053199908667\n",
      "document 0.0\n",
      "first 0.0\n",
      "is 0.0\n",
      "one 0.5528053199908667\n",
      "second 0.0\n",
      "the 0.2884767487500274\n",
      "third 0.5528053199908667\n",
      "this 0.0\n",
      "-------output 3-th document tf-idf weight------\n",
      "and 0.0\n",
      "document 0.4387767428592343\n",
      "first 0.5419765697264572\n",
      "is 0.4387767428592343\n",
      "one 0.0\n",
      "second 0.0\n",
      "the 0.35872873824808993\n",
      "third 0.0\n",
      "this 0.4387767428592343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()    \n",
    "\n",
    "X = vectorizer.fit_transform(corpus)       #先轉成 bag of words\n",
    "\n",
    "word = vectorizer.get_feature_names()\n",
    "print(word)\n",
    "\n",
    "print(X.toarray())\n",
    " \n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)    # \n",
    "tfidf_weight = tfidf.toarray() \n",
    "print(tfidf_weight)\n",
    "\n",
    "\n",
    "for i in range(len(tfidf_weight)):\n",
    "    print(\"-------output {}-th document tf-idf weight------\".format(i))\n",
    "    for j in range(len(word)):\n",
    "        print(word[j],tfidf_weight[i][j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n",
      "Vocabulary size: 14\n",
      "Vocabulary:\n",
      "['be fool', 'but the', 'doth think', 'fool doth', 'he is', 'himself to', 'is wise', 'knows himself', 'man knows', 'the fool', 'the wise', 'think he', 'to be', 'wise man']\n",
      "Transformed data (dense):\n",
      "[[0 0 1 1 1 0 1 0 0 1 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bards_words =[\"The fool doth think he is wise\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "vect1 = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(vect1.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect1.get_feature_names()))\n",
    "\n",
    "vect2 = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(vect2.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect2.get_feature_names()))\n",
    "print(\"Transformed data (dense):\\n{}\".format(vect2.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大年初六桃園八德大溪參香祈福祈求台灣平安桃園建設大步向前桃園市長鄭文燦今日上午前往桃園區清水巖下午前往八德區廣行宮大溪區中庄福德宮永安宮內柵仁安宮溪洲福山巖慈聖宮龍山寺參香並發送桃園福御守福袋給大年初六走春參香的市民朋友鄭市長表示大年初六是清水祖師聖誕也是開工的日子祈求清水祖師庇佑台灣平安健康武漢肺炎疫情不要蔓延到台灣也祈求桃園建設持續大步向前祝福所有鄉親信眾鼠來運轉今年的願望都能努力打拚實現鄭市長也呼籲市府將以高標準進行武漢肺炎防疫工作請市民朋友勤加洗手戴口罩量體溫如需前往人潮較多的地方記得要做好清潔消毒工作此外應避免聽信網路謠言造成恐慌亦可透過衛福部疾病管制署的疾管家獲知最新防疫資訊保護自身及周遭親友的健康安全鄭市長在中庄福德宮表示市府致力推動中庄地區發展中庄不只有調整池攔河堰中庄運動公園即將動工大漢溪邊也將興建堤防及防汛道路市民朋友無論在交通或觀光休憩都將更加便利另外國道號增設大鶯豐德交流道可行性研究已獲得交通部審議通過並陸續辦理相關建設計畫府會也將攜手合作讓交流道順利推動完成今日包括立法委員趙正宇市議員朱珍瑤呂林小鳳李柏坊陳治文黃家齊蔡永芳桃園工策會總幹事陳家濬市府民政局副局長林香美警察局督察長吳坤旭桃園區長陳玉明八德區長邱瑞朝大溪區長陳嘉聰桃園果菜市場公司董事長邱素芬大嵙崁文教基金會執行長李世明清水巖主委邱顯來廣行宮主委李秀明中庄福德宮主委沈琳容永安宮主委林繼雄內柵仁安宮主委簡子嚴溪洲福山巖主委楊賴傳慈聖宮主委蔡水木龍山寺董事長陳有盛等均一同參香\n"
     ]
    }
   ],
   "source": [
    "#https://medium.com/%E6%89%8B%E5%AF%AB%E7%AD%86%E8%A8%98/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-%E4%BD%BF%E7%94%A8-n-gram-%E5%AF%A6%E7%8F%BE%E8%BC%B8%E5%85%A5%E6%96%87%E5%AD%97%E9%A0%90%E6%B8%AC-10ac622aab7a\n",
    "\n",
    "from collections import Counter, namedtuple\n",
    "import json\n",
    "import re\n",
    "\n",
    "DATASET_DIR = 'dataset/WebNews.json'\n",
    "with open(DATASET_DIR, encoding = 'utf8') as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "seg_list = list(map(lambda d: d['detailcontent'], dataset))\n",
    "rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "seg_list = [rule.sub('', seg) for seg in seg_list]\n",
    "print(seg_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(documents, N=2):\n",
    "    ngram_prediction = dict()\n",
    "    total_grams = list()\n",
    "    words = list()\n",
    "    Word = namedtuple('Word', ['word', 'prob'])\n",
    "\n",
    "    for doc in documents:\n",
    "        split_words = ['<s>'] + list(doc) + ['</s>']\n",
    "        # 計算分子\n",
    "        [total_grams.append(tuple(split_words[i:i+N])) for i in range(len(split_words)-N+1)]\n",
    "        # 計算分母\n",
    "        [words.append(tuple(split_words[i:i+N-1])) for i in range(len(split_words)-N+2)]\n",
    "        \n",
    "    total_word_counter = Counter(total_grams)\n",
    "    word_counter = Counter(words)\n",
    "    \n",
    "    for key in total_word_counter:\n",
    "        word = ''.join(key[:N-1])\n",
    "        if word not in ngram_prediction:\n",
    "            ngram_prediction.update({word: set()})\n",
    "            \n",
    "        next_word_prob = total_word_counter[key]/word_counter[key[:N-1]]\n",
    "        w = Word(key[-1], '{:.3g}'.format(next_word_prob))\n",
    "        ngram_prediction[word].add(w)\n",
    "        \n",
    "    return ngram_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>大': {Word(word='嵙', prob='0.0323'), Word(word='腳', prob='0.0323'), Word(word='量', prob='0.0323'), Word(word='有', prob='0.0323'), Word(word='年', prob='0.419'), Word(word='園', prob='0.0968'), Word(word='溪', prob='0.355')}, '大年': {Word(word='初', prob='1')}, '年初': {Word(word='四', prob='0.109'), Word(word='第', prob='0.0182'), Word(word='期', prob='0.0182'), Word(word='五', prob='0.127'), Word(word='開', prob='0.0364'), Word(word='正', prob='0.0364'), Word(word='送', prob='0.0182'), Word(word='動', prob='0.0909'), Word(word='一', prob='0.0909'), Word(word='評', prob='0.0182'), Word(word='即', prob='0.0182'), Word(word='六', prob='0.0727'), Word(word='發', prob='0.0182'), Word(word='三', prob='0.0364'), Word(word='市', prob='0.0182'), Word(word='二', prob='0.0727'), Word(word='通', prob='0.0182'), Word(word='地', prob='0.0182'), Word(word='會', prob='0.0182'), Word(word='完', prob='0.109'), Word(word='將', prob='0.0182'), Word(word='與', prob='0.0182')}, '初六': {Word(word='是', prob='0.143'), Word(word='桃', prob='0.143'), Word(word='恢', prob='0.286'), Word(word='開', prob='0.143'), Word(word='蘆', prob='0.143'), Word(word='走', prob='0.143')}, '六桃': {Word(word='園', prob='1')}}\n"
     ]
    }
   ],
   "source": [
    "tri_prediction = ngram(seg_list, N=3)\n",
    "#print(tri_prediction)\n",
    "print(dict(list(tri_prediction.items())[0:5]))\n",
    "for word, ng in tri_prediction.items():\n",
    "    tri_prediction[word] = sorted(ng, key=lambda x: x.prob, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word: 隊, probability: 0.2\n",
      "next word: 首, probability: 0.143\n",
      "next word: 日, probability: 0.0571\n",
      "next word: 及, probability: 0.0571\n",
      "next word: 明, probability: 0.0571\n",
      "next word: 代, probability: 0.0571\n",
      "next word: 許, probability: 0.0286\n",
      "next word: 職, probability: 0.0286\n",
      "next word: 朴, probability: 0.0286\n",
      "next word: 語, probability: 0.0286\n"
     ]
    }
   ],
   "source": [
    "text = '韓國'\n",
    "next_words = list(tri_prediction[text])[:10]\n",
    "for next_word in next_words:\n",
    "    print('next word: {}, probability: {}'.format(next_word.word, next_word.prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW & Skip-gram  \n",
    "yield explain: https://pyzh.readthedocs.io/en/latest/the-python-yield-keyword-explained.html#id8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "data_file=\"./dataset/reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('./dataset/reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    print(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                print(\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "print(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Word2Vec model parameters\n",
    "\n",
    "size:\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me.\n",
    "\n",
    "window:\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n",
    "\n",
    "min_count:\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "workers:\n",
    "How many threads to use behind the scenes?\n",
    "\n",
    "sg: sg=1 means skip-gram and sg=0 menascbow\n",
    "'''\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10, sg=0)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word vector\n",
    "model.wv['dirty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove  \n",
    "ref: https://github.com/maciejkula/glove-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# download the model and return as object ready for use\n",
    "model_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "\n",
    "model_glove_twitter.wv.most_similar(\"pelosi\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.wv['dirty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.wv.most_similar(\"policies\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.wv.doesnt_match([\"trump\",\"bernie\",\"obama\",\"pelosi\",\"orange\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "#again, download and load the model\n",
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find similarity\n",
    "model_gigaword.wv.most_similar(positive=['dirty','grimy'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python example to train doc2vec model (with or without pre-trained word embeddings)\n",
    "\n",
    "import gensim.models as g\n",
    "import logging\n",
    "\n",
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0 #0 = dbow; 1 = dmpv\n",
    "worker_count = 1 #number of parallel processes\n",
    "\n",
    "#pretrained word embeddings\n",
    "pretrained_emb = \"./dataset/toy_data/pretrained_word_embeddings.txt\" #None if use without pretrained embeddings\n",
    "\n",
    "#input corpus\n",
    "train_corpus = \"./dataset/toy_data/train_docs.txt\"\n",
    "\n",
    "#output model\n",
    "saved_path = \"./dataset/toy_data/model.bin\"\n",
    "\n",
    "#enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)\n",
    "\n",
    "#save model\n",
    "model.save(saved_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python example to infer document vectors from trained doc2vec model\n",
    "import gensim.models as g\n",
    "import codecs\n",
    "\n",
    "#parameters\n",
    "model=\"./dataset/toy_data/model.bin\"\n",
    "test_docs=\"./dataset/toy_data/test_docs.txt\"\n",
    "output_file=\"./dataset/toy_data/test_vectors.txt\"\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "#load model\n",
    "m = g.Doc2Vec.load(model)\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n",
    "print('test docs:\\n{}'.format(test_docs))\n",
    "#infer test vectors\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
