{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)\n",
    "#> Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Saudis': 0,\n",
       " 'The': 1,\n",
       " 'a': 2,\n",
       " 'acknowledge': 3,\n",
       " 'are': 4,\n",
       " 'preparing': 5,\n",
       " 'report': 6,\n",
       " 'that': 7,\n",
       " 'will': 8,\n",
       " 'Jamal': 9,\n",
       " \"Khashoggi's\": 10,\n",
       " 'Saudi': 11,\n",
       " 'an': 12,\n",
       " 'death': 13,\n",
       " 'journalist': 14,\n",
       " 'of': 15,\n",
       " 'result': 16,\n",
       " 'the': 17,\n",
       " 'was': 18,\n",
       " 'intended': 19,\n",
       " 'interrogation': 20,\n",
       " 'lead': 21,\n",
       " 'one': 22,\n",
       " 'to': 23,\n",
       " 'went': 24,\n",
       " 'wrong,': 25,\n",
       " 'Turkey,': 26,\n",
       " 'abduction': 27,\n",
       " 'according': 28,\n",
       " 'from': 29,\n",
       " 'his': 30,\n",
       " 'sources.': 31,\n",
       " 'two': 32}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'graph': 33, 'in': 34, 'intersection': 35, 'paths': 36, 'trees': 37, 'Graph': 38, 'IV': 39, 'Widths': 40, 'and': 41, 'minors': 42, 'ordering': 43, 'quasi': 44, 'well': 45, 'A': 46, 'survey': 47}\n"
     ]
    }
   ],
   "source": [
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "\n",
    "\n",
    "# If you check now, the dictionary should have been updated with the new words (tokens).\n",
    "print(dictionary)\n",
    "#> Dictionary(45 unique tokens: ['Human', 'abc', 'applications', 'computer', 'for']...)\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n",
      "Features name:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n",
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "# CountVectorizer(stop_words=\"english\")\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\n",
    "\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"Features name:\\n{}\".format(vect.get_feature_names()))\n",
    "print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result1_vector shape: (5, 40)\n",
      "result2_vector shape: (5, 24)\n",
      "result3_vector shape: (5, 8)\n",
      "result4_vector shape: (5, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\.conda\\envs\\NLP_env\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass input=[\"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\", \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\", \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\", \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\", \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\"] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text =[\n",
    "      \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n",
    "      \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n",
    "      \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n",
    "      \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n",
    "      \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" \n",
    "     ]\n",
    "\n",
    "model1 = CountVectorizer(text,stop_words=[\"all\",\"in\",\"the\",\"is\",\"and\"])\n",
    "result1_vector = model1.fit_transform(text)\n",
    "print('result1_vector shape: {}'.format(result1_vector.shape))\n",
    "\n",
    "model2 = CountVectorizer(text,stop_words=\"english\")\n",
    "result2_vector = model2.fit_transform(text)\n",
    "print('result2_vector shape: {}'.format(result2_vector.shape))\n",
    "\n",
    "# use proportion here. Ignore terms that occurred in less than 25% of the documents\n",
    "#model3 = CountVectorizer(text,min_df=0.25)\n",
    "# ignore terms that appeared in less than n documents (can be proportion or absolute counts)\n",
    "model3 = CountVectorizer(text,min_df=2)\n",
    "result3_vector = model3.fit_transform(text)\n",
    "print('result3_vector shape: {}'.format(result3_vector.shape))\n",
    "\n",
    "# ignore terms that appeared in more than n documents (can be proportion or absolute counts)\n",
    "# use proportion here\n",
    "model4 = CountVectorizer(text,max_df=0.50)\n",
    "result4_vector = model4.fit_transform(text)\n",
    "print('result4_vector shape: {}'.format(result4_vector.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n",
      "-------output 0-th document tf-idf weight------\n",
      "and 0.0\n",
      "document 0.4387767428592343\n",
      "first 0.5419765697264572\n",
      "is 0.4387767428592343\n",
      "one 0.0\n",
      "second 0.0\n",
      "the 0.35872873824808993\n",
      "third 0.0\n",
      "this 0.4387767428592343\n",
      "-------output 1-th document tf-idf weight------\n",
      "and 0.0\n",
      "document 0.2723014675233404\n",
      "first 0.0\n",
      "is 0.2723014675233404\n",
      "one 0.0\n",
      "second 0.8532257361452786\n",
      "the 0.22262429232510395\n",
      "third 0.0\n",
      "this 0.2723014675233404\n",
      "-------output 2-th document tf-idf weight------\n",
      "and 0.5528053199908667\n",
      "document 0.0\n",
      "first 0.0\n",
      "is 0.0\n",
      "one 0.5528053199908667\n",
      "second 0.0\n",
      "the 0.2884767487500274\n",
      "third 0.5528053199908667\n",
      "this 0.0\n",
      "-------output 3-th document tf-idf weight------\n",
      "and 0.0\n",
      "document 0.4387767428592343\n",
      "first 0.5419765697264572\n",
      "is 0.4387767428592343\n",
      "one 0.0\n",
      "second 0.0\n",
      "the 0.35872873824808993\n",
      "third 0.0\n",
      "this 0.4387767428592343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()    \n",
    "\n",
    "X = vectorizer.fit_transform(corpus)       #先轉成 bag of words\n",
    "\n",
    "word = vectorizer.get_feature_names()\n",
    "print(word)\n",
    "\n",
    "print(X.toarray())\n",
    " \n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)    # \n",
    "tfidf_weight = tfidf.toarray() \n",
    "print(tfidf_weight)\n",
    "\n",
    "\n",
    "for i in range(len(tfidf_weight)):\n",
    "    print(\"-------output {}-th document tf-idf weight------\".format(i))\n",
    "    for j in range(len(word)):\n",
    "        print(word[j],tfidf_weight[i][j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n",
      "Vocabulary size: 14\n",
      "Vocabulary:\n",
      "['be fool', 'but the', 'doth think', 'fool doth', 'he is', 'himself to', 'is wise', 'knows himself', 'man knows', 'the fool', 'the wise', 'think he', 'to be', 'wise man']\n",
      "Transformed data (dense):\n",
      "[[0 0 1 1 1 0 1 0 0 1 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bards_words =[\"The fool doth think he is wise\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "vect1 = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(vect1.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect1.get_feature_names()))\n",
    "\n",
    "vect2 = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(vect2.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect2.get_feature_names()))\n",
    "print(\"Transformed data (dense):\\n{}\".format(vect2.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大年初六桃園八德大溪參香祈福祈求台灣平安桃園建設大步向前桃園市長鄭文燦今日上午前往桃園區清水巖下午前往八德區廣行宮大溪區中庄福德宮永安宮內柵仁安宮溪洲福山巖慈聖宮龍山寺參香並發送桃園福御守福袋給大年初六走春參香的市民朋友鄭市長表示大年初六是清水祖師聖誕也是開工的日子祈求清水祖師庇佑台灣平安健康武漢肺炎疫情不要蔓延到台灣也祈求桃園建設持續大步向前祝福所有鄉親信眾鼠來運轉今年的願望都能努力打拚實現鄭市長也呼籲市府將以高標準進行武漢肺炎防疫工作請市民朋友勤加洗手戴口罩量體溫如需前往人潮較多的地方記得要做好清潔消毒工作此外應避免聽信網路謠言造成恐慌亦可透過衛福部疾病管制署的疾管家獲知最新防疫資訊保護自身及周遭親友的健康安全鄭市長在中庄福德宮表示市府致力推動中庄地區發展中庄不只有調整池攔河堰中庄運動公園即將動工大漢溪邊也將興建堤防及防汛道路市民朋友無論在交通或觀光休憩都將更加便利另外國道號增設大鶯豐德交流道可行性研究已獲得交通部審議通過並陸續辦理相關建設計畫府會也將攜手合作讓交流道順利推動完成今日包括立法委員趙正宇市議員朱珍瑤呂林小鳳李柏坊陳治文黃家齊蔡永芳桃園工策會總幹事陳家濬市府民政局副局長林香美警察局督察長吳坤旭桃園區長陳玉明八德區長邱瑞朝大溪區長陳嘉聰桃園果菜市場公司董事長邱素芬大嵙崁文教基金會執行長李世明清水巖主委邱顯來廣行宮主委李秀明中庄福德宮主委沈琳容永安宮主委林繼雄內柵仁安宮主委簡子嚴溪洲福山巖主委楊賴傳慈聖宮主委蔡水木龍山寺董事長陳有盛等均一同參香\n"
     ]
    }
   ],
   "source": [
    "#https://medium.com/%E6%89%8B%E5%AF%AB%E7%AD%86%E8%A8%98/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-%E4%BD%BF%E7%94%A8-n-gram-%E5%AF%A6%E7%8F%BE%E8%BC%B8%E5%85%A5%E6%96%87%E5%AD%97%E9%A0%90%E6%B8%AC-10ac622aab7a\n",
    "\n",
    "from collections import Counter, namedtuple\n",
    "import json\n",
    "import re\n",
    "\n",
    "DATASET_DIR = 'dataset/WebNews.json'\n",
    "with open(DATASET_DIR, encoding = 'utf8') as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "seg_list = list(map(lambda d: d['detailcontent'], dataset))\n",
    "rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "seg_list = [rule.sub('', seg) for seg in seg_list]\n",
    "print(seg_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(documents, N=2):\n",
    "    ngram_prediction = dict()\n",
    "    total_grams = list()\n",
    "    words = list()\n",
    "    Word = namedtuple('Word', ['word', 'prob'])\n",
    "\n",
    "    for doc in documents:\n",
    "        split_words = ['<s>'] + list(doc) + ['</s>']\n",
    "        # 計算分子\n",
    "        [total_grams.append(tuple(split_words[i:i+N])) for i in range(len(split_words)-N+1)]\n",
    "        # 計算分母\n",
    "        [words.append(tuple(split_words[i:i+N-1])) for i in range(len(split_words)-N+2)]\n",
    "        \n",
    "    total_word_counter = Counter(total_grams)\n",
    "    word_counter = Counter(words)\n",
    "    \n",
    "    for key in total_word_counter:\n",
    "        word = ''.join(key[:N-1])\n",
    "        if word not in ngram_prediction:\n",
    "            ngram_prediction.update({word: set()})\n",
    "            \n",
    "        next_word_prob = total_word_counter[key]/word_counter[key[:N-1]]\n",
    "        w = Word(key[-1], '{:.3g}'.format(next_word_prob))\n",
    "        ngram_prediction[word].add(w)\n",
    "        \n",
    "    return ngram_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>大': {Word(word='嵙', prob='0.0323'), Word(word='腳', prob='0.0323'), Word(word='量', prob='0.0323'), Word(word='有', prob='0.0323'), Word(word='年', prob='0.419'), Word(word='園', prob='0.0968'), Word(word='溪', prob='0.355')}, '大年': {Word(word='初', prob='1')}, '年初': {Word(word='四', prob='0.109'), Word(word='第', prob='0.0182'), Word(word='期', prob='0.0182'), Word(word='五', prob='0.127'), Word(word='開', prob='0.0364'), Word(word='正', prob='0.0364'), Word(word='送', prob='0.0182'), Word(word='動', prob='0.0909'), Word(word='一', prob='0.0909'), Word(word='評', prob='0.0182'), Word(word='即', prob='0.0182'), Word(word='六', prob='0.0727'), Word(word='發', prob='0.0182'), Word(word='三', prob='0.0364'), Word(word='市', prob='0.0182'), Word(word='二', prob='0.0727'), Word(word='通', prob='0.0182'), Word(word='地', prob='0.0182'), Word(word='會', prob='0.0182'), Word(word='完', prob='0.109'), Word(word='將', prob='0.0182'), Word(word='與', prob='0.0182')}, '初六': {Word(word='是', prob='0.143'), Word(word='桃', prob='0.143'), Word(word='恢', prob='0.286'), Word(word='開', prob='0.143'), Word(word='蘆', prob='0.143'), Word(word='走', prob='0.143')}, '六桃': {Word(word='園', prob='1')}}\n"
     ]
    }
   ],
   "source": [
    "tri_prediction = ngram(seg_list, N=3)\n",
    "#print(tri_prediction)\n",
    "print(dict(list(tri_prediction.items())[0:5]))\n",
    "for word, ng in tri_prediction.items():\n",
    "    tri_prediction[word] = sorted(ng, key=lambda x: x.prob, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word: 隊, probability: 0.2\n",
      "next word: 首, probability: 0.143\n",
      "next word: 日, probability: 0.0571\n",
      "next word: 及, probability: 0.0571\n",
      "next word: 明, probability: 0.0571\n",
      "next word: 代, probability: 0.0571\n",
      "next word: 許, probability: 0.0286\n",
      "next word: 職, probability: 0.0286\n",
      "next word: 朴, probability: 0.0286\n",
      "next word: 語, probability: 0.0286\n"
     ]
    }
   ],
   "source": [
    "text = '韓國'\n",
    "next_words = list(tri_prediction[text])[:10]\n",
    "for next_word in next_words:\n",
    "    print('next word: {}, probability: {}'.format(next_word.word, next_word.prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW & Skip-gram  \n",
    "yield explain: https://pyzh.readthedocs.io/en/latest/the-python-yield-keyword-explained.html#id8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "data_file=\"./dataset/reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('./dataset/reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file ./dataset/reviews_data.txt.gz...this may take a while\n",
      "read 0 reviews\n",
      "read 10000 reviews\n",
      "read 20000 reviews\n",
      "read 30000 reviews\n",
      "read 40000 reviews\n",
      "read 50000 reviews\n",
      "read 60000 reviews\n",
      "read 70000 reviews\n",
      "read 80000 reviews\n",
      "read 90000 reviews\n",
      "read 100000 reviews\n",
      "read 110000 reviews\n",
      "read 120000 reviews\n",
      "read 130000 reviews\n",
      "read 140000 reviews\n",
      "read 150000 reviews\n",
      "read 160000 reviews\n",
      "read 170000 reviews\n",
      "read 180000 reviews\n",
      "read 190000 reviews\n",
      "read 200000 reviews\n",
      "read 210000 reviews\n",
      "read 220000 reviews\n",
      "read 230000 reviews\n",
      "read 240000 reviews\n",
      "read 250000 reviews\n",
      "Done reading data file\n"
     ]
    }
   ],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    print(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                print(\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "print(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oct',\n",
       " 'nice',\n",
       " 'trendy',\n",
       " 'hotel',\n",
       " 'location',\n",
       " 'not',\n",
       " 'too',\n",
       " 'bad',\n",
       " 'stayed',\n",
       " 'in',\n",
       " 'this',\n",
       " 'hotel',\n",
       " 'for',\n",
       " 'one',\n",
       " 'night',\n",
       " 'as',\n",
       " 'this',\n",
       " 'is',\n",
       " 'fairly',\n",
       " 'new',\n",
       " 'place',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'taxi',\n",
       " 'drivers',\n",
       " 'did',\n",
       " 'not',\n",
       " 'know',\n",
       " 'where',\n",
       " 'it',\n",
       " 'was',\n",
       " 'and',\n",
       " 'or',\n",
       " 'did',\n",
       " 'not',\n",
       " 'want',\n",
       " 'to',\n",
       " 'drive',\n",
       " 'there',\n",
       " 'once',\n",
       " 'have',\n",
       " 'eventually',\n",
       " 'arrived',\n",
       " 'at',\n",
       " 'the',\n",
       " 'hotel',\n",
       " 'was',\n",
       " 'very',\n",
       " 'pleasantly',\n",
       " 'surprised',\n",
       " 'with',\n",
       " 'the',\n",
       " 'decor',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lobby',\n",
       " 'ground',\n",
       " 'floor',\n",
       " 'area',\n",
       " 'it',\n",
       " 'was',\n",
       " 'very',\n",
       " 'stylish',\n",
       " 'and',\n",
       " 'modern',\n",
       " 'found',\n",
       " 'the',\n",
       " 'reception',\n",
       " 'staff',\n",
       " 'geeting',\n",
       " 'me',\n",
       " 'with',\n",
       " 'aloha',\n",
       " 'bit',\n",
       " 'out',\n",
       " 'of',\n",
       " 'place',\n",
       " 'but',\n",
       " 'guess',\n",
       " 'they',\n",
       " 'are',\n",
       " 'briefed',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'up',\n",
       " 'the',\n",
       " 'coroporate',\n",
       " 'image',\n",
       " 'as',\n",
       " 'have',\n",
       " 'starwood',\n",
       " 'preferred',\n",
       " 'guest',\n",
       " 'member',\n",
       " 'was',\n",
       " 'given',\n",
       " 'small',\n",
       " 'gift',\n",
       " 'upon',\n",
       " 'check',\n",
       " 'in',\n",
       " 'it',\n",
       " 'was',\n",
       " 'only',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'fridge',\n",
       " 'magnets',\n",
       " 'in',\n",
       " 'gift',\n",
       " 'box',\n",
       " 'but',\n",
       " 'nevertheless',\n",
       " 'nice',\n",
       " 'gesture',\n",
       " 'my',\n",
       " 'room',\n",
       " 'was',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'roomy',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tea',\n",
       " 'and',\n",
       " 'coffee',\n",
       " 'facilities',\n",
       " 'in',\n",
       " 'each',\n",
       " 'room',\n",
       " 'and',\n",
       " 'you',\n",
       " 'get',\n",
       " 'two',\n",
       " 'complimentary',\n",
       " 'bottles',\n",
       " 'of',\n",
       " 'water',\n",
       " 'plus',\n",
       " 'some',\n",
       " 'toiletries',\n",
       " 'by',\n",
       " 'bliss',\n",
       " 'the',\n",
       " 'location',\n",
       " 'is',\n",
       " 'not',\n",
       " 'great',\n",
       " 'it',\n",
       " 'is',\n",
       " 'at',\n",
       " 'the',\n",
       " 'last',\n",
       " 'metro',\n",
       " 'stop',\n",
       " 'and',\n",
       " 'you',\n",
       " 'then',\n",
       " 'need',\n",
       " 'to',\n",
       " 'take',\n",
       " 'taxi',\n",
       " 'but',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are',\n",
       " 'not',\n",
       " 'planning',\n",
       " 'on',\n",
       " 'going',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'historic',\n",
       " 'sites',\n",
       " 'in',\n",
       " 'beijing',\n",
       " 'then',\n",
       " 'you',\n",
       " 'will',\n",
       " 'be',\n",
       " 'ok',\n",
       " 'chose',\n",
       " 'to',\n",
       " 'have',\n",
       " 'some',\n",
       " 'breakfast',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hotel',\n",
       " 'which',\n",
       " 'was',\n",
       " 'really',\n",
       " 'tasty',\n",
       " 'and',\n",
       " 'there',\n",
       " 'was',\n",
       " 'good',\n",
       " 'selection',\n",
       " 'of',\n",
       " 'dishes',\n",
       " 'there',\n",
       " 'are',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'use',\n",
       " 'in',\n",
       " 'the',\n",
       " 'communal',\n",
       " 'area',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'pool',\n",
       " 'table',\n",
       " 'there',\n",
       " 'is',\n",
       " 'also',\n",
       " 'small',\n",
       " 'swimming',\n",
       " 'pool',\n",
       " 'and',\n",
       " 'gym',\n",
       " 'area',\n",
       " 'would',\n",
       " 'definitely',\n",
       " 'stay',\n",
       " 'in',\n",
       " 'this',\n",
       " 'hotel',\n",
       " 'again',\n",
       " 'but',\n",
       " 'only',\n",
       " 'if',\n",
       " 'did',\n",
       " 'not',\n",
       " 'plan',\n",
       " 'to',\n",
       " 'travel',\n",
       " 'to',\n",
       " 'central',\n",
       " 'beijing',\n",
       " 'as',\n",
       " 'it',\n",
       " 'can',\n",
       " 'take',\n",
       " 'long',\n",
       " 'time',\n",
       " 'the',\n",
       " 'location',\n",
       " 'is',\n",
       " 'ok',\n",
       " 'if',\n",
       " 'you',\n",
       " 'plan',\n",
       " 'to',\n",
       " 'do',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'shopping',\n",
       " 'as',\n",
       " 'there',\n",
       " 'is',\n",
       " 'big',\n",
       " 'shopping',\n",
       " 'centre',\n",
       " 'just',\n",
       " 'few',\n",
       " 'minutes',\n",
       " 'away',\n",
       " 'from',\n",
       " 'the',\n",
       " 'hotel',\n",
       " 'and',\n",
       " 'there',\n",
       " 'are',\n",
       " 'plenty',\n",
       " 'of',\n",
       " 'eating',\n",
       " 'options',\n",
       " 'around',\n",
       " 'including',\n",
       " 'restaurants',\n",
       " 'that',\n",
       " 'serve',\n",
       " 'dog',\n",
       " 'meat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 18:55:46,524 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2020-09-02 18:55:46,525 : INFO : collecting all words and their counts\n",
      "2020-09-02 18:55:46,525 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-09-02 18:55:46,856 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2020-09-02 18:55:47,081 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2020-09-02 18:55:47,332 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2020-09-02 18:55:47,573 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2020-09-02 18:55:47,849 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2020-09-02 18:55:48,100 : INFO : PROGRESS: at sentence #60000, processed 11013726 words, keeping 76786 word types\n",
      "2020-09-02 18:55:48,312 : INFO : PROGRESS: at sentence #70000, processed 12637528 words, keeping 83199 word types\n",
      "2020-09-02 18:55:48,506 : INFO : PROGRESS: at sentence #80000, processed 14099754 words, keeping 88459 word types\n",
      "2020-09-02 18:55:48,709 : INFO : PROGRESS: at sentence #90000, processed 15662152 words, keeping 93357 word types\n",
      "2020-09-02 18:55:48,906 : INFO : PROGRESS: at sentence #100000, processed 17164490 words, keeping 97886 word types\n",
      "2020-09-02 18:55:49,102 : INFO : PROGRESS: at sentence #110000, processed 18652295 words, keeping 102132 word types\n",
      "2020-09-02 18:55:49,309 : INFO : PROGRESS: at sentence #120000, processed 20152532 words, keeping 105923 word types\n",
      "2020-09-02 18:55:49,513 : INFO : PROGRESS: at sentence #130000, processed 21684333 words, keeping 110104 word types\n",
      "2020-09-02 18:55:49,722 : INFO : PROGRESS: at sentence #140000, processed 23330209 words, keeping 114108 word types\n",
      "2020-09-02 18:55:49,914 : INFO : PROGRESS: at sentence #150000, processed 24838757 words, keeping 118174 word types\n",
      "2020-09-02 18:55:50,115 : INFO : PROGRESS: at sentence #160000, processed 26390913 words, keeping 118670 word types\n",
      "2020-09-02 18:55:50,315 : INFO : PROGRESS: at sentence #170000, processed 27913919 words, keeping 123356 word types\n",
      "2020-09-02 18:55:50,525 : INFO : PROGRESS: at sentence #180000, processed 29535615 words, keeping 126748 word types\n",
      "2020-09-02 18:55:50,730 : INFO : PROGRESS: at sentence #190000, processed 31096462 words, keeping 129847 word types\n",
      "2020-09-02 18:55:50,955 : INFO : PROGRESS: at sentence #200000, processed 32805274 words, keeping 133255 word types\n",
      "2020-09-02 18:55:51,160 : INFO : PROGRESS: at sentence #210000, processed 34434201 words, keeping 136364 word types\n",
      "2020-09-02 18:55:51,376 : INFO : PROGRESS: at sentence #220000, processed 36083485 words, keeping 139418 word types\n",
      "2020-09-02 18:55:51,570 : INFO : PROGRESS: at sentence #230000, processed 37571765 words, keeping 142399 word types\n",
      "2020-09-02 18:55:51,774 : INFO : PROGRESS: at sentence #240000, processed 39138193 words, keeping 145232 word types\n",
      "2020-09-02 18:55:51,979 : INFO : PROGRESS: at sentence #250000, processed 40695052 words, keeping 147966 word types\n",
      "2020-09-02 18:55:52,093 : INFO : collected 150059 word types from a corpus of 41519358 raw words and 255404 sentences\n",
      "2020-09-02 18:55:52,094 : INFO : Loading a fresh vocabulary\n",
      "2020-09-02 18:55:52,649 : INFO : effective_min_count=2 retains 70537 unique words (47% of original 150059, drops 79522)\n",
      "2020-09-02 18:55:52,649 : INFO : effective_min_count=2 leaves 41439836 word corpus (99% of original 41519358, drops 79522)\n",
      "2020-09-02 18:55:52,818 : INFO : deleting the raw counts dictionary of 150059 items\n",
      "2020-09-02 18:55:52,821 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2020-09-02 18:55:52,822 : INFO : downsampling leaves estimated 30349251 word corpus (73.2% of prior 41439836)\n",
      "2020-09-02 18:55:52,996 : INFO : estimated required memory for 70537 words and 150 dimensions: 119912900 bytes\n",
      "2020-09-02 18:55:52,996 : INFO : resetting layer weights\n",
      "2020-09-02 18:56:03,987 : INFO : training model with 10 workers on 70537 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-09-02 18:56:05,004 : INFO : EPOCH 1 - PROGRESS: at 7.10% examples, 2172899 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:56:06,008 : INFO : EPOCH 1 - PROGRESS: at 13.19% examples, 2160365 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:07,011 : INFO : EPOCH 1 - PROGRESS: at 19.40% examples, 2186209 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:08,012 : INFO : EPOCH 1 - PROGRESS: at 25.47% examples, 2170363 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:56:09,014 : INFO : EPOCH 1 - PROGRESS: at 33.44% examples, 2175407 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:10,017 : INFO : EPOCH 1 - PROGRESS: at 41.38% examples, 2187141 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:11,017 : INFO : EPOCH 1 - PROGRESS: at 48.87% examples, 2175198 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:12,019 : INFO : EPOCH 1 - PROGRESS: at 56.46% examples, 2181696 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:13,022 : INFO : EPOCH 1 - PROGRESS: at 64.11% examples, 2179650 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:14,030 : INFO : EPOCH 1 - PROGRESS: at 71.10% examples, 2172793 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:56:15,036 : INFO : EPOCH 1 - PROGRESS: at 78.52% examples, 2178232 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:16,038 : INFO : EPOCH 1 - PROGRESS: at 85.37% examples, 2172078 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:17,039 : INFO : EPOCH 1 - PROGRESS: at 93.15% examples, 2175883 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:17,902 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:56:17,910 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:56:17,916 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:56:17,918 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:56:17,922 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:56:17,923 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:56:17,924 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:56:17,924 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:56:17,930 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:56:17,931 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:56:17,931 : INFO : EPOCH - 1 : training on 41519358 raw words (30347132 effective words) took 13.9s, 2177981 effective words/s\n",
      "2020-09-02 18:56:18,945 : INFO : EPOCH 2 - PROGRESS: at 7.09% examples, 2166045 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:19,949 : INFO : EPOCH 2 - PROGRESS: at 13.44% examples, 2195849 words/s, in_qsize 20, out_qsize 2\n",
      "2020-09-02 18:56:20,953 : INFO : EPOCH 2 - PROGRESS: at 19.34% examples, 2172433 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:56:21,956 : INFO : EPOCH 2 - PROGRESS: at 25.53% examples, 2172305 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:22,960 : INFO : EPOCH 2 - PROGRESS: at 33.48% examples, 2174643 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:56:23,964 : INFO : EPOCH 2 - PROGRESS: at 41.33% examples, 2181042 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:24,966 : INFO : EPOCH 2 - PROGRESS: at 49.18% examples, 2183954 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:25,975 : INFO : EPOCH 2 - PROGRESS: at 56.72% examples, 2186122 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:26,975 : INFO : EPOCH 2 - PROGRESS: at 64.17% examples, 2177666 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:27,977 : INFO : EPOCH 2 - PROGRESS: at 71.38% examples, 2178234 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:56:28,982 : INFO : EPOCH 2 - PROGRESS: at 78.47% examples, 2175560 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:29,986 : INFO : EPOCH 2 - PROGRESS: at 85.31% examples, 2168635 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:56:30,991 : INFO : EPOCH 2 - PROGRESS: at 93.07% examples, 2171520 words/s, in_qsize 17, out_qsize 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 18:56:31,919 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:56:31,924 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:56:31,925 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:56:31,929 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:56:31,931 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:56:31,932 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:56:31,936 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:56:31,939 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:56:31,941 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:56:31,943 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:56:31,943 : INFO : EPOCH - 2 : training on 41519358 raw words (30349950 effective words) took 14.0s, 2166712 effective words/s\n",
      "2020-09-02 18:56:32,951 : INFO : EPOCH 3 - PROGRESS: at 7.12% examples, 2183822 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:33,954 : INFO : EPOCH 3 - PROGRESS: at 13.38% examples, 2192526 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:34,954 : INFO : EPOCH 3 - PROGRESS: at 19.42% examples, 2193114 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:35,954 : INFO : EPOCH 3 - PROGRESS: at 25.67% examples, 2188887 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:36,957 : INFO : EPOCH 3 - PROGRESS: at 33.38% examples, 2174288 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:37,958 : INFO : EPOCH 3 - PROGRESS: at 40.62% examples, 2158821 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:56:38,962 : INFO : EPOCH 3 - PROGRESS: at 48.49% examples, 2162307 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:39,964 : INFO : EPOCH 3 - PROGRESS: at 56.02% examples, 2166204 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:40,967 : INFO : EPOCH 3 - PROGRESS: at 63.68% examples, 2169219 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:41,979 : INFO : EPOCH 3 - PROGRESS: at 70.90% examples, 2166819 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:56:42,979 : INFO : EPOCH 3 - PROGRESS: at 77.94% examples, 2163585 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:43,983 : INFO : EPOCH 3 - PROGRESS: at 85.01% examples, 2163729 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:44,987 : INFO : EPOCH 3 - PROGRESS: at 92.61% examples, 2163042 words/s, in_qsize 20, out_qsize 2\n",
      "2020-09-02 18:56:45,968 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:56:45,971 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:56:45,972 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:56:45,979 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:56:45,980 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:56:45,981 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:56:45,982 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:56:45,983 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:56:45,985 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:56:45,989 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 2161444 words/s, in_qsize 0, out_qsize 1\n",
      "2020-09-02 18:56:45,990 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:56:45,991 : INFO : EPOCH - 3 : training on 41519358 raw words (30351707 effective words) took 14.0s, 2161227 effective words/s\n",
      "2020-09-02 18:56:46,996 : INFO : EPOCH 4 - PROGRESS: at 6.94% examples, 2133336 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:47,997 : INFO : EPOCH 4 - PROGRESS: at 12.84% examples, 2118094 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:56:49,006 : INFO : EPOCH 4 - PROGRESS: at 18.90% examples, 2115773 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:50,010 : INFO : EPOCH 4 - PROGRESS: at 24.90% examples, 2131678 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:51,015 : INFO : EPOCH 4 - PROGRESS: at 32.96% examples, 2144536 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:52,015 : INFO : EPOCH 4 - PROGRESS: at 40.67% examples, 2157084 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:53,022 : INFO : EPOCH 4 - PROGRESS: at 48.18% examples, 2145991 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:54,023 : INFO : EPOCH 4 - PROGRESS: at 55.70% examples, 2151956 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:55,031 : INFO : EPOCH 4 - PROGRESS: at 63.44% examples, 2158341 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:56,038 : INFO : EPOCH 4 - PROGRESS: at 70.76% examples, 2158857 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:57,043 : INFO : EPOCH 4 - PROGRESS: at 77.97% examples, 2161107 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:58,047 : INFO : EPOCH 4 - PROGRESS: at 84.83% examples, 2155403 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:56:59,052 : INFO : EPOCH 4 - PROGRESS: at 92.59% examples, 2159134 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:00,010 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:57:00,013 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:57:00,016 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:57:00,019 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:57:00,020 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:57:00,023 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:57:00,025 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:57:00,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:57:00,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:57:00,029 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:57:00,030 : INFO : EPOCH - 4 : training on 41519358 raw words (30345870 effective words) took 14.0s, 2162233 effective words/s\n",
      "2020-09-02 18:57:01,041 : INFO : EPOCH 5 - PROGRESS: at 6.40% examples, 1961095 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:02,042 : INFO : EPOCH 5 - PROGRESS: at 12.16% examples, 1993432 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:03,047 : INFO : EPOCH 5 - PROGRESS: at 18.52% examples, 2067394 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:04,049 : INFO : EPOCH 5 - PROGRESS: at 24.22% examples, 2076560 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:57:05,053 : INFO : EPOCH 5 - PROGRESS: at 32.35% examples, 2110895 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:06,059 : INFO : EPOCH 5 - PROGRESS: at 39.98% examples, 2123760 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:07,065 : INFO : EPOCH 5 - PROGRESS: at 48.02% examples, 2138670 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:08,071 : INFO : EPOCH 5 - PROGRESS: at 55.45% examples, 2140621 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:57:09,077 : INFO : EPOCH 5 - PROGRESS: at 62.73% examples, 2136371 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:10,079 : INFO : EPOCH 5 - PROGRESS: at 70.46% examples, 2149394 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:11,085 : INFO : EPOCH 5 - PROGRESS: at 77.70% examples, 2153221 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:12,092 : INFO : EPOCH 5 - PROGRESS: at 84.74% examples, 2152428 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:13,093 : INFO : EPOCH 5 - PROGRESS: at 92.46% examples, 2156050 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:14,097 : INFO : EPOCH 5 - PROGRESS: at 99.68% examples, 2151723 words/s, in_qsize 11, out_qsize 2\n",
      "2020-09-02 18:57:14,106 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:57:14,110 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:57:14,112 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:57:14,114 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 18:57:14,116 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:57:14,118 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:57:14,122 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:57:14,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:57:14,128 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:57:14,128 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:57:14,129 : INFO : EPOCH - 5 : training on 41519358 raw words (30350760 effective words) took 14.1s, 2153205 effective words/s\n",
      "2020-09-02 18:57:14,129 : INFO : training on a 207596790 raw words (151745419 effective words) took 70.1s, 2163398 effective words/s\n",
      "2020-09-02 18:57:14,130 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-09-02 18:57:14,130 : INFO : training model with 10 workers on 70537 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-09-02 18:57:15,140 : INFO : EPOCH 1 - PROGRESS: at 6.98% examples, 2135221 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:16,153 : INFO : EPOCH 1 - PROGRESS: at 13.06% examples, 2133647 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:17,158 : INFO : EPOCH 1 - PROGRESS: at 19.04% examples, 2128594 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:18,167 : INFO : EPOCH 1 - PROGRESS: at 24.55% examples, 2099721 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:57:19,171 : INFO : EPOCH 1 - PROGRESS: at 31.70% examples, 2066796 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:20,180 : INFO : EPOCH 1 - PROGRESS: at 38.62% examples, 2050647 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:21,183 : INFO : EPOCH 1 - PROGRESS: at 45.66% examples, 2033942 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:22,186 : INFO : EPOCH 1 - PROGRESS: at 52.28% examples, 2021142 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:23,189 : INFO : EPOCH 1 - PROGRESS: at 59.14% examples, 2018177 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:24,190 : INFO : EPOCH 1 - PROGRESS: at 66.45% examples, 2026179 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:25,191 : INFO : EPOCH 1 - PROGRESS: at 72.61% examples, 2010871 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:26,194 : INFO : EPOCH 1 - PROGRESS: at 78.99% examples, 2006354 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:57:27,198 : INFO : EPOCH 1 - PROGRESS: at 85.73% examples, 2009669 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:28,205 : INFO : EPOCH 1 - PROGRESS: at 92.89% examples, 2010741 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:57:29,171 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:57:29,175 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:57:29,175 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:57:29,177 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:57:29,183 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:57:29,187 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:57:29,188 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:57:29,188 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:57:29,191 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:57:29,193 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:57:29,194 : INFO : EPOCH - 1 : training on 41519358 raw words (30349167 effective words) took 15.1s, 2015331 effective words/s\n",
      "2020-09-02 18:57:30,203 : INFO : EPOCH 2 - PROGRESS: at 6.48% examples, 1984601 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:31,205 : INFO : EPOCH 2 - PROGRESS: at 12.48% examples, 2054896 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:32,206 : INFO : EPOCH 2 - PROGRESS: at 18.56% examples, 2075822 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:33,209 : INFO : EPOCH 2 - PROGRESS: at 24.13% examples, 2069269 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:57:34,216 : INFO : EPOCH 2 - PROGRESS: at 31.76% examples, 2076534 words/s, in_qsize 20, out_qsize 0\n",
      "2020-09-02 18:57:35,218 : INFO : EPOCH 2 - PROGRESS: at 39.06% examples, 2079910 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:36,221 : INFO : EPOCH 2 - PROGRESS: at 46.99% examples, 2097667 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:37,223 : INFO : EPOCH 2 - PROGRESS: at 54.42% examples, 2109581 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:38,229 : INFO : EPOCH 2 - PROGRESS: at 62.10% examples, 2119833 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:39,233 : INFO : EPOCH 2 - PROGRESS: at 69.77% examples, 2129190 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:40,239 : INFO : EPOCH 2 - PROGRESS: at 76.92% examples, 2131521 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:41,242 : INFO : EPOCH 2 - PROGRESS: at 83.93% examples, 2132781 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:42,248 : INFO : EPOCH 2 - PROGRESS: at 91.56% examples, 2137337 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:43,254 : INFO : EPOCH 2 - PROGRESS: at 99.05% examples, 2139816 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:43,350 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:57:43,352 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:57:43,357 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:57:43,358 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:57:43,359 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:57:43,360 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:57:43,361 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:57:43,367 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:57:43,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:57:43,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:57:43,370 : INFO : EPOCH - 2 : training on 41519358 raw words (30349148 effective words) took 14.2s, 2141383 effective words/s\n",
      "2020-09-02 18:57:44,376 : INFO : EPOCH 3 - PROGRESS: at 7.03% examples, 2159663 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:45,379 : INFO : EPOCH 3 - PROGRESS: at 13.28% examples, 2181222 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:46,383 : INFO : EPOCH 3 - PROGRESS: at 19.23% examples, 2162829 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:57:47,383 : INFO : EPOCH 3 - PROGRESS: at 25.60% examples, 2182175 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:48,384 : INFO : EPOCH 3 - PROGRESS: at 33.64% examples, 2191712 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:49,391 : INFO : EPOCH 3 - PROGRESS: at 41.25% examples, 2181951 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:50,395 : INFO : EPOCH 3 - PROGRESS: at 49.14% examples, 2185027 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:51,401 : INFO : EPOCH 3 - PROGRESS: at 56.11% examples, 2167224 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:57:52,401 : INFO : EPOCH 3 - PROGRESS: at 63.59% examples, 2165003 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:53,402 : INFO : EPOCH 3 - PROGRESS: at 70.69% examples, 2160118 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:54,414 : INFO : EPOCH 3 - PROGRESS: at 77.86% examples, 2159814 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:55,421 : INFO : EPOCH 3 - PROGRESS: at 84.87% examples, 2157871 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:57:56,423 : INFO : EPOCH 3 - PROGRESS: at 92.07% examples, 2148658 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:57,424 : INFO : EPOCH 3 - PROGRESS: at 99.37% examples, 2147631 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:57:57,481 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:57:57,484 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:57:57,487 : INFO : worker thread finished; awaiting finish of 7 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 18:57:57,491 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:57:57,492 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:57:57,494 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:57:57,495 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:57:57,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:57:57,498 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:57:57,502 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:57:57,503 : INFO : EPOCH - 3 : training on 41519358 raw words (30349665 effective words) took 14.1s, 2148001 effective words/s\n",
      "2020-09-02 18:57:58,511 : INFO : EPOCH 4 - PROGRESS: at 6.79% examples, 2084080 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:57:59,514 : INFO : EPOCH 4 - PROGRESS: at 12.79% examples, 2106231 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:00,518 : INFO : EPOCH 4 - PROGRESS: at 18.75% examples, 2097605 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:01,522 : INFO : EPOCH 4 - PROGRESS: at 24.01% examples, 2056921 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:58:02,524 : INFO : EPOCH 4 - PROGRESS: at 31.46% examples, 2061773 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:03,525 : INFO : EPOCH 4 - PROGRESS: at 38.88% examples, 2073237 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:04,528 : INFO : EPOCH 4 - PROGRESS: at 46.52% examples, 2078141 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:05,529 : INFO : EPOCH 4 - PROGRESS: at 53.42% examples, 2073452 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:06,531 : INFO : EPOCH 4 - PROGRESS: at 60.80% examples, 2078187 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:07,536 : INFO : EPOCH 4 - PROGRESS: at 67.93% examples, 2074474 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:08,537 : INFO : EPOCH 4 - PROGRESS: at 74.82% examples, 2072401 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:09,542 : INFO : EPOCH 4 - PROGRESS: at 81.63% examples, 2077682 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:10,542 : INFO : EPOCH 4 - PROGRESS: at 88.91% examples, 2081656 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:11,546 : INFO : EPOCH 4 - PROGRESS: at 96.21% examples, 2084598 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:12,060 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:58:12,068 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:58:12,069 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:58:12,072 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:58:12,075 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:58:12,084 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:58:12,085 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:58:12,085 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:58:12,086 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:58:12,089 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:58:12,090 : INFO : EPOCH - 4 : training on 41519358 raw words (30349637 effective words) took 14.6s, 2081119 effective words/s\n",
      "2020-09-02 18:58:13,097 : INFO : EPOCH 5 - PROGRESS: at 6.55% examples, 2014677 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:14,099 : INFO : EPOCH 5 - PROGRESS: at 12.42% examples, 2043765 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:15,099 : INFO : EPOCH 5 - PROGRESS: at 18.30% examples, 2048828 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:16,104 : INFO : EPOCH 5 - PROGRESS: at 23.97% examples, 2055681 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:17,109 : INFO : EPOCH 5 - PROGRESS: at 31.31% examples, 2055603 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:18,112 : INFO : EPOCH 5 - PROGRESS: at 38.21% examples, 2042229 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:19,114 : INFO : EPOCH 5 - PROGRESS: at 45.71% examples, 2044349 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:20,115 : INFO : EPOCH 5 - PROGRESS: at 52.78% examples, 2048547 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:21,119 : INFO : EPOCH 5 - PROGRESS: at 59.84% examples, 2047207 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:22,120 : INFO : EPOCH 5 - PROGRESS: at 66.92% examples, 2046222 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:23,124 : INFO : EPOCH 5 - PROGRESS: at 73.58% examples, 2039005 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:24,126 : INFO : EPOCH 5 - PROGRESS: at 80.16% examples, 2040220 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:25,130 : INFO : EPOCH 5 - PROGRESS: at 87.06% examples, 2042027 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:26,134 : INFO : EPOCH 5 - PROGRESS: at 94.02% examples, 2038417 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:26,952 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:58:26,956 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:58:26,957 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:58:26,963 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:58:26,964 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:58:26,965 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:58:26,967 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:58:26,968 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:58:26,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:58:26,977 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:58:26,978 : INFO : EPOCH - 5 : training on 41519358 raw words (30345774 effective words) took 14.9s, 2038837 effective words/s\n",
      "2020-09-02 18:58:27,988 : INFO : EPOCH 6 - PROGRESS: at 6.14% examples, 1886684 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:28,990 : INFO : EPOCH 6 - PROGRESS: at 12.16% examples, 1993117 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:29,992 : INFO : EPOCH 6 - PROGRESS: at 18.28% examples, 2042795 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:31,003 : INFO : EPOCH 6 - PROGRESS: at 24.24% examples, 2075016 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:32,007 : INFO : EPOCH 6 - PROGRESS: at 32.05% examples, 2092754 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:33,009 : INFO : EPOCH 6 - PROGRESS: at 39.56% examples, 2102573 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:58:34,010 : INFO : EPOCH 6 - PROGRESS: at 47.03% examples, 2098516 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:35,022 : INFO : EPOCH 6 - PROGRESS: at 54.38% examples, 2103822 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:36,025 : INFO : EPOCH 6 - PROGRESS: at 61.91% examples, 2110239 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:37,033 : INFO : EPOCH 6 - PROGRESS: at 69.21% examples, 2108510 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:38,039 : INFO : EPOCH 6 - PROGRESS: at 76.09% examples, 2104980 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:39,039 : INFO : EPOCH 6 - PROGRESS: at 82.84% examples, 2102191 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:58:40,052 : INFO : EPOCH 6 - PROGRESS: at 90.29% examples, 2106412 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:41,056 : INFO : EPOCH 6 - PROGRESS: at 97.75% examples, 2110966 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:41,326 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:58:41,331 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:58:41,335 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:58:41,336 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:58:41,341 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:58:41,343 : INFO : worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 18:58:41,344 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:58:41,344 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:58:41,349 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:58:41,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:58:41,351 : INFO : EPOCH - 6 : training on 41519358 raw words (30347482 effective words) took 14.4s, 2112045 effective words/s\n",
      "2020-09-02 18:58:42,358 : INFO : EPOCH 7 - PROGRESS: at 6.77% examples, 2076616 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:43,359 : INFO : EPOCH 7 - PROGRESS: at 12.70% examples, 2098190 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:44,365 : INFO : EPOCH 7 - PROGRESS: at 18.56% examples, 2074667 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:45,367 : INFO : EPOCH 7 - PROGRESS: at 24.32% examples, 2087216 words/s, in_qsize 20, out_qsize 1\n",
      "2020-09-02 18:58:46,368 : INFO : EPOCH 7 - PROGRESS: at 32.00% examples, 2094024 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:47,368 : INFO : EPOCH 7 - PROGRESS: at 39.32% examples, 2094779 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:48,371 : INFO : EPOCH 7 - PROGRESS: at 46.99% examples, 2100152 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:49,375 : INFO : EPOCH 7 - PROGRESS: at 53.83% examples, 2089574 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:50,380 : INFO : EPOCH 7 - PROGRESS: at 61.26% examples, 2093473 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:51,381 : INFO : EPOCH 7 - PROGRESS: at 68.57% examples, 2095358 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:52,388 : INFO : EPOCH 7 - PROGRESS: at 75.58% examples, 2094815 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:53,392 : INFO : EPOCH 7 - PROGRESS: at 82.13% examples, 2089346 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:54,398 : INFO : EPOCH 7 - PROGRESS: at 89.10% examples, 2085057 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:55,398 : INFO : EPOCH 7 - PROGRESS: at 96.35% examples, 2086641 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:55,861 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:58:55,865 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:58:55,866 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:58:55,867 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:58:55,874 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:58:55,875 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:58:55,876 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:58:55,877 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:58:55,882 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:58:55,884 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:58:55,884 : INFO : EPOCH - 7 : training on 41519358 raw words (30352401 effective words) took 14.5s, 2088998 effective words/s\n",
      "2020-09-02 18:58:56,893 : INFO : EPOCH 8 - PROGRESS: at 7.09% examples, 2173560 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:58:57,897 : INFO : EPOCH 8 - PROGRESS: at 13.49% examples, 2208601 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:58,900 : INFO : EPOCH 8 - PROGRESS: at 19.58% examples, 2206074 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:58:59,904 : INFO : EPOCH 8 - PROGRESS: at 25.64% examples, 2182028 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:59:00,910 : INFO : EPOCH 8 - PROGRESS: at 33.28% examples, 2163233 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:59:01,913 : INFO : EPOCH 8 - PROGRESS: at 40.51% examples, 2148693 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:02,915 : INFO : EPOCH 8 - PROGRESS: at 48.47% examples, 2157097 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:03,917 : INFO : EPOCH 8 - PROGRESS: at 55.91% examples, 2159362 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:04,919 : INFO : EPOCH 8 - PROGRESS: at 63.33% examples, 2156804 words/s, in_qsize 20, out_qsize 0\n",
      "2020-09-02 18:59:05,919 : INFO : EPOCH 8 - PROGRESS: at 70.58% examples, 2155956 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:06,928 : INFO : EPOCH 8 - PROGRESS: at 77.78% examples, 2157335 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:07,938 : INFO : EPOCH 8 - PROGRESS: at 84.83% examples, 2156319 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:59:08,942 : INFO : EPOCH 8 - PROGRESS: at 92.32% examples, 2152976 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:59:09,945 : INFO : EPOCH 8 - PROGRESS: at 99.39% examples, 2147276 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:09,992 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:59:09,994 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:59:09,997 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:59:10,000 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:59:10,002 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:59:10,004 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:59:10,008 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:59:10,010 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:59:10,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:59:10,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:59:10,015 : INFO : EPOCH - 8 : training on 41519358 raw words (30350326 effective words) took 14.1s, 2148375 effective words/s\n",
      "2020-09-02 18:59:11,027 : INFO : EPOCH 9 - PROGRESS: at 6.72% examples, 2054119 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:12,034 : INFO : EPOCH 9 - PROGRESS: at 13.11% examples, 2144111 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:59:13,041 : INFO : EPOCH 9 - PROGRESS: at 19.33% examples, 2167554 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:14,041 : INFO : EPOCH 9 - PROGRESS: at 25.72% examples, 2183879 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:15,041 : INFO : EPOCH 9 - PROGRESS: at 33.32% examples, 2165388 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:16,041 : INFO : EPOCH 9 - PROGRESS: at 40.41% examples, 2143461 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:17,044 : INFO : EPOCH 9 - PROGRESS: at 48.26% examples, 2149320 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:18,046 : INFO : EPOCH 9 - PROGRESS: at 55.84% examples, 2156471 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:59:19,053 : INFO : EPOCH 9 - PROGRESS: at 63.59% examples, 2162732 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:59:20,066 : INFO : EPOCH 9 - PROGRESS: at 70.97% examples, 2165041 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:21,071 : INFO : EPOCH 9 - PROGRESS: at 77.94% examples, 2158961 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:22,072 : INFO : EPOCH 9 - PROGRESS: at 85.02% examples, 2160677 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:23,072 : INFO : EPOCH 9 - PROGRESS: at 92.72% examples, 2162604 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:24,012 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:59:24,019 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:59:24,021 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:59:24,023 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:59:24,026 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:59:24,027 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:59:24,028 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:59:24,031 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:59:24,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:59:24,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 18:59:24,035 : INFO : EPOCH - 9 : training on 41519358 raw words (30346184 effective words) took 14.0s, 2165251 effective words/s\n",
      "2020-09-02 18:59:25,039 : INFO : EPOCH 10 - PROGRESS: at 6.89% examples, 2118632 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:59:26,045 : INFO : EPOCH 10 - PROGRESS: at 12.72% examples, 2098573 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:59:27,051 : INFO : EPOCH 10 - PROGRESS: at 18.92% examples, 2117875 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:59:28,054 : INFO : EPOCH 10 - PROGRESS: at 25.01% examples, 2137344 words/s, in_qsize 17, out_qsize 2\n",
      "2020-09-02 18:59:29,055 : INFO : EPOCH 10 - PROGRESS: at 33.03% examples, 2150885 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:30,059 : INFO : EPOCH 10 - PROGRESS: at 40.67% examples, 2157611 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:31,060 : INFO : EPOCH 10 - PROGRESS: at 48.24% examples, 2149462 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:32,061 : INFO : EPOCH 10 - PROGRESS: at 54.77% examples, 2123711 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:33,062 : INFO : EPOCH 10 - PROGRESS: at 62.10% examples, 2121569 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:59:34,062 : INFO : EPOCH 10 - PROGRESS: at 69.30% examples, 2117063 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:35,062 : INFO : EPOCH 10 - PROGRESS: at 76.09% examples, 2111409 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:59:36,066 : INFO : EPOCH 10 - PROGRESS: at 82.91% examples, 2109459 words/s, in_qsize 18, out_qsize 1\n",
      "2020-09-02 18:59:37,066 : INFO : EPOCH 10 - PROGRESS: at 89.82% examples, 2103427 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:38,068 : INFO : EPOCH 10 - PROGRESS: at 97.11% examples, 2104311 words/s, in_qsize 19, out_qsize 0\n",
      "2020-09-02 18:59:38,425 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-02 18:59:38,430 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-02 18:59:38,433 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-02 18:59:38,434 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-02 18:59:38,439 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-02 18:59:38,440 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-02 18:59:38,441 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-02 18:59:38,442 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-02 18:59:38,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-02 18:59:38,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-02 18:59:38,446 : INFO : EPOCH - 10 : training on 41519358 raw words (30349017 effective words) took 14.4s, 2106446 effective words/s\n",
      "2020-09-02 18:59:38,446 : INFO : training on a 415193580 raw words (303488801 effective words) took 144.3s, 2102953 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(303488801, 415193580)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Word2Vec model parameters\n",
    "\n",
    "size:\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me.\n",
    "\n",
    "window:\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n",
    "\n",
    "min_count:\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "workers:\n",
    "How many threads to use behind the scenes?\n",
    "\n",
    "sg: sg=1 means skip-gram and sg=0 menascbow\n",
    "'''\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10, sg=0)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 19:11:05,310 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('filthy', 0.8742550015449524),\n",
       " ('stained', 0.7924962639808655),\n",
       " ('unclean', 0.7805405259132385),\n",
       " ('smelly', 0.7659915685653687),\n",
       " ('dusty', 0.7650768160820007),\n",
       " ('grubby', 0.7523942589759827),\n",
       " ('dingy', 0.7406297326087952),\n",
       " ('soiled', 0.7367775440216064),\n",
       " ('grimy', 0.7195453643798828),\n",
       " ('disgusting', 0.7135657072067261)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9165330529212952),\n",
       " ('friendly', 0.8376738429069519),\n",
       " ('cordial', 0.8167857527732849),\n",
       " ('professional', 0.7822703123092651),\n",
       " ('curteous', 0.7742598056793213),\n",
       " ('attentive', 0.774246335029602)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('germany', 0.675749659538269),\n",
       " ('canada', 0.6650902032852173),\n",
       " ('england', 0.6330846548080444),\n",
       " ('spain', 0.6234018206596375),\n",
       " ('malaysia', 0.592072606086731),\n",
       " ('thailand', 0.5844281315803528)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('duvet', 0.7160459160804749),\n",
       " ('mattress', 0.7111192345619202),\n",
       " ('pillowcase', 0.6824204325675964),\n",
       " ('blanket', 0.6793428659439087),\n",
       " ('matress', 0.6785378456115723),\n",
       " ('quilt', 0.6743865013122559),\n",
       " ('pillows', 0.6349917054176331),\n",
       " ('sheets', 0.632304847240448),\n",
       " ('pillowcases', 0.6291857361793518),\n",
       " ('foam', 0.6135457158088684)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7659916"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\.conda\\envs\\NLP_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3935169 ,  0.11407837, -3.1584964 , -0.7915665 ,  0.72225785,\n",
       "       -2.3195415 , -2.064683  ,  0.74891615,  0.18662123, -1.5445935 ,\n",
       "       -1.2669321 ,  1.8349296 ,  0.9413326 ,  3.2725563 , -1.3877819 ,\n",
       "       -1.2167646 , -2.3522906 , -3.12771   ,  0.35763958,  0.39196694,\n",
       "        0.59825236,  0.40232947,  3.9343026 , -0.75055695,  2.2588851 ,\n",
       "       -0.9341895 ,  1.4763014 ,  0.9846982 , -1.9921819 , -0.26836854,\n",
       "       -2.328544  , -2.3392472 ,  4.0021057 ,  4.20997   ,  0.96173555,\n",
       "       -1.3712412 ,  0.8687379 , -1.8497181 , -3.9322212 , -0.85039043,\n",
       "        0.15363996, -3.4320147 , -1.9162629 , -1.6582947 ,  2.703479  ,\n",
       "        0.92882884,  3.0210185 , -1.5654312 , -2.8198316 , -1.462156  ,\n",
       "       -3.6354198 ,  1.1741832 ,  0.2906967 , -0.8730496 ,  2.7410195 ,\n",
       "       -3.552861  ,  0.7585775 ,  1.7082635 , -2.1707187 , -1.9713386 ,\n",
       "        1.2040828 , -0.04789567,  0.98023486,  1.8673567 , -0.4345094 ,\n",
       "        0.6879132 ,  3.9073038 , -0.87230873, -2.459168  , -1.6901205 ,\n",
       "       -2.550578  ,  4.862922  , -2.4646976 , -0.44069263,  2.419837  ,\n",
       "       -3.3490472 , -1.2977519 , -0.46683866, -2.5532267 , -1.5795006 ,\n",
       "       -0.970031  ,  1.0229474 , -0.8813151 ,  1.0247132 ,  0.02840165,\n",
       "       -1.1081946 , -2.8070853 ,  1.8743097 , -0.78127885, -0.4506161 ,\n",
       "       -0.3198648 , -0.06463879,  1.0264757 ,  2.9607205 ,  2.024858  ,\n",
       "       -0.8400499 , -2.145752  , -3.5627768 , -1.8641373 , -2.475333  ,\n",
       "        6.5724235 , -1.7062124 , -0.3490075 , -0.3849243 ,  1.3798479 ,\n",
       "       -2.1242092 ,  0.9789985 ,  1.1198701 , -2.3448553 , -0.03746291,\n",
       "       -4.0661592 ,  3.4039485 , -4.7181835 , -1.1220206 ,  3.2832682 ,\n",
       "       -5.862114  , -1.7024194 ,  3.0240974 ,  2.5706422 ,  1.6730094 ,\n",
       "        2.0496314 , -3.8057365 ,  3.8688307 , -0.36138123, -0.93862927,\n",
       "        2.5085301 , -3.8797708 , -1.6478158 , -1.903582  , -0.01874057,\n",
       "        0.7229474 ,  1.2705132 ,  1.0077326 , -0.7617776 ,  1.2061132 ,\n",
       "       -2.5684164 , -1.2680364 , -0.5997946 , -3.6263528 , -3.6262474 ,\n",
       "        4.9928746 , -4.780937  ,  0.1678223 , -1.4057853 , -0.31616965,\n",
       "       -2.929006  ,  0.24365579, -1.1174601 ,  1.74718   , -1.3442191 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print word vector\n",
    "model.wv['dirty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove  \n",
    "ref: https://github.com/maciejkula/glove-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# download the model and return as object ready for use\n",
    "model_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "\n",
    "model_glove_twitter.wv.most_similar(\"pelosi\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.wv['dirty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.wv.most_similar(\"policies\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.wv.doesnt_match([\"trump\",\"bernie\",\"obama\",\"pelosi\",\"orange\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "#again, download and load the model\n",
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find similarity\n",
    "model_gigaword.wv.most_similar(positive=['dirty','grimy'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python example to train doc2vec model (with or without pre-trained word embeddings)\n",
    "\n",
    "import gensim.models as g\n",
    "import logging\n",
    "\n",
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0 #0 = dbow; 1 = dmpv\n",
    "worker_count = 1 #number of parallel processes\n",
    "\n",
    "#pretrained word embeddings\n",
    "pretrained_emb = \"./dataset/toy_data/pretrained_word_embeddings.txt\" #None if use without pretrained embeddings\n",
    "\n",
    "#input corpus\n",
    "train_corpus = \"./dataset/toy_data/train_docs.txt\"\n",
    "\n",
    "#output model\n",
    "saved_path = \"./dataset/toy_data/model.bin\"\n",
    "\n",
    "#enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)\n",
    "\n",
    "#save model\n",
    "model.save(saved_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python example to infer document vectors from trained doc2vec model\n",
    "import gensim.models as g\n",
    "import codecs\n",
    "\n",
    "#parameters\n",
    "model=\"./dataset/toy_data/model.bin\"\n",
    "test_docs=\"./dataset/toy_data/test_docs.txt\"\n",
    "output_file=\"./dataset/toy_data/test_vectors.txt\"\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "#load model\n",
    "m = g.Doc2Vec.load(model)\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n",
    "print('test docs:\\n{}'.format(test_docs))\n",
    "#infer test vectors\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
