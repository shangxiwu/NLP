{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Basic"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> 340cb883b90173fbacb941c21ab5356a0094116a
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)\n",
    "#> Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Saudis',\n",
       "  'are',\n",
       "  'preparing',\n",
       "  'a',\n",
       "  'report',\n",
       "  'that',\n",
       "  'will',\n",
       "  'acknowledge',\n",
       "  'that'],\n",
       " ['Saudi',\n",
       "  'journalist',\n",
       "  'Jamal',\n",
       "  \"Khashoggi's\",\n",
       "  'death',\n",
       "  'was',\n",
       "  'the',\n",
       "  'result',\n",
       "  'of',\n",
       "  'an'],\n",
       " ['interrogation',\n",
       "  'that',\n",
       "  'went',\n",
       "  'wrong,',\n",
       "  'one',\n",
       "  'that',\n",
       "  'was',\n",
       "  'intended',\n",
       "  'to',\n",
       "  'lead'],\n",
       " ['to',\n",
       "  'his',\n",
       "  'abduction',\n",
       "  'from',\n",
       "  'Turkey,',\n",
       "  'according',\n",
       "  'to',\n",
       "  'two',\n",
       "  'sources.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Saudis': 0,\n",
       " 'The': 1,\n",
       " 'a': 2,\n",
       " 'acknowledge': 3,\n",
       " 'are': 4,\n",
       " 'preparing': 5,\n",
       " 'report': 6,\n",
       " 'that': 7,\n",
       " 'will': 8,\n",
       " 'Jamal': 9,\n",
       " \"Khashoggi's\": 10,\n",
       " 'Saudi': 11,\n",
       " 'an': 12,\n",
       " 'death': 13,\n",
       " 'journalist': 14,\n",
       " 'of': 15,\n",
       " 'result': 16,\n",
       " 'the': 17,\n",
       " 'was': 18,\n",
       " 'intended': 19,\n",
       " 'interrogation': 20,\n",
       " 'lead': 21,\n",
       " 'one': 22,\n",
       " 'to': 23,\n",
       " 'went': 24,\n",
       " 'wrong,': 25,\n",
       " 'Turkey,': 26,\n",
       " 'abduction': 27,\n",
       " 'according': 28,\n",
       " 'from': 29,\n",
       " 'his': 30,\n",
       " 'sources.': 31,\n",
       " 'two': 32}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "outputs": [],
>>>>>>> 340cb883b90173fbacb941c21ab5356a0094116a
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "\n",
    "\n",
    "# If you check now, the dictionary should have been updated with the new words (tokens).\n",
    "print(dictionary)\n",
    "#> Dictionary(45 unique tokens: ['Human', 'abc', 'applications', 'computer', 'for']...)\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "# CountVectorizer(stop_words=\"english\")\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\n",
    "\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"Features name:\\n{}\".format(vect.get_feature_names()))\n",
    "print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text =[\n",
    "      \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n",
    "      \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n",
    "      \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n",
    "      \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n",
    "      \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" \n",
    "     ]\n",
    "\n",
    "model1 = CountVectorizer(text,stop_words=[\"all\",\"in\",\"the\",\"is\",\"and\"])\n",
    "result1_vector = model1.fit_transform(text)\n",
    "print('result1_vector shape: {}'.format(result1_vector.shape))\n",
    "\n",
    "model2 = CountVectorizer(text,stop_words=\"english\")\n",
    "result2_vector = model2.fit_transform(text)\n",
    "print('result2_vector shape: {}'.format(result2_vector.shape))\n",
    "\n",
    "# use proportion here. Ignore terms that occurred in less than 25% of the documents\n",
    "#model3 = CountVectorizer(text,min_df=0.25)\n",
    "# ignore terms that appeared in less than n documents (can be proportion or absolute counts)\n",
    "model3 = CountVectorizer(text,min_df=2)\n",
    "result3_vector = model3.fit_transform(text)\n",
    "print('result3_vector shape: {}'.format(result3_vector.shape))\n",
    "\n",
    "# ignore terms that appeared in more than n documents (can be proportion or absolute counts)\n",
    "# use proportion here\n",
    "model4 = CountVectorizer(text,max_df=0.50)\n",
    "result4_vector = model4.fit_transform(text)\n",
    "print('result4_vector shape: {}'.format(result4_vector.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()    \n",
    "\n",
    "X = vectorizer.fit_transform(corpus)       #先轉成 bag of words\n",
    "\n",
    "word = vectorizer.get_feature_names()\n",
    "print(word)\n",
    "\n",
    "print(X.toarray())\n",
    " \n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)    # \n",
    "tfidf_weight = tfidf.toarray() \n",
    "print(tfidf_weight)\n",
    "\n",
    "\n",
    "for i in range(len(tfidf_weight)):\n",
    "    print(\"-------output {}-th document tf-idf weight------\".format(i))\n",
    "    for j in range(len(word)):\n",
    "        print(word[j],tfidf_weight[i][j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bards_words =[\"The fool doth think he is wise\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "vect1 = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(vect1.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect1.get_feature_names()))\n",
    "\n",
    "vect2 = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(vect2.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect2.get_feature_names()))\n",
    "print(\"Transformed data (dense):\\n{}\".format(vect2.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/%E6%89%8B%E5%AF%AB%E7%AD%86%E8%A8%98/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-%E4%BD%BF%E7%94%A8-n-gram-%E5%AF%A6%E7%8F%BE%E8%BC%B8%E5%85%A5%E6%96%87%E5%AD%97%E9%A0%90%E6%B8%AC-10ac622aab7a\n",
    "\n",
    "from collections import Counter, namedtuple\n",
    "import json\n",
    "import re\n",
    "\n",
    "DATASET_DIR = 'dataset/WebNews.json'\n",
    "with open(DATASET_DIR, encoding = 'utf8') as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "seg_list = list(map(lambda d: d['detailcontent'], dataset))\n",
    "rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "seg_list = [rule.sub('', seg) for seg in seg_list]\n",
    "print(seg_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(documents, N=2):\n",
    "    ngram_prediction = dict()\n",
    "    total_grams = list()\n",
    "    words = list()\n",
    "    Word = namedtuple('Word', ['word', 'prob'])\n",
    "\n",
    "    for doc in documents:\n",
    "        split_words = ['<s>'] + list(doc) + ['</s>']\n",
    "        # 計算分子\n",
    "        [total_grams.append(tuple(split_words[i:i+N])) for i in range(len(split_words)-N+1)]\n",
    "        # 計算分母\n",
    "        [words.append(tuple(split_words[i:i+N-1])) for i in range(len(split_words)-N+2)]\n",
    "        \n",
    "    total_word_counter = Counter(total_grams)\n",
    "    word_counter = Counter(words)\n",
    "    \n",
    "    for key in total_word_counter:\n",
    "        word = ''.join(key[:N-1])\n",
    "        if word not in ngram_prediction:\n",
    "            ngram_prediction.update({word: set()})\n",
    "            \n",
    "        next_word_prob = total_word_counter[key]/word_counter[key[:N-1]]\n",
    "        w = Word(key[-1], '{:.3g}'.format(next_word_prob))\n",
    "        ngram_prediction[word].add(w)\n",
    "        \n",
    "    return ngram_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_prediction = ngram(seg_list, N=3)\n",
    "#print(tri_prediction)\n",
    "print(dict(list(tri_prediction.items())[0:5]))\n",
    "for word, ng in tri_prediction.items():\n",
    "    tri_prediction[word] = sorted(ng, key=lambda x: x.prob, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '韓國'\n",
    "next_words = list(tri_prediction[text])[:10]\n",
    "for next_word in next_words:\n",
    "    print('next word: {}, probability: {}'.format(next_word.word, next_word.prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW & Skip-gram  \n",
    "yield explain: https://pyzh.readthedocs.io/en/latest/the-python-yield-keyword-explained.html#id8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "data_file=\"./dataset/reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('./dataset/reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    print(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                print(\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "print(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Word2Vec model parameters\n",
    "\n",
    "size:\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me.\n",
    "\n",
    "window:\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n",
    "\n",
    "min_count:\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "workers:\n",
    "How many threads to use behind the scenes?\n",
    "\n",
    "sg: sg=1 means skip-gram and sg=0 menascbow\n",
    "'''\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10, sg=0)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word vector\n",
    "model.wv['dirty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove  \n",
    "ref: https://github.com/maciejkula/glove-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# download the model and return as object ready for use\n",
    "model_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "\n",
    "model_glove_twitter.wv.most_similar(\"pelosi\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.wv['dirty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.wv.most_similar(\"policies\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter.wv.doesnt_match([\"trump\",\"bernie\",\"obama\",\"pelosi\",\"orange\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "#again, download and load the model\n",
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find similarity\n",
    "model_gigaword.wv.most_similar(positive=['dirty','grimy'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python example to train doc2vec model (with or without pre-trained word embeddings)\n",
    "\n",
    "import gensim.models as g\n",
    "import logging\n",
    "\n",
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0 #0 = dbow; 1 = dmpv\n",
    "worker_count = 1 #number of parallel processes\n",
    "\n",
    "#pretrained word embeddings\n",
    "pretrained_emb = \"./dataset/toy_data/pretrained_word_embeddings.txt\" #None if use without pretrained embeddings\n",
    "\n",
    "#input corpus\n",
    "train_corpus = \"./dataset/toy_data/train_docs.txt\"\n",
    "\n",
    "#output model\n",
    "saved_path = \"./dataset/toy_data/model.bin\"\n",
    "\n",
    "#enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)\n",
    "\n",
    "#save model\n",
    "model.save(saved_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python example to infer document vectors from trained doc2vec model\n",
    "import gensim.models as g\n",
    "import codecs\n",
    "\n",
    "#parameters\n",
    "model=\"./dataset/toy_data/model.bin\"\n",
    "test_docs=\"./dataset/toy_data/test_docs.txt\"\n",
    "output_file=\"./dataset/toy_data/test_vectors.txt\"\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "#load model\n",
    "m = g.Doc2Vec.load(model)\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n",
    "print('test docs:\\n{}'.format(test_docs))\n",
    "#infer test vectors\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
